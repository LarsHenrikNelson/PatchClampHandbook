{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b419a00",
   "metadata": {},
   "source": [
    "(curves)=\n",
    "# Curves\n",
    "Curves are abundant in nueroscience. In particular curves you will typically see are exponential decay, sigmoid, logarithmic and linear. Exponential decays occur in PSC/PSPs (both rise and decay), membrane taus (current clamp), and membrane currents (Ih-currents). Sigmoid currents occur in dose-response curves (FI curves and Ih currents). Logarithmic curves occur in membrane vorage changes in spiking activity in cells. Linear curves are seen in IV curves. In neuroscince many of these curves are measuring some sort of dose-response or response-time relationship.\n",
    "\n",
    "You have seen these curves in action in several chapters such as the [Current Clamp](current_clamp) and [m/sEPSC](miniature_psc). In this chapter we will delve into the specific parameters of the curves and how the curve_fit function in Scipy works.\n",
    "\n",
    "Some quick basics that all curves typically have. Many curves go from 0 to 1 or 1 to 0. However, most data we collect is not scaled like this. Curves typically have some scaling factor which means you divide or multiple the equation by a number to change the range of possible the equation can output. Curves also have shifting factors which are added or subtracted to the equation and move the equation up or down or to the left or  right. This is really a core concept that I did not pick up early on but wish I had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95596f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import Checkbox, ColumnDataSource, CustomJS, Select, Slider, Spinner\n",
    "from bokeh.plotting import figure, gridplot\n",
    "from scipy import optimize, stats\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd444492",
   "metadata": {},
   "source": [
    "## Exponential decay\n",
    "Exponential decay occurs in processes where a value decreases proportionally to its current value. Exponential decay is primarily parameterized by the decay constant, $\\lambda$. $\\lambda$ can be describe as a time constant, $\\tau$ where $\\tau = \\frac{1}{\\lambda}$. $\\tau$, the time constant or the time if takes for the process to reach a value of ~1/3 or $\\frac{1}{e}$ of the original value. The basic exponential equation is: $N_{0}e^{-\\frac{t}{\\tau}}$. $N_{0}$ is the original value, and in electrophysiological terms is the amplitude of a PSC/PSP. The equation expects some things. One is that you value decay towards zero. The second is that typically $N_{0}$ is at time point zero. When we curve fit the mEPSCs in the [m/sEPSC](mininature_psc) chapter we created a new time array that started at zero.\n",
    "\n",
    "The decay equation can be scaled and shifted in several ways. $N_{0}$ can be positive or negative. $N_{0}$ is multiplicative scaling factor, since you leave it out of the equation and you will just get 1 for time value zero since $e^0 = 1$. $\\tau$ is a also just a multiplicative scaling factor but for time. You can shift the whole equation up or down by adding a constant if your decay does not converge towards zero but some other constant value. There is no way to just shift the equation along in time. That is because you are assumming $N_{0}$ is $time = 0$ and if you shift $t$ in the equations you change where you are measuring the decay from. To shift the equation in time you just add a constant to your x values after putting them through the equation. You can also add decay equations together to get double, triple or even more decays. However, adding more than 2 or 3 decays together can lead to over fitting and decreased interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7dc3e",
   "metadata": {},
   "source": [
    "### Single exponential decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b4ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_decay(x, amplitude=1, tau=1, yshift=0):\n",
    "    y = yshift + (amplitude * np.exp(-x / tau))\n",
    "    return y\n",
    "\n",
    "\n",
    "x = np.arange(300) / 10\n",
    "source = ColumnDataSource({\"x\": x, \"y\": exp_decay(x, amplitude=15, tau=2.5)})\n",
    "\n",
    "plot = figure(width=400, height=400)\n",
    "\n",
    "plot.line(x, exp_decay(x, amplitude=15, tau=2.5), line_width=2, line_color=\"black\")\n",
    "plot.line(\"x\", \"y\", source=source, line_width=3, line_alpha=0.6, line_color=\"magenta\")\n",
    "\n",
    "yshift = Slider(start=-10, end=10, value=0, step=0.25, title=\"Y shift\")\n",
    "xshift = Slider(start=-10, end=10, value=0, step=0.25, title=\"X shift\")\n",
    "decay_tau = Slider(start=0.75, end=50, value=2.5, step=0.25, title=\"Decay tau\")\n",
    "amplitude = Slider(start=-30, end=30, value=15, step=0.5, title=\"Amplitude\")\n",
    "length = Slider(start=10, end=70, value=30, step=0.25, title=\"Length\")\n",
    "\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        source=source,\n",
    "        decay_tau=decay_tau,\n",
    "        amplitude=amplitude,\n",
    "        length=length,\n",
    "        yshift=yshift,\n",
    "        xshift=xshift,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    const len = Math.round(length.value * 10)\n",
    "    const t_length = Array.from({ length: len }, (_, i) => (xshift.value + i)/10)\n",
    "    const y = t_length.map(x => {\n",
    "        return (amplitude.value * Math.exp(-x / decay_tau.value) + yshift.value\n",
    "    })\n",
    "    source.data.x = t_length;\n",
    "    source.data.y = y;\n",
    "    source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "yshift.js_on_change(\"value\", callback)\n",
    "decay_tau.js_on_change(\"value\", callback)\n",
    "amplitude.js_on_change(\"value\", callback)\n",
    "length.js_on_change(\"value\", callback)\n",
    "\n",
    "show(row(plot, column(decay_tau, amplitude, length, yshift, xshift)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e1cda",
   "metadata": {},
   "source": [
    "### Double exponential decay\n",
    "You will see that you can get much more complicated shapes with a double exponential decay. Some shapes don't even look like a regular decay. There are a couple ways you can parameterize the double exponential decay. You could have a y_shift for each decay or just one for both. Usually when we think a curve is a double exponential decay we are assuming that the amplitude of both decays has the same sign. Additionally, you could make the amplitude the same for each decay or allow them to be different. It starts to get very complicated. You can have any combination of factors you choose. For fitting PSC/PSP decays I usually include an amplitude and tau parameter for each decay but ensure that the amplitudes both have the same sign. In the example below I do not ensure the amplitudes are the same sign. I also encourage you to reparameterize the equation if you feel confident enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8999f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_decay(x, amplitude_fast=1, tau_fast=1, amplitude_slow=0, tau_slow=1):\n",
    "    y = (amplitude_slow * np.exp(-x / tau_slow)) + (\n",
    "        amplitude_fast * np.exp(-x / tau_fast)\n",
    "    )\n",
    "    return y\n",
    "\n",
    "\n",
    "x = np.arange(300) / 10\n",
    "source_db = ColumnDataSource(\n",
    "    {\"x\": x, \"y\": db_decay(x, amplitude_fast=15, tau_fast=2.5)}\n",
    ")\n",
    "\n",
    "plot = figure(width=400, height=400)\n",
    "\n",
    "plot.line(\n",
    "    x, db_decay(x, amplitude_fast=15, tau_fast=2.5), line_width=2, line_color=\"black\"\n",
    ")\n",
    "plot.line(\n",
    "    \"x\", \"y\", source=source_db, line_width=3, line_alpha=0.6, line_color=\"magenta\"\n",
    ")\n",
    "\n",
    "tau_slow = Slider(start=0.75, end=50, value=1, step=0.25, title=\"Tau slow\")\n",
    "amplitude_slow = Slider(start=-30, end=30, value=1, step=0.5, title=\"Amplitude_slow\")\n",
    "tau_fast = Slider(start=0.75, end=50, value=2.5, step=0.25, title=\"Tau fast\")\n",
    "amplitude_fast = Slider(start=-30, end=30, value=15, step=0.5, title=\"Amplitude fast\")\n",
    "length = Slider(start=10, end=70, value=30, step=0.25, title=\"Length\")\n",
    "\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        source=source_db,\n",
    "        tau_slow=tau_slow,\n",
    "        amplitude_slow=amplitude_slow,\n",
    "        tau_fast=tau_fast,\n",
    "        amplitude_fast=amplitude_fast,\n",
    "        length=length,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    const len = Math.round(length.value * 10)\n",
    "    const t_length = Array.from({ length: len }, (_, i) => (0 + i)/10)\n",
    "    const y = t_length.map(x => {\n",
    "        return ((amplitude_slow.value * Math.exp((-x) / tau_slow.value))\n",
    "        +(amplitude_fast.value * Math.exp((-x) / tau_fast.value)))\n",
    "    })\n",
    "    source.data.x = t_length;\n",
    "    source.data.y = y;\n",
    "    source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "tau_slow.js_on_change(\"value\", callback)\n",
    "amplitude_slow.js_on_change(\"value\", callback)\n",
    "tau_fast.js_on_change(\"value\", callback)\n",
    "amplitude_fast.js_on_change(\"value\", callback)\n",
    "length.js_on_change(\"value\", callback)\n",
    "\n",
    "show(row(plot, column(tau_slow, amplitude_slow, tau_fast, amplitude_fast, length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f92f32",
   "metadata": {},
   "source": [
    "## Sigmoid curve\n",
    "You will often see sigmoidal curves in input-output experiments. Some of these experiments include FI curves for firing rate and current density curves for ion channels (like Ih channels). This is because there is often a minimal and maximal amount of voltage/current primarily due the receptors present on a cell as well as the membrane composition. The basic sigmoid curve equation is: $\\frac{1}{1+e^{-x}}$. This specific equation is actually the logistic function, but there are other functions that have sigmoidal shapes. You can further parameterize the equation by subtracting/adding a value to x to shift the equation along the x-axis. You can divide x by a number which is the slope. You can multiply the whole equation by a value to scale the output. Lastly you can add a value to the exquation to offset the axis. The full equation looks like this: $\\frac{1}{1+e^\\frac{x-m}{s}}*y+o$ where $s=slope$, $m=x  offset$, $y=y scale$ and $o=y offset$. You may also see the equation as: $bottom-\\frac{bottom-top}{1+10^{(LogEC50-X)*HillSlope}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e1495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, max_value, midpoint, slope, offset):\n",
    "    return 1 / (1 + np.exp((x - midpoint) / -slope)) * max_value + offset\n",
    "\n",
    "\n",
    "x = np.linspace(0, 400)\n",
    "y1 = sigmoid(x, 16.0, 200.0, 40.0, 0)\n",
    "y2 = sigmoid(x, 16.0, 200.0, 40.0, 0)\n",
    "\n",
    "source1 = ColumnDataSource({\"x\": np.linspace(0, 400), \"y1\": y1, \"y2\": y2})\n",
    "source2 = ColumnDataSource(\n",
    "    {\"x\": np.linspace(0, 400, num=49), \"y1\": np.diff(y1), \"y2\": np.diff(y2)}\n",
    ")\n",
    "\n",
    "plot1 = figure(width=300, height=300)\n",
    "plot1.line(\"x\", \"y1\", source=source1, line_width=2, line_alpha=0.6, line_color=\"black\")\n",
    "plot1.line(\n",
    "    \"x\", \"y2\", source=source1, line_width=3, line_alpha=0.6, line_color=\"magenta\"\n",
    ")\n",
    "plot2 = figure(width=300, height=300)\n",
    "plot2.line(\"x\", \"y1\", source=source2, line_width=2, line_alpha=0.6, line_color=\"black\")\n",
    "plot2.line(\n",
    "    \"x\", \"y2\", source=source2, line_width=3, line_alpha=0.6, line_color=\"magenta\"\n",
    ")\n",
    "\n",
    "max_val = Slider(value=16.0, start=-30, end=30.0, step=1.0, title=\"Max value\")\n",
    "slope = Slider(value=40, start=-60, end=60.0, step=1.0, title=\"Slope\")\n",
    "midpoint = Slider(value=200.0, start=-400, end=400.0, step=1.0, title=\"Midpoint\")\n",
    "offset = Slider(value=0, start=-30.0, end=30.0, step=1.0, title=\"Offset\")\n",
    "\n",
    "\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        source1=source1,\n",
    "        source2=source2,\n",
    "        max_val=max_val,\n",
    "        slope=slope,\n",
    "        midpoint=midpoint,\n",
    "        offset=offset,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    const data = source1.data;\n",
    "    const x_array = data['x'];\n",
    "    const y1 = x_array.map((x) => {\n",
    "        return 1 / (1 + Math.exp((x - midpoint.value) / -slope.value)) * max_val.value + offset.value\n",
    "    })\n",
    "    source1.data['y1'] = y1\n",
    "    const y1_diff = y1.slice(1).map((value, index) => value - y1[index])\n",
    "    source2.data['y1'] = y1_diff\n",
    "    source1.change.emit();\n",
    "    source2.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "max_val.js_on_change(\"value\", callback)\n",
    "slope.js_on_change(\"value\", callback)\n",
    "midpoint.js_on_change(\"value\", callback)\n",
    "offset.js_on_change(\"value\", callback)\n",
    "\n",
    "show(\n",
    "    column(\n",
    "        column(max_val, offset, slope, midpoint),\n",
    "        gridplot([[plot1, plot2]]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400b232",
   "metadata": {},
   "source": [
    "## Logarithmic\n",
    "The logarithmic curve is pretty much just the exponential decay with some alternative assumptions and a different formula. The logarithmic function seems to closely model how the measures of spike shape, such as the area under the curve, changes as the spike frequency increases. This equation is not often used in neuroscience but I will present it here since it is relevant to the current clamp data we collected. The basic equation is: $C*log(x)$. $C$ is a scaling value and $x$ is the x position. The equation can further modified to shift is in the y direction: $C*log(x)+v$. You can technically modify the equation to shift the x value however you cannot have a $log(0)$. Shifting the x can improve curve fitting but has to be done carefully to avoid numerical issues with the log functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f136b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_func(x, vscale, offset=0, xshift=0):\n",
    "    return vscale * np.log(x-xshift) + offset\n",
    "\n",
    "\n",
    "x = np.arange(1, 301)\n",
    "source = ColumnDataSource({\"x\": x, \"y\": log_func(x, vscale=5)})\n",
    "\n",
    "plot = figure(width=400, height=400)\n",
    "\n",
    "plot.line(x, log_func(x, vscale=5), line_width=2, line_color=\"black\")\n",
    "plot.line(\"x\", \"y\", source=source, line_width=3, line_alpha=0.6, line_color=\"magenta\")\n",
    "\n",
    "yshift = Slider(start=-10, end=10, value=0, step=0.25, title=\"Y shift\")\n",
    "yscale = Slider(start=-10, end=10, value=5, step=0.5, title=\"Y scale\")\n",
    "length = Slider(start=1, end=600, value=301, step=0.25, title=\"Length\")\n",
    "\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        source=source,\n",
    "        yscale=yscale,\n",
    "        length=length,\n",
    "        yshift=yshift,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    const len = Math.round(length.value)\n",
    "    const t_length = Array.from({ length: len }, (_, i) => (1 + i))\n",
    "    const y = t_length.map(x => {\n",
    "        return (yscale.value * Math.log(x)+yshift.value)\n",
    "    })\n",
    "    source.data.x = t_length;\n",
    "    source.data.y = y;\n",
    "    source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "yshift.js_on_change(\"value\", callback)\n",
    "yscale.js_on_change(\"value\", callback)\n",
    "length.js_on_change(\"value\", callback)\n",
    "\n",
    "show(row(plot, column(yscale, length, yshift)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e046d9",
   "metadata": {},
   "source": [
    "## Fitting curves in Python\n",
    "This next section we will go over how to curve fit in Python. There are two ways to fit curves in Python: optimization-based curve fitting or polynomial curve fitting. If you want values that are interpretable you will want to use the the equations we have provided. If you just want to see if there are different curve shapes you can use the polynomial fit but you will have to run stats on all the coefficients since you will not have a 1 to 1 mapping of what coefficient represents what part of the curve shape. I recommend sticking with the optimization-based methods since it is easy to tweak to get resonable fits.\n",
    "\n",
    "Curve fitting in Python can be done using the [curve_fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html) function in the optimization module of Scipy. Curve fit has 3 main parameters you have to pass and two others that are very useful for getting good fits. You will need some function to pass that outputs an array of values. The function must accept an array of x values for the first parameter and have 1 or more parameters that are going to optimized `def test(x, param1, param2, ...)`. You will also need a x array that is passed to the function and y array that the output of the function is compared to. The curve_fit function minimizes the sum of squares alternatively known as least squares. That means it does this `sum((y-y_hat)**2)` in Python code or in mathematic notation: $\\sum_{i=1}^{n}({y_{i}-\\hat{y}_{i}})^2$ ($\\hat{y}$ is the output of the function).\n",
    "\n",
    "Since curve_fit is an optimization algorithm it works by finding the an minumum which is based on the derivative of the function. The more complex your function is, the harder it will be to fit your curve. This is because there are several minima that the algorithm could arrive at. There are three ways that you can improve the curve_fit function output. \n",
    "1. Provided start values (p0).\n",
    "2. Provided and upper and lower bounds (bounds). Some equations you must provide bounds to get a good fit since some equations cannot have zeros such as log or equations where you are trying to optimize a divisor (sigmoid function).\n",
    "3. Set your x-values in a range that is acceptable for the equation. For logarithmic equation make sure your x-value is above 0. You can do this by adding `1e-10` to your x-values is they start at 0. If your x-values are very large you can subtract the mininum and add `1e-10`.\n",
    "\n",
    "Another factort to consider when curve fitting is what values special functions like log and exponential can take. Exponentials can easily overflow or go beyond the maximum value the computer can represent. Log can take zeros. So when you set your bounds and initial values make sure they are set so that you do not get any numerical issues. If you are working with very large or very small values it can be beneficial to transform your data by scaling (multiply/divide) or shifting (add/subtract). \n",
    "\n",
    "Scipy curve_fit outputs two numpy array. One is the parameters you are optimizing and one is the covariance matrix. The diagonal of the covariance matrix gives the variance of your parameters. You can use this to calculate the confidence intervals of your parameters which gives you an idea about the reliability of the fit. You can also use the covariance matrix to calculate the confidence intervals of your curve. You can also use the covariance matrix to calculate the confidence intervals of your parameters. This relies on some more advanced math knowledge (but is really simple code). We will calculate a simple version based on code from here: https://github.com/gjpelletier/delta_method. You essential need to calculate the derivative of your equation which in our case means calculate the partial derivative for each parameter and create something called the Jacobian matrix (a matrix of partial derivatives). To calculate the Jacobian matrix you do the following for each parameter in the order they occur: \n",
    "1. Add a small value (`1e-8`) to the current parameter (i.e tau).\n",
    "2. Get a new set of y values.\n",
    "3. Subtract old y values from the new then divide by the difference between the new and old parameter.\n",
    "4. Subtract a small value from the current parameter.\n",
    "5. Get a new set of y values.\n",
    "6. Subtract old y values from the new then divide by the difference between the new and old parameter.\n",
    "7. Average the two sets of y values together and set them as column in your output matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9ad51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(func, x, popt, h=1e-8):\n",
    "    y_new = func(x, *popt)\n",
    "    # calculate derivative gradients at x (change in func(x) per change in each popt)\n",
    "    grad_new = np.empty((len(x), len(popt)))\n",
    "    for i in range(len(popt)):\n",
    "        # make a copy of popt\n",
    "        popt2 = np.copy(popt)\n",
    "        # gradient forward\n",
    "        popt2[i] = (1 + h) * popt[i]\n",
    "        y_new2 = func(x, *popt2)\n",
    "        grad_up = (y_new2 - y_new) / (popt2[i] - popt[i])\n",
    "\n",
    "        # gradient backward\n",
    "        popt2[i] = (1 - h) * popt[i]\n",
    "        y_new2 = func(x, *popt2)\n",
    "        grad_dn = (y_new2 - y_new) / (popt2[i] - popt[i])\n",
    "\n",
    "        # centered gradient is the average gradient forward and backward\n",
    "        grad_new[:, i] = (grad_up + grad_dn) / 2\n",
    "    return grad_new\n",
    "\n",
    "\n",
    "def confidence_intervals(func, x, popt, pcov, alpha=0.05):\n",
    "    # calculate variance in y_new due to each parameter and for all parameters combined\n",
    "    jac = jacobian(func, x, popt)\n",
    "    sigma = np.sqrt(np.sum(\n",
    "        (jac @ pcov) * jac, axis=1\n",
    "    )) # total variance from all popt values at each x\n",
    "    # - - -\n",
    "    # # lwr_conf and upr_conf are confidence intervals of the best-fit curve\n",
    "    nobs = len(x)\n",
    "    nparam = len(popt)\n",
    "    df = nobs - nparam\n",
    "    qt = stats.t.ppf(1 - alpha / 2, df)\n",
    "    delta_f = sigma * qt\n",
    "    y_new = func(x, *popt)\n",
    "    lwr_conf = y_new - delta_f\n",
    "    upr_conf = y_new + delta_f\n",
    "    return lwr_conf, upr_conf\n",
    "\n",
    "\n",
    "def param_ci(y, popt, pcov, alpha=0.05):\n",
    "    n = len(y)\n",
    "    p = len(popt)\n",
    "\n",
    "    dof = max(0, n - p)\n",
    "    tval = stats.t.ppf(1.0 - alpha / 2.0, dof)\n",
    "    ci = np.sqrt(np.diag(pcov)) * tval\n",
    "    return ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c8e54",
   "metadata": {},
   "source": [
    "### Curve fit exponential decay\n",
    "The exponential decay is fairly easy to curve fit. We are going to generate some synthetic data and then curve fit the data. There are three parameters that we will be fitting: amplitude, tau and yshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5637143",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(300) / 10\n",
    "y = exp_decay(x, amplitude=-15, tau=5.0, yshift=-1)\n",
    "rng = np.random.default_rng(seed=42)\n",
    "y = y + rng.uniform(size=300)\n",
    "\n",
    "# initial parameters\n",
    "my = np.min(y)\n",
    "mx = x[y > (np.min(y) / np.exp(1))][0]\n",
    "p0 = [my, mx, 0]\n",
    "ub = [0, np.inf, np.inf]\n",
    "lb = [-np.inf, 0, -np.inf]\n",
    "bounds = [lb, ub]\n",
    "\n",
    "popt, pcov = optimize.curve_fit(exp_decay, x, y, p0, bounds=bounds)\n",
    "\n",
    "lower, upper = confidence_intervals(exp_decay, x, popt, pcov)\n",
    "ci = param_ci(y, popt, pcov, alpha=0.05)\n",
    "\n",
    "fig = figure()\n",
    "fig.line(x, y, line_color=\"black\")\n",
    "fig.line(x, exp_decay(x, *popt), line_color=\"magenta\", line_width=2)\n",
    "fig.varea(x, lower, upper, color=\"orange\", alpha=0.5)\n",
    "print(f\"Amplitude: -15 vs {popt[0]:.2f} \\u00b1 {ci[0]:.2f}\")\n",
    "print(f\"Tau: 5 vs {popt[1]:.2f} \\u00b1 {ci[1]:.2f}\")\n",
    "print(f\"Y offset: -1 vs {popt[2]:.2f} \\u00b1 {ci[2]:.2f}\")\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df542ba",
   "metadata": {},
   "source": [
    "### Curve fit the double exponential decay\n",
    "The double exponential decay is harder to curve fit. We are going to generate some synthetic data and then curve fit the data. You will notice the fit is not perfect and our values are a bit different. In this case the starting values (`p0`) make a large difference. While you cannot do this in the browser, I recommend running the code in the jupyter notebook and changing the input to the db_decay equations as well as the start values and bounds. You will see that is can get pretty hard to fit well is you cannot make a good first guess. There are four parameters that we will be fitting: amplitude 1, tau 1, amplitude 2, and tau 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba3949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(300) / 10\n",
    "y = db_decay(x, -15, 5.0, -3, 2.5)\n",
    "rng = np.random.default_rng(seed=42)\n",
    "y = y + rng.uniform(size=300)\n",
    "\n",
    "# initial parameters\n",
    "my = np.min(y)\n",
    "mx = x[y > (np.min(y) / np.exp(1))][0]\n",
    "p0 = [my, mx, 0, 0]\n",
    "ub = [0, np.inf, 0, np.inf]\n",
    "lb = [-np.inf, 0, -np.inf, 0]\n",
    "bounds = [lb, ub]\n",
    "\n",
    "popt, pcov = optimize.curve_fit(db_decay, x, y, p0, bounds=bounds)\n",
    "\n",
    "lower, upper = confidence_intervals(db_decay, x, popt, pcov)\n",
    "ci = param_ci(y, popt, pcov)\n",
    "\n",
    "fig = figure()\n",
    "fig.line(x, y, line_color=\"black\")\n",
    "fig.line(x, db_decay(x, *popt), line_color=\"magenta\", line_width=2)\n",
    "fig.varea(x, lower, upper, color=\"orange\", alpha=0.5)\n",
    "print(f\"Amplitude 1: -15 vs {popt[0]:.2f} \\u00b1 {ci[0]:.2f}\")\n",
    "print(f\"Tau 1: 5 vs {popt[1]:.2f} \\u00b1 {ci[1]:.2f}\")\n",
    "print(f\"Amplitude 2: -3 vs {popt[2]:.3f} \\u00b1 {ci[0]:.2f}\")\n",
    "print(f\"Tau 2: 2.5 vs {popt[3]:.3f} \\u00b1 {ci[1]:.2f}\")\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b306d46",
   "metadata": {},
   "source": [
    "### Curve fit the sigmoid\n",
    "Similar to the double exponential decay, I have found that the sigmoid curve fitting is very sensitive to start parameters. The sigmoid curve is also sensitive to how much of the curve you have. If you have a partial curve you may not get a good fit. The sigmoid function has four parameters we will be fitting:  max_value, midpoint, slope, and offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a276c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 400)\n",
    "y = sigmoid(x, 16.0, 200.0, 40.0, 0)\n",
    "rng = np.random.default_rng(seed=42)\n",
    "y = y + rng.uniform(size=50)\n",
    "\n",
    "# initial parameters\n",
    "\n",
    "p0 = [np.max(y), x[-1]/2, 15, 0]\n",
    "ub = [np.inf, np.inf, np.inf, np.inf]\n",
    "lb = [0, 0, 1e-8, -np.inf]\n",
    "bounds = [lb, ub]\n",
    "\n",
    "popt, pcov = optimize.curve_fit(sigmoid, x, y, p0, bounds=bounds)\n",
    "\n",
    "lower, upper = confidence_intervals(sigmoid, x, popt, pcov)\n",
    "ci = param_ci(y, popt, pcov)\n",
    "\n",
    "fig = figure()\n",
    "fig.line(x, y, line_color=\"black\")\n",
    "fig.line(x, sigmoid(x, *popt), line_color=\"magenta\", line_width=2)\n",
    "fig.varea(x, lower, upper, color=\"orange\", alpha=0.5)\n",
    "print(f\"Max value: 16 vs {popt[0]:.2f} \\u00b1 {ci[0]:.2f}\")\n",
    "print(f\"Midpoint: 200 vs {popt[1]:.2f} \\u00b1 {ci[1]:.2f}\")\n",
    "print(f\"Slope: 40 vs {popt[2]:.3f} \\u00b1 {ci[0]:.2f}\")\n",
    "print(f\"Offset: 0 vs {popt[3]:.3f} \\u00b1 {ci[1]:.2f}\")\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e271823",
   "metadata": {},
   "source": [
    "### Curve fit the logarithmic curve\n",
    "The logarithmic curve is fairly easy to fit but has one requirement if you are going to use the xshift value. The xshift value must be above 0. This example shows how to correctly set the bounds for a value that cannot be above a certain value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(300,600)\n",
    "y = log_func(x, 15, 200, xshift=299)\n",
    "rng = np.random.default_rng(seed=42)\n",
    "y = y + rng.uniform(size=300)*5\n",
    "\n",
    "# # initial parameters\n",
    "xmin= np.min(x)\n",
    "xmax = np.max(x)\n",
    "ymin = np.min(y)\n",
    "ymax = np.max(y)\n",
    "if np.abs(ymin) > np.abs(ymax):\n",
    "    numerator = ymin-ymax\n",
    "    offset_est = ymax\n",
    "else:\n",
    "    offset_est = ymin\n",
    "    numerator = ymax-ymin\n",
    "divisor = max((xmax - xmin), 1)\n",
    "vscale_est = numerator / max(np.log(divisor), 1)\n",
    "p0 = [vscale_est, offset_est, 0]\n",
    "ub = [np.inf, np.inf, np.min(x)-1e-6]\n",
    "lb = [-np.inf, -np.inf, -np.inf]\n",
    "bounds = [lb, ub]\n",
    "\n",
    "popt, pcov = optimize.curve_fit(log_func, x, y, p0, bounds=bounds)\n",
    "\n",
    "lower, upper = confidence_intervals(log_func, x, popt, pcov)\n",
    "# ci = param_ci(y, popt, pcov)\n",
    "\n",
    "fig = figure()\n",
    "fig.line(x,y, line_color=\"black\")\n",
    "fig.line(x, log_func(x, *popt), line_color=\"magenta\", line_width=2)\n",
    "fig.varea(x, lower, upper, color=\"orange\", alpha=0.3)\n",
    "print(f\"Y scale: 15 vs {popt[0]:.2f} \\u00b1 {ci[0]:.2f}\")\n",
    "print(f\"Y offset: 200 vs {popt[1]:.2f} \\u00b1 {ci[1]:.2f}\")\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df468d93",
   "metadata": {},
   "source": [
    "### Curve fitting equations with some variable and some fixed parameters\n",
    "Occassionally you may want to curve fit an equation but fix one or two parameters. One way to do that is in the actual function definition. You can set a default value `def func(x, a, b, c=0)`. Since default arguments have to be set after positional arguments the curve fit function does not even see them. Another way is to pass a lambda function and set defaults for specific arguments such as `lambda x, a, b: log_func(x, a, b, c=10)`. You can also define a new function or use Python's partial functions (very similar to lambda functions)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephysbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
