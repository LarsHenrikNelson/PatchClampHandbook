{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2cbf4f",
   "metadata": {},
   "source": [
    "(distributions)=\n",
    "# Distributions\n",
    "Distributions of data are one of the first things you should look at especially if you have large sample sizes. Think mini amplitude, IEI, and rise rate. Most the statistical analyses we use make assumptions about the distribution of our data yet rarely do papers show distributions of their data. Part of this is because most papers just average per mouse or per cell. Additionally, many scientists have a small n, the number samples, which makes it hard to see what the distribution is. In this section we will cover some ways to understand the distribution of your data even for small sample sizes and how to visualize your data. We will also cover what a distribution is and cover some distributions I think that you should know.\n",
    "\n",
    "Most other distributions contain the equivalent of the mean but the value is quiet different from the mean. When taking the mean of non-gaussian distributions you are not get the true central tendency of your data and may be measuring the effect of outliers rather than a true shift in the distribution of data. In the case of some thing like a beta distribution the mean may not accurately describe the data.\n",
    "\n",
    "Distributions can show counts or probabilities. When you make a histogram or kernel density estimate (KDE) you are creating a distribution of your data (technically a non-parametric distribution). Most often in statistical text books you will see something called probability density functions (PDF) for continuous variables or probability mass functions (PMF) for discrete variables. These are parametric distributions becuase that have parameters that describe the distribution. The important thing about these \"functions\" is that the distribution of values you get from the functions integrates (i.e. the area under the curve) to 1 for the when you take the distribution from negative limit to positive limit. When you put in a single number with some parameters you get a number out called a likelihood. \n",
    "\n",
    "The last thing to note is that finding the distribution that fits your data describes your data but does not tell you how or why it was generated that way. The how and why questions are not something we will get at here and are what computational neuroscientists study.\n",
    "\n",
    "You will also often see cumulative distribution functions (CDFs). The CDF is the just the integral of the PDF and the PDF the derivative of the CDF. When taking the integral of a continous function you can just use a Numpy [cumsum](https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html) and multiply by your delta X to get the CDF if you have evenly spaced samples. For the area under the curve you can use a variety of functions provided by scipy such as the [trapezoid](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.trapezoid.html#scipy.integrate.trapezoid), [simpson](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.trapezoid.html#scipy.integrate.simpson), or the [romberg](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.romb.html#scipy.integrate.romb). Below we will plot the PDF and CDF for each of the distributions.\n",
    "\n",
    "For this tutorial, there are interactive samples. Not all of these examples not use the preferred way of using and working with distributions in Python. The recommended way is to use the Scipy [stats](https://docs.scipy.org/doc/scipy/reference/stats.html) module. The stats module distrbutions provide a lot of useful features but can be a little bit intimidating to start with. Scipy stats distributions can provide PDF/PMFs, CDFs, PPFs and fit distributions to your data using maxmimum likelihood.\n",
    "\n",
    "If you want to learn a little more about PDFs and PMFs I suggest watching Very Normal on Youtube.\n",
    "\n",
    "First we are going to look over some of the data we tend to collect in electrophysiology experiments and see what the distribution of data looks like. Then we are going to go over some specific distributions and see which distributions look most like our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790146f1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.layouts import column, row, layout\n",
    "from bokeh.models import ColumnDataSource, CustomJS, Select, Slider, Whisker\n",
    "from bokeh.plotting import figure\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "cwd = Path.cwd().parent / \"data/pv\"\n",
    "df = pd.read_csv(cwd / \"mini_data.csv\")\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2ac44",
   "metadata": {},
   "source": [
    "## Examining distributions in your data.\n",
    "The most common method to look at distributions of data is to use a histogram, KDE (note: violin plot is just a KDE) and somewhat the box plot. My preferred method is to use a kernel density estimate (KDE). These methods are non-parametric in that they technically do not have any parameters other than your data to create a distribution. The reason I prefer KDEs to histograms is that you can interpolate where you do not have data. KDEs are essentially the non-parametric probabability density function like the ones we already covered. I will show you how create a histogram and KDE in Python and then we will use the KDE to compare our data to the distributions above.\n",
    "\n",
    "There are a couple things you should look for in your data.\n",
    "1. What are the bounds? Are -infinity to infinity, 0 to 1, 0 to infinity? Data that is bounded is often not normally distributed.\n",
    "2. Is there a skew to your data? You should take note if your distribution has a long tail to the right (right skew) or a long tail to the left (left skew)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69240df6",
   "metadata": {},
   "source": [
    "### How to Create a Histogram\n",
    "To create a histogram in Python the easiest way is to use Numpy's [histogram](https://numpy.org/doc/stable/reference/generated/numpy.histogram.html). For this example we will use 50 bins, however Numpy has a pretty good algorithm for automatically selecting bins. One setting we will use is the density equals true. This will ensure that each bin shows the likelihood so that is matches with the KDE. If you just want to plot a histogram in other popular plotting packags such as Matplotlib or Seaborn, they have modules where you can just put in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist(data):\n",
    "    hist, edges = np.histogram(\n",
    "        data, range=(data.min() - 1e-6, data.max() + 1e-6), density=True, bins=50\n",
    "    )\n",
    "    return hist, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb5e68",
   "metadata": {},
   "source": [
    "### How to Create a Kernel Density Estimate (KDE)\n",
    "To create a KDE in Python we are going to use Scipy's [gaussian_kde](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html). There are other implementations of KDEs such  [Scikit Learn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html) and [KDEpy](https://kdepy.readthedocs.io/en/latest/index.html).\n",
    "\n",
    "KDE stands for Kernel Density Estimate. KDE is a non-parametric density function. There is no predefine function to create the PDF from your data, you create a unique one from your data. The aim of the KDE is not to represent the density function of your data exactly as it is, like a histogram but, to represent it as if you had an infinitely large sample size. KDEs can represent your data as it is by setting a feature called the bandwidth. The only real subjective feature of a KDE is how much to \"smooth\" your probability distribution that the KDE outputs. \n",
    "\n",
    "The KDE works by essiantially putting a gaussian distribution (or other distribution) around each point of data and then sums them up. Depending on how smooth we want the distribution we can increase or decrease the bandwidth. For the gaussian distribution the bandwidth is related to the standard deviation. The larger the bandwidth the smoother the KDE and vice versa. There are some algorithms to precompute the bandwidth called Silvermans, Scotts and Improved Sheather-Jones (ISJ) if you don't want to compute the bandwidth yourself however, Scipy does not provide the ISJ algorithm. There are also different ways to compute the KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c0cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kde(data):\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    padding = (max_val - min_val) * 0.1  # Add 10% padding\n",
    "    grid_min = min_val - padding\n",
    "    grid_max = max_val + padding\n",
    "    grid_min = max(grid_min, 0)\n",
    "    positions = np.linspace(grid_min, grid_max, num=248)\n",
    "    kernel = stats.gaussian_kde(data)\n",
    "    y = kernel(positions)\n",
    "    return positions, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f78bd7",
   "metadata": {},
   "source": [
    "## How to create a box plot\n",
    "A box consists of a box that goes from the 0.25 quartile to the 0.75 quartile. There is a line in the middle that is the 0.5 or median. There are whiskers that can be plotted one of two ways. One way is to use the interquartile range (IQR) as is or you can calculate use the minimum point that is closest to the IQR which is useful if the IQR goes beyond any values in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c953bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box(data):\n",
    "    q1, q2, q3 = np.quantile(data, q=[0.25, 0.5, 0.75])\n",
    "    iqr = q3-q1\n",
    "    upper = q3+1.5*iqr\n",
    "    lower = q1-1.5*iqr\n",
    "    wiskhi = data.loc[(q3 <= data) & (data <= upper)]\n",
    "    wiskhi = [q3 if len(wiskhi) == 0 else wiskhi.max()]\n",
    "    wisklo = data[(lower <= data) & (data <= q1)]\n",
    "    wisklo = [q1 if len(wisklo) == 0 else wisklo.min()]\n",
    "    return q1, q2, q3, wiskhi, wisklo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4377ed5",
   "metadata": {},
   "source": [
    "Below we will process all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526ff52",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "box_dict = {}\n",
    "kde_dict = {}\n",
    "hist_dict = {}\n",
    "\n",
    "columns = [\n",
    "    \"Est Tau (ms)\",\n",
    "    \"Rise Time (ms)\",\n",
    "    \"Rise Rate (pA/ms)\",\n",
    "    \"Amplitude (pA)\",\n",
    "    \"IEI (ms)\",\n",
    "]\n",
    "\n",
    "for i in columns:\n",
    "    data = df[i]\n",
    "    if i == \"IEI (ms)\":\n",
    "        data = data[data > 0]\n",
    "    for f, n in [(lambda x: x, \"identity\"), (np.sqrt, \"sqrt\"), (np.log10, \"log\")]:\n",
    "        q1, q2, q3, wiskhi, wisklo = box(f(data))\n",
    "        box_dict[f\"{i}_q1_{n}\"] = [q1]\n",
    "        box_dict[f\"{i}_q2_{n}\"] = [q2]\n",
    "        box_dict[f\"{i}_q3_{n}\"] = [q3]\n",
    "        box_dict[f\"{i}_u_{n}\"] = wiskhi\n",
    "        box_dict[f\"{i}_l_{n}\"] = wisklo\n",
    "\n",
    "        positions, y = kde(f(data))\n",
    "        kde_dict[f\"{i}_x_{n}\"] = positions\n",
    "        kde_dict[f\"{i}_y_{n}\"] = y\n",
    "\n",
    "        hist_data, edges = np.histogram(\n",
    "            data, range=(data.min() - 1e-6, data.max() + 1e-6), density=True, bins=50\n",
    "        )\n",
    "        hist_dict[f\"{i}_hist_{n}\"] = hist_data\n",
    "        hist_dict[f\"{i}_left_{n}\"] = edges[:-1]\n",
    "        hist_dict[f\"{i}_right_{n}\"] = edges[1:]\n",
    "        \n",
    "box_dict[\"q1\"] = box_dict[\"Amplitude (pA)_q1_identity\"]\n",
    "box_dict[\"q2\"] = box_dict[\"Amplitude (pA)_q2_identity\"]\n",
    "box_dict[\"q3\"] = box_dict[\"Amplitude (pA)_q3_identity\"]\n",
    "box_dict[\"u\"] = box_dict[\"Amplitude (pA)_u_identity\"]\n",
    "box_dict[\"l\"] = box_dict[\"Amplitude (pA)_l_identity\"]\n",
    "\n",
    "kde_dict[\"kde_y\"] = kde_dict[\"Amplitude (pA)_y_identity\"]\n",
    "kde_dict[\"x\"] = kde_dict[\"Amplitude (pA)_x_identity\"]\n",
    " \n",
    "hist_dict[\"left\"] = hist_dict[\"Amplitude (pA)_left_identity\"]\n",
    "hist_dict[\"right\"] = hist_dict[\"Amplitude (pA)_right_identity\"]\n",
    "hist_dict[\"hist_y\"] = hist_dict[\"Amplitude (pA)_hist_identity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa01cee7",
   "metadata": {},
   "source": [
    "### Examining the data\n",
    "Below are some plots of our data. There are three common transforms;  sqrt, log10 and the negative inverse. These transforms are used to correct for right-skew in your data. The functions correcting for skewness go from light correction to heavy correction in this order: sqrt &rarr; log10 &rarr; negative inverse. You can also see that since we do not transform the data before any corrections the histogram can look pretty weird. Part of not correcting the histogram before is due to limitations of how this book is written and published. However, you can see how the transform \"resizes\" the bins, putting greater emphasis (larger bin size) on smaller values. All these transforms decrease the effects of outliers. Additionally, I also plot the mean, median and mode. These are all measures of central tendency. You can see how the transform shifts the mean and median towards the mode. However the mean, median and mode almost never fully align. This is because we have truncated distributions in addition to skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d6ea2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "menu = Select(title=\"Variables\", value=\"IEI (ms)\", options=columns)\n",
    "kde_source = ColumnDataSource(kde_dict)\n",
    "hist_source = ColumnDataSource(hist_dict)\n",
    "box_source = ColumnDataSource(box_dict)\n",
    "transform = Select(\n",
    "    title=\"Transform\",\n",
    "    value=\"identity\",\n",
    "    options=[\"identity\", \"sqrt\", \"log\"],\n",
    ")\n",
    "\n",
    "hist_figure = figure(height=200, width=250, title=\"Histogram\")\n",
    "hist_data = hist_figure.quad(\n",
    "    top=\"hist_y\",\n",
    "    bottom=0,\n",
    "    left=\"left\",\n",
    "    right=\"right\",\n",
    "    line_color=\"white\",\n",
    "    alpha=0.5,\n",
    "    source=hist_source,\n",
    ")\n",
    "\n",
    "kde_figure = figure(height=200, width=250, title=\"KDE\")\n",
    "line = kde_figure.line(x=\"x\", y=\"kde_y\", source=kde_source, line_color=\"black\")\n",
    "\n",
    "box_figure = figure(height=200, width=250, title=\"Box\", y_range=(box_source.data[\"l\"][0]*0.8, box_source.data[\"u\"][0]*1.1))\n",
    "whisker = Whisker(base=0, upper=\"u\", lower=\"l\", source=box_source)\n",
    "whisker.upper_head.size = whisker.lower_head.size = 20\n",
    "box_figure.add_layout(whisker)\n",
    "box_figure.vbar(0, 1, \"q2\", \"q3\", source=box_source, color=\"orange\", line_color=\"black\")\n",
    "box_figure.vbar(0, 1, \"q1\", \"q2\", source=box_source, color=\"orange\", line_color=\"black\")\n",
    "\n",
    "central_tendency = {}\n",
    "for i in columns:\n",
    "    data = df[i]\n",
    "    if i == \"IEI (ms)\":\n",
    "        data = data[data > 0]\n",
    "    index = kde_source.data[f\"{i}_y_identity\"].argmax()\n",
    "    central_tendency[f\"{i}_identity\"] = [\n",
    "        np.mean(data),\n",
    "        np.median(data),\n",
    "        kde_source.data[f\"{i}_x_identity\"][index],\n",
    "    ]\n",
    "    sqrt_data = np.sqrt(data)\n",
    "    index = kde_source.data[f\"{i}_y_sqrt\"].argmax()\n",
    "    central_tendency[f\"{i}_sqrt\"] = [\n",
    "        np.mean(sqrt_data),\n",
    "        np.median(sqrt_data),\n",
    "        kde_source.data[f\"{i}_x_sqrt\"][index],\n",
    "    ]\n",
    "    log_data = np.log10(data)\n",
    "    index = kde_source.data[f\"{i}_y_log\"].argmax()\n",
    "    central_tendency[f\"{i}_log\"] = [\n",
    "        np.mean(log_data),\n",
    "        np.median(log_data),\n",
    "        kde_source.data[f\"{i}_x_log\"][index],\n",
    "    ]\n",
    "central_tendency[\"x\"] = central_tendency[\"Amplitude (pA)_identity\"]\n",
    "central_tendency[\"color\"] = [\"orange\", \"blue\", \"magenta\"]\n",
    "central_tendency = ColumnDataSource(central_tendency)\n",
    "\n",
    "vspans = kde_figure.vspan(x=\"x\", color=\"color\", source=central_tendency)\n",
    "\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        hist_source=hist_source,\n",
    "        kde_source=kde_source,\n",
    "        box_source=box_source,\n",
    "        box_figure=box_figure,\n",
    "        central_tendency=central_tendency,\n",
    "        menu=menu,\n",
    "        transform=transform,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    hist_source.data.hist_y = hist_source.data[`${menu.value}_hist_${transform.value}`]; \n",
    "    hist_source.data.left = hist_source.data[`${menu.value}_left_${transform.value}`]; \n",
    "    hist_source.data.right = hist_source.data[`${menu.value}_right_${transform.value}`]; \n",
    "    kde_source.data.kde_y = kde_source.data[`${menu.value}_y_${transform.value}`];\n",
    "    kde_source.data.x = kde_source.data[`${menu.value}_x_${transform.value}`];\n",
    "    central_tendency.data.x = central_tendency.data[`${menu.value}_${transform.value}`];\n",
    "    box_source.data.q1 = box_source.data[`${menu.value}_q1_${transform.value}`];\n",
    "    box_source.data.q2 = box_source.data[`${menu.value}_q2_${transform.value}`];\n",
    "    box_source.data.q3 = box_source.data[`${menu.value}_q3_${transform.value}`];\n",
    "    box_source.data.u = box_source.data[`${menu.value}_u_${transform.value}`];\n",
    "    box_source.data.l = box_source.data[`${menu.value}_l_${transform.value}`];\n",
    "    box_figure.y_range.start = box_source.data.l[0] - box_source.data.l[0] * 0.5\n",
    "    box_figure.y_range.end = box_source.data.u[0] * 1.1\n",
    "    central_tendency.change.emit();\n",
    "    kde_source.change.emit();\n",
    "    hist_source.change.emit();\n",
    "    box_source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "menu.js_on_change(\"value\", callback)\n",
    "transform.js_on_change(\"value\", callback)\n",
    "\n",
    "show(column(row(menu, transform), row(hist_figure, kde_figure, box_figure)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3875ce",
   "metadata": {},
   "source": [
    "## Gaussian distribution\n",
    "In terms of data that we collect in electrophysiology the gaussian distribution is actually not that common but most of the time we assume our data follows a gaussian distribution.  Some non-gaussian distributions, such as the beta and von Mises (radians) distributions, can coverge to a normal distribution with certain parameters. The are problem when you assume that non-gaussian data follows a gaussian distribution AND you do not check your statistical models. We will cover some of this later in this chapter. The gaussian distribution has two parameters, the mean and standard deviation. The mean is what is called a location parameter and shifts the distribution around. The standard deviation is related to the spread of data symmetrically around the mean. Technically the gaussian distribution is unbounded. This means that you can get any value from -$\\infty$ to +$\\infty$. However, due to limits that we have on computers we generally don't show all the values up to +/- $\\infty$, but only up to a couple standard deviations past the mean on each side. The equation for the gaussian PDF is: $$\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\frac{(x-\\mu)^2}{2\\sigma^2}$$\n",
    "\n",
    "Below you can see how changing the mean and standard deviation changes the magenta distribution relative to the grey reference distribution. We plot both the PDF and the CDF (the integral of the PDF). There are a couple things to note. The area under the curve of the PDF will always equal 1. Changing the standard deviation decreases the likelihood of getting any value but increases the range of likely values you can get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17044145",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "mu = 0\n",
    "std = 1\n",
    "x = np.linspace(mu - std * 4, mu + 4 * std, num=400)\n",
    "y = np.exp(-((x - mu) ** 2) / (2 * std**2)) / np.sqrt(2 * np.pi * std**2)\n",
    "source = ColumnDataSource(\n",
    "    {\n",
    "        \"x\": x,\n",
    "        \"y\": y,\n",
    "        \"yc\": np.cumsum(y) * 0.020050125313283207,\n",
    "    }\n",
    ")\n",
    "pdf = figure(height=250, width=350, title=\"PDF\")\n",
    "pline = pdf.line(\"x\", \"y\", source=source, color=\"magenta\")\n",
    "pline1 = pdf.line(x, y, color=\"grey\")\n",
    "cdf = figure(height=250, width=350, title=\"CDF\")\n",
    "cline = cdf.line(\"x\", \"yc\", source=source, color=\"magenta\")\n",
    "cline1 = cdf.line(source.data[\"x\"], source.data[\"yc\"], color=\"grey\")\n",
    "\n",
    "mu = Slider(start=-10, end=10, value=0, step=0.5, title=\"Mu (mean)\")\n",
    "std = Slider(start=0.1, end=10, value=1, step=0.5, title=\"Sigma (std)\")\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        source=source,\n",
    "        mu=mu,\n",
    "        std=std,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    const arr = [];\n",
    "    const start = mu.value-std.value*4\n",
    "    const end = mu.value+std.value*4\n",
    "    const step = (end - start) / (400 - 1);\n",
    "\n",
    "    for (let i = 0; i < 400; i++) {\n",
    "        arr.push(start + step * i);\n",
    "    }\n",
    "    const temp_y = arr.map(x => {\n",
    "        const coefficient = 1 / Math.sqrt(2 * Math.PI * Math.pow(std.value, 2));\n",
    "        const exponent = -Math.pow((x - mu.value), 2) / (2 * Math.pow(std.value, 2));\n",
    "        return coefficient * Math.exp(exponent);\n",
    "    })\n",
    "    const cumsum = [temp_y[0]]\n",
    "    for (let i = 1; i < 400; i++) {\n",
    "        cumsum.push((cumsum[i-1] + temp_y[i]));\n",
    "    }\n",
    "    source.data.y = temp_y;\n",
    "    source.data.x = arr;\n",
    "    source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "mu.js_on_change(\"value\", callback)\n",
    "std.js_on_change(\"value\", callback)\n",
    "show(column(row(mu, std), row(pdf, cdf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f9441",
   "metadata": {},
   "source": [
    "First we will start by looking at some of the data we previously collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc504f0",
   "metadata": {},
   "source": [
    "## Lognormal distribution\n",
    "The lognormal distribution is a distribution where if you log transform the data you will get the normal distribution. If your data has a right skew (long tail to the right) your data may follow a lognormal distribution. You may ask why not just log transform the data? Log transforming means your data will no longer be in the same scale which makes downstream interpretations more complicated. The lognormal distribution is very common in biological sciences. Things like rates, lengths, concentrations and energies often follow a lognormal distribution. The lognormal distribution bounds are (0,+$\\infty$), The () brackets are exclusive which means that you can never have a 0 in the distribution since any log of 0 is undefined. The PDF of the lognormal distribution is: $$\\frac{1}{x\\sigma\\sqrt{2\\pi\\sigma^2}}\\exp(\\frac{-(ln(x)-\\mu)^2}{2\\sigma^2})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce1fbe8",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "mu = 0\n",
    "std = 1\n",
    "x = np.linspace(0.000001, np.exp(mu) + 4 * np.exp(std), num=400)\n",
    "y = np.exp(-((np.log(x) - mu) ** 2) / (2 * std**2)) / (\n",
    "    x * std * np.sqrt(2 * np.pi * std**2)\n",
    ")\n",
    "source = ColumnDataSource(\n",
    "    {\n",
    "        \"x\": x,\n",
    "        \"y\": y,\n",
    "        \"yc\": np.cumsum(y) * 0.020050125313283207,\n",
    "    }\n",
    ")\n",
    "pdf = figure(height=250, width=350, title=\"PDF\")\n",
    "pline = pdf.line(\"x\", \"y\", source=source, color=\"magenta\")\n",
    "pline1 = pdf.line(x, y, color=\"grey\")\n",
    "cdf = figure(height=250, width=350, title=\"CDF\")\n",
    "cline = cdf.line(\"x\", \"yc\", source=source, color=\"magenta\")\n",
    "cline1 = cdf.line(source.data[\"x\"], source.data[\"yc\"], color=\"grey\")\n",
    "\n",
    "mu = Slider(start=0, end=10, value=0, step=0.5, title=\"Mu (mean)\")\n",
    "std = Slider(start=0.25, end=10, value=1, step=0.25, title=\"Sigma (std)\")\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        source=source,\n",
    "        mu=mu,\n",
    "        std=std,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    const arr = [];\n",
    "    const start = 0.00001;\n",
    "    const end = Math.exp(mu.value)+Math.exp(std.value)*4;\n",
    "    const step = (end - start) / (400 - 1);\n",
    "\n",
    "    for (let i = 0; i < 400; i++) {\n",
    "        arr.push(start + step * i);\n",
    "    }\n",
    "    const temp_y = arr.map(x => {\n",
    "        const coefficient = 1 / (Math.sqrt(2 * Math.PI * Math.pow(std.value, 2))*x*std.value);\n",
    "        const exponent = -Math.pow((Math.log(x) - mu.value), 2) / (2 * Math.pow(std.value, 2));\n",
    "        return coefficient * Math.exp(exponent);\n",
    "    })\n",
    "    const cumsum = [temp_y[0]]\n",
    "    for (let i = 1; i < 400; i++) {\n",
    "        cumsum.push((cumsum[i-1] + temp_y[i]));\n",
    "    }\n",
    "    source.data.y = temp_y;\n",
    "    source.data.x = arr;\n",
    "    source.data.yx = cumsum;\n",
    "    source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "mu.js_on_change(\"value\", callback)\n",
    "std.js_on_change(\"value\", callback)\n",
    "show(column(row(mu, std), row(pdf, cdf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f67379",
   "metadata": {},
   "source": [
    "## Gamma distribution\n",
    "The gamma distribution is another important distribution for neuroscience data. The gamma distribution is used to model waiting times and rates. A lot of data we collect are rates, such as mini or spike frequency. If your data has a right skew (long tail to the right) your data may follow a gamma distribution. Interestingly, gamma distributed variables often maximize the information content of a signal which is useful in the brain. The gamma distribution is the generalization the exponential, Erlang and chi-squared distribution. The gamma distribution is defined by a shape, $\\alpha$, and scale, $\\theta$, parameter. Similar to the lognormal distribution the gamma distribution bounds are (0,+$\\infty$). The PDF of the gamma distribution is: $$\\frac{1}{\\Gamma(\\alpha)\\theta^{\\alpha}}x^{\\alpha-1}e^{-x/\\theta}$$\n",
    "\n",
    "$\\Gamma$ is the Gamma function is the factorial function that is generalized to complex numbers (except non-positive complex numbers)\n",
    "\n",
    "You can also get the mean: $\\mu=\\alpha\\theta$ and variance: $\\sigma^{2}=\\alpha\\theta^2$ of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200bc94",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "shapes = [1.0, 1.25, 1.25, 2.0, 3.0, 5.0, 7.5]\n",
    "scales = [2.0, 2.0, 3.0, 2.0, 2.0, 1.0, 1.0]\n",
    "x = np.linspace(0, 20, num=400)\n",
    "keys = []\n",
    "glines = {}\n",
    "cdflines = {}\n",
    "gamma_fig = figure(height=250, width=350)\n",
    "gamma_cdf = figure(height=250, width=350)\n",
    "\n",
    "for i, j in zip(shapes, scales):\n",
    "    pdf = stats.gamma.pdf(x, i, scale=j)\n",
    "    cdf = stats.gamma.cdf(x, i, scale=j)\n",
    "    k = f\"a={i} theta={j}\"\n",
    "    keys.append(k)\n",
    "    glines[k] = gamma_fig.line(x, pdf, color=\"grey\")\n",
    "    cdflines[k] = gamma_cdf.line(x, cdf, color=\"grey\")\n",
    "select = Select(title=\"Option:\", value=\"a=1.0 theta=2.0\", options=keys)\n",
    "glines[\"a=1.0 theta=2.0\"].glyph.line_color = \"magenta\"\n",
    "glines[\"a=1.0 theta=2.0\"].glyph.line_width = 3\n",
    "cdflines[\"a=1.0 theta=2.0\"].glyph.line_color = \"magenta\"\n",
    "cdflines[\"a=1.0 theta=2.0\"].glyph.line_width = 3\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        glines=glines,\n",
    "        cdflines=cdflines,\n",
    "        select=select,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    for (let key in glines) {\n",
    "        if (select.value == key) {\n",
    "            glines[key].glyph.line_color = 'magenta';\n",
    "            cdflines[key].glyph.line_color = 'magenta';\n",
    "            glines[key].glyph.line_width = 3;\n",
    "            cdflines[key].glyph.line_width = 3;\n",
    "        }\n",
    "        else {\n",
    "            glines[key].glyph.line_color = 'grey';\n",
    "            cdflines[key].glyph.line_color = 'grey';\n",
    "            glines[key].glyph.line_width = 1;\n",
    "            cdflines[key].glyph.line_width = 1;\n",
    "        }\n",
    "    }\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "select.js_on_change(\"value\", callback)\n",
    "show(column(select, row(gamma_fig, gamma_cdf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6badd2",
   "metadata": {},
   "source": [
    "## Beta distribution\n",
    "The beta distribution is used to model data that has finite intervals, and proportions and percentages. Some bounds values you might see in neuroscience are correlations (-1,1) and PPR (if you use the percentage formula and not the ratio). The beta distribution's bound are (0,1) so if you have bounded data that is not between those you can transform it.  The beta distribution is defined by a shape, $\\alpha$, and scale, $\\beta$, parameter. The PDF of the beta distribution is: $$\\frac {x^{\\alpha -1}(1-x)^{\\beta -1}}{\\frac {\\Gamma (\\alpha )\\Gamma (\\beta )}{\\Gamma (\\alpha +\\beta ) (\\alpha ,\\beta )}}$$\n",
    "$\\Gamma$ is the Gamma function is the factorial function that is generalized to complex numbers (except non-positive complex numbers). The denominator of the PDF is a normalization factor to ensure the distribution has a total probability of 1. \n",
    "\n",
    "One thing that you will notice is that the beta distribution can take on a wide range of shapes. This makes is very powerful for oddly shaped distributions. It can even even model normally distribution and uniformally distribution data when $\\alpha$=$\\beta$\n",
    "\n",
    "You can also get the mean: $\\mu=\\frac{\\alpha}{\\alpha+\\beta}$ and variance: $\\sigma^{2}=\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$ of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c533b5",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "shapes = [0.5, 5, 1, 2, 5, 1.25, 1, 5]\n",
    "scales = [0.5, 1, 3, 2, 2, 5, 1, 5]\n",
    "x = np.linspace(0, 1, num=400)\n",
    "keys = []\n",
    "blines = {}\n",
    "cdflines = {}\n",
    "beta_fig = figure(height=250, width=350)\n",
    "beta_cdf = figure(height=250, width=350)\n",
    "\n",
    "for i, j in zip(shapes, scales):\n",
    "    pdf = stats.beta.pdf(x, i, j)\n",
    "    cdf = stats.beta.cdf(x, i, j)\n",
    "    k = f\"a={i} b={j}\"\n",
    "    keys.append(k)\n",
    "    blines[k] = beta_fig.line(x, pdf, color=\"grey\")\n",
    "    cdflines[k] = beta_cdf.line(x, cdf, color=\"grey\")\n",
    "select = Select(title=\"Option:\", value=\"a=0.5 b=0.5\", options=keys)\n",
    "blines[\"a=0.5 b=0.5\"].glyph.line_color = \"magenta\"\n",
    "blines[\"a=0.5 b=0.5\"].glyph.line_width = 3\n",
    "cdflines[\"a=0.5 b=0.5\"].glyph.line_color = \"magenta\"\n",
    "cdflines[\"a=0.5 b=0.5\"].glyph.line_width = 3\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        blines=blines,\n",
    "        cdflines=cdflines,\n",
    "        select=select,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    for (let key in blines) {\n",
    "        if (select.value == key) {\n",
    "            blines[key].glyph.line_color = 'magenta';\n",
    "            cdflines[key].glyph.line_color = 'magenta';\n",
    "            blines[key].glyph.line_width = 3;\n",
    "            cdflines[key].glyph.line_width = 3;\n",
    "        }\n",
    "        else {\n",
    "            blines[key].glyph.line_color = 'grey';\n",
    "            cdflines[key].glyph.line_color = 'grey';\n",
    "            blines[key].glyph.line_width = 1;\n",
    "            cdflines[key].glyph.line_width = 1;\n",
    "        }\n",
    "    }\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "select.js_on_change(\"value\", callback)\n",
    "show(column(select, row(beta_fig, beta_cdf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83631be7",
   "metadata": {},
   "source": [
    "## Poisson distribution\n",
    "The Poisson distribution is a discete distribution used model the probability of a certain number of events occuring within a time frame thus it models the rate of events. The poisson distribution is similar to the gamma distribution kind of like the discrete cousin of the gamma distribution. The Poisson distribution has an important related feature called dispersion or the fano factor: $\\frac{\\sigma^2}{\\mu} = 1$, which is also related to the coefficient of variation: $\\frac{\\sigma}{\\mu} = \\lambda^{-\\frac{1}{2}}$. The fano factor is often used to check the variability of a spike train or even minis. The closer the fano factor is to 0, then the more predicable the process is. The farther you go above 1 the more clustered your events will be suggesting that there is some reason you events cluster (make sense for neurons that can fire spike trains). The Poisson PMF is defined by $\\lambda, or the rate, and k, the integer value. The Poisson PMF (not PDF since it is distrete) is: $${\\frac {\\lambda ^{k}e^{-\\lambda }}{k!}}$$ Since the Poisson distribution is a discrete distribution you actually get probabilities out instead of likelihoods. One thing to note is that the plot uses scatter instead of a line. This is to indicate that the Poisson distribution can only take integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c6f01",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "lam = 1\n",
    "k = np.arange(0, 21)\n",
    "y = (lam**k * np.exp(-lam)) / np.array([math.factorial(i) for i in k])\n",
    "source = ColumnDataSource({\"k\": k, \"y\": y, \"yc\": np.cumsum(y)})\n",
    "pdf = figure(height=250, width=350, title=\"PDF\")\n",
    "pline = pdf.scatter(\"k\", \"y\", source=source, color=\"magenta\")\n",
    "pline1 = pdf.scatter(k, y, color=\"grey\")\n",
    "cdf = figure(height=250, width=350, title=\"CDF\")\n",
    "cline = cdf.scatter(\"k\", \"yc\", source=source, color=\"magenta\")\n",
    "cline1 = cdf.scatter(source.data[\"k\"], source.data[\"yc\"], color=\"grey\")\n",
    "\n",
    "lam = Slider(start=0.5, end=10, value=1, step=0.25, title=\"Lambda\")\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        source=source,\n",
    "        lam=lam,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    const arr = [];\n",
    "    for (let i = 0; i < 21; i++) {\n",
    "        let result = 1;\n",
    "        for (let k = 2; k <= i; k++) {\n",
    "            result *= k;\n",
    "        }\n",
    "        const val = ((lam.value**i)*(Math.exp(-1*lam.value)))/result;\n",
    "        arr.push(val);\n",
    "    }\n",
    "    const cumsum = [arr[0]]\n",
    "    for (let i = 1; i < 21; i++) {\n",
    "        cumsum.push((cumsum[i-1] + arr[i]));\n",
    "    }\n",
    "    source.data.y = arr;\n",
    "    source.data.yc = cumsum\n",
    "    source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "lam.js_on_change(\"value\", callback)\n",
    "show(column(row(lam), row(pdf, cdf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c820ec",
   "metadata": {},
   "source": [
    "With the Poisson distribution you could model how many minis or spikes are likely to occur in a time frame based on a rate. One interesting thing about the Poisson distribution is that the Poisson distribution expects events to be randomly distributed over time. To model the interevent intervals between minis you can use the continuous exponential distribution to draw random IEIs. If you can have overlapping events or events with infinitesimally small IEIs then you can just use a uniform distribution to choose points between your start and end time (i.e. 0 and 1 second). Below is an example of how randomly choosen points over a time period can lead to a Poisson count distribution and a exponential distribution of IEIs. Additionally, you can see if we take the variance divided by the mean we get a value close to one. Think about what processes might lead you have events non-randomly distributed in time. What if the fano factor differs between your experiments groups? Can you use this information to inform your interpretation of the data and future experiments? Remember that when you put TTX in your bath you are blocking any evoked activity therefore your events should be randomly distributed over time.\n",
    "\n",
    "Below you can see an example of how a Poisson distribution for a mini rate of 3 Hz looks like, the IEI distribution that is generated from it and the distribution of events over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7558f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "ieis = []\n",
    "mini_train = []\n",
    "for i in range(30):\n",
    "    times = rng.uniform(0, 10000, size = 30)\n",
    "    times = np.sort(times)\n",
    "    ieis.extend(np.diff(np.sort(times)))\n",
    "    mini_train.extend(times+i*10000)\n",
    "\n",
    "pdf = figure(height=250, width=350, title=\"PDF\")\n",
    "lam = 3\n",
    "k = np.arange(0, 21)\n",
    "y = (lam**k * np.exp(-lam)) / np.array([math.factorial(i) for i in k])\n",
    "pline = pdf.scatter(k, y, color=\"magenta\")\n",
    "\n",
    "fig = figure(height=250, width=350)\n",
    "hist, bins = np.histogram(ieis, bins=50, density=True)\n",
    "hist_data = fig.quad(\n",
    "    top=hist,\n",
    "    bottom=0,\n",
    "    left=bins[:-1],\n",
    "    right=bins[1:],\n",
    "    alpha=0.5,\n",
    ")\n",
    "loc, scale = stats.expon.fit(ieis)\n",
    "x = np.linspace(stats.expon.ppf(0.0001, scale=scale), stats.expon.ppf(0.9999, scale=scale), 100)\n",
    "\n",
    "line = fig.line(x, stats.expon.pdf(x, scale=scale), line_color=\"black\")\n",
    "time_series = figure(height=250)\n",
    "x = np.column_stack((np.zeros(len(mini_train)),  rng.random(len(mini_train))))\n",
    "y = np.repeat(mini_train, 2).reshape((-1,2))\n",
    "ts_line = time_series.multi_line([i for i in y], [i for i in x])\n",
    "\n",
    "temp = np.zeros(10000*30)\n",
    "mini_train = np.array(mini_train).astype(int)\n",
    "temp[mini_train] = 1\n",
    "print(f\"Fano factor: {np.var(temp)/np.mean(temp)}\")\n",
    "\n",
    "show(layout([[pdf, fig], [time_series]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4769c13e",
   "metadata": {},
   "source": [
    "### Fitting your data to a distribution\n",
    "In this next section we will look at how data fits to different distributions. Due to limitations of the format we will stick with IEI data however, I encourage to you modify the code below on either the other data we have or better yet on your own data. You should note that it is quite hard to accurately describe your data with distributions as you will see. Part of this is the limitation of the tools we have to fit a distribution to our data and that our data are messy due to measurement noise, recording conditions, etc. We will fit a gamma and lognormal distribution to the IEI data with and without a transform.\n",
    "\n",
    "You will also notice that the gamma distribution does not fit very well with the untransformed data. You will also notice how the normal, lognormal and gamma distribution converge as the transform gets stronger. The biggest problem with IEI data is that it is tightly clusters around the mode and has a long tail to the right (right-skew). The mode can be estimated by looking at peak of the KDE. Many models assume some relationship between the mean and the variance of the data so the functions have limited types of shapes that they can support. For all the models that we used, the lognormal distribution is the best. This is probably because it is parameterized the same way as the normal distribution. Another thing to think about is that earlier we showed how we can model IEIs by randomly distributing events over time. Why does our data here not match that data? When we put TTX in the bath minis (mE/IPSCs, not sE/IPSCs) should be randomly distributed over time. One reason is that we cannot resolve mini events that are overlapping. This leads to a lognormal like distribution where we can occasionally resolve overlapping events and small amplitude events. Additionally, there are mini events that are smaller than the noise of the recording so we will miss those minis but it does not mean they do not exist. We have two factors which are simply limitations of the technique we are using which could change the distribution of the data that we collect.\n",
    "\n",
    "Lastly, when Scipy fits data to a distribution it will modify the location of the distribution which usually does not occur when fitting regression models if the model does not explicitly contain a location parameter (of which the normal distribution is one of the few that explicity contain it). So if you want to run a regression model (t-test, ANOVAs included) you may need to shift your data to just above 0 to improve the fit if you are using a gamma, beta or lognormal distribution. Additionally, Scipy fits distributions using Maximum Likelihood Estimation. There are other methods such as Expectation Maximization and Bayesian statistics that can fit distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e41de2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "dist_data = df.loc[df[\"IEI (ms)\"] > 0, \"IEI (ms)\"]\n",
    "\n",
    "\n",
    "def fit_distributions(data):\n",
    "    dist_dict = {}\n",
    "\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    padding = (max_val - min_val) * 0.1  # Add 10% padding\n",
    "    grid_min = min_val - padding\n",
    "    grid_max = max_val + padding\n",
    "    grid_min = max(grid_min, 0)\n",
    "    positions = np.linspace(grid_min, grid_max, num=248)\n",
    "    kernel = stats.gaussian_kde(data)\n",
    "    y = kernel(positions)\n",
    "    dist_dict[\"kde_x\"] = positions\n",
    "    dist_dict[\"kde_y\"] = y\n",
    "\n",
    "    # Gaussian distribution fit\n",
    "    norm_vals = stats.norm.fit(data)\n",
    "    dist_dict[\"norm_x\"] = np.linspace(1e-10, data.max() + 1e-10, 248)\n",
    "    dist_dict[\"norm_y\"] = stats.norm.pdf(\n",
    "        dist_dict[\"norm_x\"], loc=norm_vals[0], scale=norm_vals[1]\n",
    "    )\n",
    "\n",
    "    # Lognormal distribution fit\n",
    "    logvals = stats.lognorm.fit(data)\n",
    "    dist_dict[\"lognorm_x\"] = np.linspace(1e-10, data.max() + 1e-10, 248)\n",
    "    dist_dict[\"lognorm_y\"] = stats.lognorm.pdf(\n",
    "        dist_dict[\"lognorm_x\"], logvals[0], loc=logvals[1], scale=logvals[2]\n",
    "    )\n",
    "\n",
    "    # Gamma distribution fit\n",
    "    gammavals = stats.gamma.fit(data)\n",
    "    dist_dict[\"gamma_x\"] = np.linspace(1e-10, data.max() + 1e-10, 248)\n",
    "    dist_dict[\"gamma_y\"] = stats.gamma.pdf(\n",
    "        dist_dict[\"lognorm_x\"], gammavals[0], gammavals[1], gammavals[2]\n",
    "    )\n",
    "\n",
    "    return dist_dict\n",
    "\n",
    "\n",
    "def create_fig(source, title):\n",
    "    fig = figure(height=250, width=400, title=title)\n",
    "    fig.line(\"lognorm_x\", \"lognorm_y\", source=source, line_color=\"magenta\")\n",
    "    fig.line(\"norm_x\", \"norm_y\", source=source, line_color=\"orange\")\n",
    "    fig.line(\"gamma_x\", \"gamma_y\", source=source, line_color=\"blue\")\n",
    "    fig.line(\"kde_x\", \"kde_y\", source=source, line_color=\"black\", line_width=3)\n",
    "    return fig\n",
    "\n",
    "\n",
    "source = ColumnDataSource(fit_distributions(dist_data))\n",
    "fig = create_fig(source, title=\"No transform\")\n",
    "\n",
    "sqrt_source = ColumnDataSource(fit_distributions(np.sqrt(dist_data)))\n",
    "sqrt_fig = create_fig(sqrt_source, title=\"Sqrt\")\n",
    "\n",
    "log10_source = ColumnDataSource(fit_distributions(np.log10(dist_data)))\n",
    "log10_fig = create_fig(log10_source, title=\"Log10\")\n",
    "\n",
    "legend = figure(y_range=(5, 5), height=250, width=400, title=\"Legend\")\n",
    "legend.line(\n",
    "    [0, 1], [0, 0], line_color=\"magenta\", line_width=3, legend_label=\"lognormal\"\n",
    ")\n",
    "legend.line([0, 1], [1, 1], line_color=\"orange\", line_width=3, legend_label=\"normal\")\n",
    "legend.line([0, 1], [3, 3], line_color=\"blue\", line_width=3, legend_label=\"gamma\")\n",
    "legend.line([0, 1], [4, 4], line_color=\"black\", line_width=3, legend_label=\"kde\")\n",
    "legend.legend.location = \"center\"\n",
    "\n",
    "show(column(row(fig, sqrt_fig), row(log10_fig, legend)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f9e5e4",
   "metadata": {},
   "source": [
    "Based on the data you will see that the best fit for this IEI data is probably a lognormal distribution. If you want to run stats on the IEI data you will probably need to transform the data or use a lognormal distribution. This is true even though the IEIs should be exponentially distribution. Lastly, the lognormal distribution is interesting because it is everywhere. If you want to read more about the different neural processes that lognormally distributed you can read more in *The log-dynamic brain: how skewed distributions affect network operations* {cite}`buzsaki_log-dynamic_2014`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1529597e",
   "metadata": {},
   "source": [
    "Hopefully at this point you have a better idea of what distributions are and how they represent our data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephysbook (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
