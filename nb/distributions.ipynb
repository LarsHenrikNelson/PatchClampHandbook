{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047c2191",
   "metadata": {},
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9750dbb",
   "metadata": {},
   "source": [
    "Distributions and curves are some the most common statistical topics that you will use in electrophysiology analysis. Distributions and curves help us model data. Modeling data helps us describe the data we have and simplify complex data. While this may sound complex or intimidating it is not. A simple linear regression is a model. Distributions of our data are a model. Curve fitting is modeling. We have used these models in both the current clamp and mini analysis chapters but we will go more in-depth here to describe some of the underlying properties. Modeling can be divided into what, how and why models (see: https://compneuro.neuromatch.io/tutorials/intro.html for more). We will primarily focus on what models since these are the first step to using the other types of models. I rarely see what models discussed in patch clamp electrophysiology analysis, however I believe they are needed to analyze our data well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2cbf4f",
   "metadata": {},
   "source": [
    "## Distributions\n",
    "Distributions of data are one of the first things you should look at especially if you have large sample sizes. Think mini amplitude, IEI, and rise rate. Most the statistical analyses we do make assumptions about the distribution of our data yet rarely do papers show distributions of their data. Most other distribution contain the equivalent of the mean but the value is quiet different from the mean. When taking the mean of non-gaussian distributions you are not get the true central tendency of your data and may be measuring the effect of outliers rather than a true shift in the distribution of data. In the case of some thing like a beta distribution the mean may not accurately describe the data.\n",
    "\n",
    "Distributions can show counts or probabilities. When you make a histogram or kernel density estimate (KDE) you are creating a distribution of your data (technically a non-parametric distribution). Most often in statistical text books you will see something called probability density functions (PDF) for continuous variables or probability mass functions (PMF) for discrete variables. These are parametric distributions becuase that have parameters that describe the distribution. The important thing about these \"functions\" is that the distribution of values you get from the functions integrates (i.e. the area under the curve) to 1 for the when you take the distribution from negative limit to positive limit. When you put in a single number with some parameters you get a number out called a likelihood. \n",
    "\n",
    "The last thing to note is that finding the distribution that fits your data describes your data but does not tell you how or why is was generated that way. This how and why questions are not something we will get at here and are what computational neuroscientists study.\n",
    "\n",
    "You will also often see cumulative distribution functions (CDFs). The CDF is the just the integral of the PDF and the PDF the derivative of the CDF. When taking the integral of a continous function you can just use a Numpy [cumsum](https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html) and multiply by your delta X to get the CDF if you have evenly spaced samples. For the area under the curve you can use a variety of functions provided by scipy such as the [trapezoid](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.trapezoid.html#scipy.integrate.trapezoid), [simpson](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.trapezoid.html#scipy.integrate.simpson), or the [romberg](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.romb.html#scipy.integrate.romb). Below we will plot the PDF and CDF for each of the distributions.\n",
    "\n",
    "For this tutorial, there are interactive samples. Not all of these examples not use the preferred way of using and working with distributions in Python. The recommended way is to use the Scipy [stats](https://docs.scipy.org/doc/scipy/reference/stats.html) module. The stats module distrbutions provide a lot of useful features but can be a little bit intimidating to start with. Scipy stats distributions can provide PDF/PMFs, CDFs, PPFs and fit distributions to your data using maxmimum likelihood.\n",
    "\n",
    "If you want to learn a little more about PDFs and PMFs I suggest watching Very Normal on Youtube."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b6b9e",
   "metadata": {},
   "source": [
    "First we are going to look over some of the data we tend to collect in electrophysiology experiments and see what the distribution of data looks like. Then we are going to go over some specific distributions and see which distributions look most like our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4733ea",
   "metadata": {},
   "source": [
    "We are going to go over some common distributions and their properties that you will see in electrophysiology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790146f1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import ColumnDataSource, CustomJS, Select, Slider\n",
    "from bokeh.plotting import figure\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "cwd = Path.cwd().parent / \"data\"\n",
    "\n",
    "# output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3875ce",
   "metadata": {},
   "source": [
    "### Gaussian distribution\n",
    "In terms of data that we collect in electrophysiology the gaussian distribution is actually not that common but most of the time we assume our data follows a gaussian distribution.  Some non-gaussian distributions, such as the beta and von Mises (radians) distributions, can coverge to a normal distribution with certain parameters. The are problem when you assume that non-gaussian data follows a gaussian distribution AND you do not check your statistical models. We will cover some of this later in this chapter. The gaussian distribution has two parameters, the mean and standard deviation. The mean is what is called a location parameter and shifts the distribution around. The standard deviation is related to the spread of data symmetrically around the mean. Technically the gaussian distribution is unbounded. This means that you can get any value from -$\\infty$ to +$\\infty$. However, due to limits that we have on computers we generally don't show all the values up to +/- $\\infty$, but only up to a couple standard deviations past the mean on each side. The equation for the gaussian PDF is: $$\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\frac{(x-\\mu)^2}{2\\sigma^2}$$\n",
    "\n",
    "Below you can see how changing the mean and standard deviation changes the magenta distribution relative to the grey reference distribution. We plot both the PDF and the CDF (the integral of the PDF). There are a couple things to note. The area under the curve of the PDF will always equal 1. Changing the standard deviation decreases the likelihood of getting any value but increases the range of likely values you can get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17044145",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "mu = 0\n",
    "std = 1\n",
    "x = np.linspace(mu - std * 4, mu + 4 * std, num=400)\n",
    "y = np.exp(-((x - mu) ** 2) / (2 * std**2)) / np.sqrt(2 * np.pi * std**2)\n",
    "source = ColumnDataSource(\n",
    "    {\n",
    "        \"x\": x,\n",
    "        \"y\": y,\n",
    "        \"yc\": np.cumsum(y) * 0.020050125313283207,\n",
    "    }\n",
    ")\n",
    "pdf = figure(height=250, width=350, title=\"PDF\")\n",
    "pline = pdf.line(\"x\", \"y\", source=source, color=\"magenta\")\n",
    "pline1 = pdf.line(x, y, color=\"grey\")\n",
    "cdf = figure(height=250, width=350, title=\"CDF\")\n",
    "cline = cdf.line(\"x\", \"yc\", source=source, color=\"magenta\")\n",
    "cline1 = cdf.line(source.data[\"x\"], source.data[\"yc\"], color=\"grey\")\n",
    "\n",
    "mu = Slider(start=-10, end=10, value=0, step=0.5, title=\"Mu (mean)\")\n",
    "std = Slider(start=0.1, end=10, value=1, step=0.5, title=\"Sigma (std)\")\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        source=source,\n",
    "        mu=mu,\n",
    "        std=std,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    const arr = [];\n",
    "    const start = mu.value-std.value*4\n",
    "    const end = mu.value+std.value*4\n",
    "    const step = (end - start) / (400 - 1);\n",
    "\n",
    "    for (let i = 0; i < 400; i++) {\n",
    "        arr.push(start + step * i);\n",
    "    }\n",
    "    const temp_y = arr.map(x => {\n",
    "        const coefficient = 1 / Math.sqrt(2 * Math.PI * Math.pow(std.value, 2));\n",
    "        const exponent = -Math.pow((x - mu.value), 2) / (2 * Math.pow(std.value, 2));\n",
    "        return coefficient * Math.exp(exponent);\n",
    "    })\n",
    "    const cumsum = [temp_y[0]]\n",
    "    for (let i = 1; i < 400; i++) {\n",
    "        cumsum.push((cumsum[i-1] + temp_y[i]));\n",
    "    }\n",
    "    source.data.y = temp_y;\n",
    "    source.data.x = arr;\n",
    "    source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "mu.js_on_change(\"value\", callback)\n",
    "std.js_on_change(\"value\", callback)\n",
    "show(column(row(mu, std), row(pdf, cdf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f9441",
   "metadata": {},
   "source": [
    "First we will start by looking at some of the data we previously collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc504f0",
   "metadata": {},
   "source": [
    "### Lognormal distribution\n",
    "The lognormal distribution is a distribution where if you log transform the data you will get the normal distribution. If your data has a right skew (long tail to the right) your data may follow a lognormal distribution. You may ask why not just log transform the data? Log transforming means your data will no longer be in the same scale which makes downstream interpretations more complicated. The lognormal distribution is very common in biological sciences. Things like rates, lengths, concentrations and energies often follow a lognormal distribution. The lognormal distribution bounds are (0,+$\\infty$), The () brackets are exclusive which means that you can never have a 0 in the distribution since any log of 0 is undefined. The PDF of the lognormal distribution is: $$\\frac{1}{x\\sigma\\sqrt{2\\pi\\sigma^2}}\\exp(\\frac{-(ln(x)-\\mu)^2}{2\\sigma^2})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce1fbe8",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "mu = 0\n",
    "std = 1\n",
    "x = np.linspace(0.000001, np.exp(mu) + 4 * np.exp(std), num=400)\n",
    "y = np.exp(-((np.log(x) - mu) ** 2) / (2 * std**2)) / (\n",
    "    x * std * np.sqrt(2 * np.pi * std**2)\n",
    ")\n",
    "source = ColumnDataSource(\n",
    "    {\n",
    "        \"x\": x,\n",
    "        \"y\": y,\n",
    "        \"yc\": np.cumsum(y) * 0.020050125313283207,\n",
    "    }\n",
    ")\n",
    "pdf = figure(height=250, width=350, title=\"PDF\")\n",
    "pline = pdf.line(\"x\", \"y\", source=source, color=\"magenta\")\n",
    "pline1 = pdf.line(x, y, color=\"grey\")\n",
    "cdf = figure(height=250, width=350, title=\"CDF\")\n",
    "cline = cdf.line(\"x\", \"yc\", source=source, color=\"magenta\")\n",
    "cline1 = cdf.line(source.data[\"x\"], source.data[\"yc\"], color=\"grey\")\n",
    "\n",
    "mu = Slider(start=0, end=10, value=0, step=0.5, title=\"Mu (mean)\")\n",
    "std = Slider(start=0.25, end=10, value=1, step=0.25, title=\"Sigma (std)\")\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        source=source,\n",
    "        mu=mu,\n",
    "        std=std,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    const arr = [];\n",
    "    const start = 0.00001;\n",
    "    const end = Math.exp(mu.value)+Math.exp(std.value)*4;\n",
    "    const step = (end - start) / (400 - 1);\n",
    "\n",
    "    for (let i = 0; i < 400; i++) {\n",
    "        arr.push(start + step * i);\n",
    "    }\n",
    "    const temp_y = arr.map(x => {\n",
    "        const coefficient = 1 / (Math.sqrt(2 * Math.PI * Math.pow(std.value, 2))*x*std.value);\n",
    "        const exponent = -Math.pow((Math.log(x) - mu.value), 2) / (2 * Math.pow(std.value, 2));\n",
    "        return coefficient * Math.exp(exponent);\n",
    "    })\n",
    "    const cumsum = [temp_y[0]]\n",
    "    for (let i = 1; i < 400; i++) {\n",
    "        cumsum.push((cumsum[i-1] + temp_y[i]));\n",
    "    }\n",
    "    source.data.y = temp_y;\n",
    "    source.data.x = arr;\n",
    "    source.data.yx = cumsum;\n",
    "    source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "mu.js_on_change(\"value\", callback)\n",
    "std.js_on_change(\"value\", callback)\n",
    "show(column(row(mu, std), row(pdf, cdf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f67379",
   "metadata": {},
   "source": [
    "### Gamma distribution\n",
    "The gamma distribution is another important distribution for neuroscience data. The gamma distribution is used to model waiting times and rates. A lot of data we collect are rates, such as mini or spike frequency. If your data has a right skew (long tail to the right) your data may follow a gamma distribution. Interestingly, gamma distributed variables often maximize the information content of a signal which is useful in the brain. The gamma distribution is the generalization the exponential, Erlang and chi-squared distribution. The gamma distribution is defined by a shape, $\\alpha$, and scale, $\\theta$, parameter. Similar to the lognormal distribution the gamma distribution bounds are (0,+$\\infty$). The PDF of the gamma distribution is: $$\\frac{1}{\\Gamma(\\alpha)\\theta^{\\alpha}}x^{\\alpha-1}e^{-x/\\theta}$$\n",
    "\n",
    "$\\Gamma$ is the Gamma function is the factorial function that is generalized to complex numbers (except non-positive complex numbers)\n",
    "\n",
    "You can also get the mean: $\\mu=\\alpha\\theta$ and variance: $\\sigma^{2}=\\alpha\\theta^2$ of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200bc94",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "shapes = [1.0, 1.25, 1.25, 2.0, 3.0, 5.0, 7.5]\n",
    "scales = [2.0, 2.0, 3.0, 2.0, 2.0, 1.0, 1.0]\n",
    "x = np.linspace(0, 20, num=400)\n",
    "keys = []\n",
    "glines = {}\n",
    "cdflines = {}\n",
    "gamma_fig = figure(height=250, width=350)\n",
    "gamma_cdf = figure(height=250, width=350)\n",
    "\n",
    "for i, j in zip(shapes, scales):\n",
    "    pdf = stats.gamma.pdf(x, i, scale=j)\n",
    "    cdf = stats.gamma.cdf(x, i, scale=j)\n",
    "    k = f\"a={i} theta={j}\"\n",
    "    keys.append(k)\n",
    "    glines[k] = gamma_fig.line(x, pdf, color=\"grey\")\n",
    "    cdflines[k] = gamma_cdf.line(x, cdf, color=\"grey\")\n",
    "select = Select(title=\"Option:\", value=\"a=1.0 theta=2.0\", options=keys)\n",
    "glines[\"a=1.0 theta=2.0\"].glyph.line_color = \"magenta\"\n",
    "glines[\"a=1.0 theta=2.0\"].glyph.line_width = 3\n",
    "cdflines[\"a=1.0 theta=2.0\"].glyph.line_color = \"magenta\"\n",
    "cdflines[\"a=1.0 theta=2.0\"].glyph.line_width = 3\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        glines=glines,\n",
    "        cdflines=cdflines,\n",
    "        select=select,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    for (let key in glines) {\n",
    "        if (select.value == key) {\n",
    "            glines[key].glyph.line_color = 'magenta';\n",
    "            cdflines[key].glyph.line_color = 'magenta';\n",
    "            glines[key].glyph.line_width = 3;\n",
    "            cdflines[key].glyph.line_width = 3;\n",
    "        }\n",
    "        else {\n",
    "            glines[key].glyph.line_color = 'grey';\n",
    "            cdflines[key].glyph.line_color = 'grey';\n",
    "            glines[key].glyph.line_width = 1;\n",
    "            cdflines[key].glyph.line_width = 1;\n",
    "        }\n",
    "    }\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "select.js_on_change(\"value\", callback)\n",
    "show(column(select, row(gamma_fig, gamma_cdf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6badd2",
   "metadata": {},
   "source": [
    "## Beta distribution\n",
    "The beta distribution is used to model data that has finite intervals, and proportions and percentages. Some bounds values you might see in neuroscience are correlations (-1,1) and PPR (if you use the percentage formula and not the ratio). The beta distribution's bound are (0,1) so if you have bounded data that is not between those you can transform it.  The beta distribution is defined by a shape, $\\alpha$, and scale, $\\beta$, parameter. The PDF of the beta distribution is: $$\\frac {x^{\\alpha -1}(1-x)^{\\beta -1}}{\\frac {\\Gamma (\\alpha )\\Gamma (\\beta )}{\\Gamma (\\alpha +\\beta ) (\\alpha ,\\beta )}}$$\n",
    "$\\Gamma$ is the Gamma function is the factorial function that is generalized to complex numbers (except non-positive complex numbers). The denominator of the PDF is a normalization factor to ensure the distribution has a total probability of 1. \n",
    "\n",
    "One thing that you will notice is that the beta distribution can take on a wide range of shapes. This makes is very powerful for oddly shaped distributions. It can even even model normally distribution and uniformally distribution data when $\\alpha$=$\\beta$\n",
    "\n",
    "You can also get the mean: $\\mu=\\frac{\\alpha}{\\alpha+\\beta}$ and variance: $\\sigma^{2}=\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$ of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c533b5",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "shapes = [0.5, 5, 1, 2, 5, 1.25, 1, 5]\n",
    "scales = [0.5, 1, 3, 2, 2, 5, 1, 5]\n",
    "x = np.linspace(0, 1, num=400)\n",
    "keys = []\n",
    "blines = {}\n",
    "cdflines = {}\n",
    "beta_fig = figure(height=250, width=350)\n",
    "beta_cdf = figure(height=250, width=350)\n",
    "\n",
    "for i, j in zip(shapes, scales):\n",
    "    pdf = stats.beta.pdf(x, i, j)\n",
    "    cdf = stats.beta.cdf(x, i, j)\n",
    "    k = f\"a={i} b={j}\"\n",
    "    keys.append(k)\n",
    "    blines[k] = beta_fig.line(x, pdf, color=\"grey\")\n",
    "    cdflines[k] = beta_cdf.line(x, cdf, color=\"grey\")\n",
    "select = Select(title=\"Option:\", value=\"a=0.5 b=0.5\", options=keys)\n",
    "blines[\"a=0.5 b=0.5\"].glyph.line_color = \"magenta\"\n",
    "blines[\"a=0.5 b=0.5\"].glyph.line_width = 3\n",
    "cdflines[\"a=0.5 b=0.5\"].glyph.line_color = \"magenta\"\n",
    "cdflines[\"a=0.5 b=0.5\"].glyph.line_width = 3\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        blines=blines,\n",
    "        cdflines=cdflines,\n",
    "        select=select,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    for (let key in blines) {\n",
    "        if (select.value == key) {\n",
    "            blines[key].glyph.line_color = 'magenta';\n",
    "            cdflines[key].glyph.line_color = 'magenta';\n",
    "            blines[key].glyph.line_width = 3;\n",
    "            cdflines[key].glyph.line_width = 3;\n",
    "        }\n",
    "        else {\n",
    "            blines[key].glyph.line_color = 'grey';\n",
    "            cdflines[key].glyph.line_color = 'grey';\n",
    "            blines[key].glyph.line_width = 1;\n",
    "            cdflines[key].glyph.line_width = 1;\n",
    "        }\n",
    "    }\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "select.js_on_change(\"value\", callback)\n",
    "show(column(select, row(beta_fig, beta_cdf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83631be7",
   "metadata": {},
   "source": [
    "## Poisson distribution\n",
    "The Poisson distribution is a discete distribution used model the probability of a certain number of events occuring within a time frame thus is related to the rate of events occur. The poisson distribution is similar to the gamma distribution kind of like the discrete cousin of the gamma distribution. The Poisson distribution has an important related feature called dispersion or the fano factor: $\\frac{\\sigma^2}{\\mu} = 1$, which is also related to the coefficient of variation: $\\frac{\\sigma}{\\mu} = \\lambda^{-\\frac{1}{2}}$. The fano factor is often used to check the variability of a spike train or even minis. The closer the fano factor is to 0, then the more predicable the process is. The farther you go above 1 the more clustered your events will be suggesting that there is some reason you events cluster (make sense for neurons that can fire spike trains). The Poisson PMF is defined by $\\lambda, or the rate, and k, the integer value. The Poisson PMF (not PDF since it is distrete) is: $${\\frac {\\lambda ^{k}e^{-\\lambda }}{k!}}$$. Since the Poisson distribution is a discrete distribution you actually get probabilities out instead of likelihoods.\n",
    "\n",
    "With the Poisson distribution you could model how many minis are likely to occur in a time frame based on a rate but you cannot model when the minis occur. To do that you would need to use the continuous exponential distribution (which is related to the gamma distribution). One thing to note is that the plot uses scatter instead of a line. This is to indicate that the Poisson distribution can only take integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c6f01",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "lam = 1\n",
    "k = np.arange(0, 21)\n",
    "y = (lam**k * np.exp(-lam)) / np.array([math.factorial(i) for i in k])\n",
    "source = ColumnDataSource({\"k\": k, \"y\": y, \"yc\": np.cumsum(y)})\n",
    "pdf = figure(height=250, width=350, title=\"PDF\")\n",
    "pline = pdf.scatter(\"k\", \"y\", source=source, color=\"magenta\")\n",
    "pline1 = pdf.scatter(k, y, color=\"grey\")\n",
    "cdf = figure(height=250, width=350, title=\"CDF\")\n",
    "cline = cdf.scatter(\"k\", \"yc\", source=source, color=\"magenta\")\n",
    "cline1 = cdf.scatter(source.data[\"k\"], source.data[\"yc\"], color=\"grey\")\n",
    "\n",
    "lam = Slider(start=0.5, end=10, value=1, step=0.5, title=\"Lambda\")\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        source=source,\n",
    "        lam=lam,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    const arr = [];\n",
    "    for (let i = 0; i < 21; i++) {\n",
    "        let result = 1;\n",
    "        for (let k = 2; k <= i; k++) {\n",
    "            result *= k;\n",
    "        }\n",
    "        const val = ((lam.value**i)*(Math.exp(-1*lam.value)))/result;\n",
    "        arr.push(val);\n",
    "    }\n",
    "    const cumsum = [arr[0]]\n",
    "    for (let i = 1; i < 21; i++) {\n",
    "        cumsum.push((cumsum[i-1] + arr[i]));\n",
    "    }\n",
    "    source.data.y = arr;\n",
    "    source.data.yc = cumsum\n",
    "    source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "lam.js_on_change(\"value\", callback)\n",
    "show(column(row(lam), row(pdf, cdf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174fc2b8",
   "metadata": {},
   "source": [
    "## Examining distributions in your data.\n",
    "So far we have covered five distributions that are heavily used (or should be) in neuroscience. The next part is look at some data that we get in patch clamp recordings, fit each of these distributions (when possible) to the data and see how the fit looks.\n",
    "\n",
    "The most common method to look at distributions of data is to use a histogram. My preferred method is to use a kernel density estimate (KDE). Both of these methods are non-parametric in that they technically do not have any parameters other than your data to create a distribution. The reason I prefer KDEs to histograms is that you can interpolate where you do not have data. KDEs are essentially the non-parametric probabability density function like the ones we already covered. I will show you how create a histogram and KDE in Python and then we will use the KDE to compare our data to the distributions above. One thing to note is that we will only use the Poisson for the IEI data since IEI can easily be converted to integers if you know what the sample rate of the data is (in our case 10000 Hz). To do this we will use the data we collected in Miniature/spontaneous postsynaptic currents chapter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def750db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cwd/\"mini_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08808442",
   "metadata": {},
   "source": [
    "### How to Create a Histogram\n",
    "To create a histogram in Python the easiest way is to use Numpy's [histogram](https://numpy.org/doc/stable/reference/generated/numpy.histogram.html). For this example we will use 50 bins, however Numpy has a pretty good algorithm for automatically selecting bins. One setting we will use is the density equals true. This will ensure that each bin shows the likelihood so that is matches with the KDE. If you just want to plot a histogram in other popular plotting packags such as Matplotlib or Seaborn, they have modules where you can just put in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bff703",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dict = {}\n",
    "for i in df:\n",
    "    hist, edges = np.histogram(df[i], range=(df[i].min()-1e-6, df[i].max()+1e-6), density=True, bins=50)\n",
    "    hist_dict[f\"{i}_hist\"] = hist\n",
    "    hist_dict[f\"{i}_left\"] = edges[:-1]\n",
    "    hist_dict[f\"{i}_right\"] = edges[1:]\n",
    "hist_dict[\"left\"] = edges[:-1]\n",
    "hist_dict[\"right\"] = edges[1:]\n",
    "hist_dict[\"hist_y\"] = hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c001c0f",
   "metadata": {},
   "source": [
    "### How to Create a KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7caf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_dict = {}\n",
    "for i in df:\n",
    "    data = df[i]\n",
    "    if i == \"iei\":\n",
    "        data = data[data > 0]\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    padding = (max_val - min_val) * 0.1  # Add 10% padding\n",
    "    grid_min = min_val - padding\n",
    "    grid_max = max_val + padding\n",
    "    grid_min = max(grid_min, 0)\n",
    "    positions = np.linspace(grid_min, grid_max, num=248)\n",
    "    kernel = stats.gaussian_kde(data)\n",
    "    y = kernel(positions)\n",
    "    kde_dict[f\"{i}_x\"] = positions\n",
    "    kde_dict[f\"{i}_y\"] = y\n",
    "\n",
    "kde_dict[\"kde_y\"] = y\n",
    "kde_dict[\"x\"] = positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c6f13",
   "metadata": {},
   "source": [
    "### Examining the data\n",
    "Below are some plots of our data. There are three common transforms;  sqrt, log10 and the negative inverse. These transforms are used to correct for right-skew in your data. The functions correcting for skewness go from light correction to heavy correction in this order: sqrt &rarr; log10 &rarr; negative inverse. You can also see that since we do not transform the data before any corrections the histogram can look pretty weird. Part of not correcting the histogram before is due to limitations of how this book is written and published. However, you can see how the transform \"resizes\" the bins, putting greater emphasis (larger bin size) on smaller values. All these transforms decrease the effects of outliers. Additionally, I also plot the mean, median and mode. These are all measures of central tendency. You can see how the transform shifts the mean and median towards the mode. However the mean, median and mode almost never fully align. This is because we have truncated distributions in addition to skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff9bb3e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "menu = Select(\n",
    "    title=\"Variables\",\n",
    "    value=\"iei\",\n",
    "    options=df.columns.to_list()\n",
    ")\n",
    "kde_source = ColumnDataSource(kde_dict)\n",
    "hist_source = ColumnDataSource(hist_dict)\n",
    "transform = Select(title=\"Transform\", value=\"Identity\", options=[\"Identity\", \"sqrt\", \"log\", \"neg_inverse\"])\n",
    "\n",
    "hist_figure = figure(height=250, width=400)\n",
    "hist_data = hist_figure.quad(top=\"hist_y\", bottom=0, left=\"left\", right=\"right\", line_color=\"white\", alpha=0.5, source=hist_source)\n",
    "\n",
    "kde_figure = figure(height=250, width=400)\n",
    "line = kde_figure.line(x=\"x\", y=\"kde_y\", source=kde_source, line_color=\"black\")\n",
    "\n",
    "central_tendency = {}\n",
    "for i in df:\n",
    "    data = df[i]\n",
    "    if i == \"iei\":\n",
    "        data = data[data > 0]\n",
    "    index = kde_source.data[f\"{i}_y\"].argmax()\n",
    "    central_tendency[f\"{i}_Identity\"] = [np.mean(data), np.median(data), kde_source.data[f\"{i}_x\"][index]]\n",
    "    sqrt_data = np.sqrt(data)\n",
    "    central_tendency[f\"{i}_sqrt\"] = [np.mean(sqrt_data), np.median(sqrt_data), np.sqrt(kde_source.data[f\"{i}_x\"][index])]\n",
    "    log_data = np.log10(data)\n",
    "    central_tendency[f\"{i}_log\"] = [np.mean(log_data), np.median(log_data), np.log10(kde_source.data[f\"{i}_x\"][index])]\n",
    "    ninv_data = -1/data\n",
    "    central_tendency[f\"{i}_neg_inverse\"] = [np.mean(ninv_data), np.median(ninv_data), -1/kde_source.data[f\"{i}_x\"][index]]\n",
    "central_tendency[\"x\"] = central_tendency[\"iei_Identity\"]\n",
    "central_tendency[\"color\"] = [\"orange\", \"blue\", \"magenta\"]\n",
    "central_tendency = ColumnDataSource(central_tendency)\n",
    "\n",
    "vspans = kde_figure.vspan(x=\"x\", color=\"color\", source=central_tendency)\n",
    "\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        hist_source=hist_source,\n",
    "        kde_source=kde_source,\n",
    "        central_tendency=central_tendency,\n",
    "        menu=menu,\n",
    "        transform=transform,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    if (transform.value == \"Identity\") {\n",
    "        var left = hist_source.data[`${menu.value}_left`];\n",
    "        var right = hist_source.data[`${menu.value}_right`];\n",
    "        var x = kde_source.data[`${menu.value}_x`];\n",
    "        var mx = central_tendency.data[`${menu.value}_Identity`];\n",
    "    } else if (transform.value == \"log\") {\n",
    "        var left = hist_source.data[`${menu.value}_left`].map(num => Math.log10(num));\n",
    "        var right = hist_source.data[`${menu.value}_right`].map(num => Math.log10(num));\n",
    "        var x = kde_source.data[`${menu.value}_x`].map(num => Math.log10(num));\n",
    "        var mx = central_tendency.data[`${menu.value}_log`];\n",
    "    } else if (transform.value == \"sqrt\") {\n",
    "        var left = hist_source.data[`${menu.value}_left`].map(num => Math.sqrt(num));\n",
    "        var right = hist_source.data[`${menu.value}_right`].map(num => Math.sqrt(num));\n",
    "        var x = kde_source.data[`${menu.value}_x`].map(num => Math.sqrt(num));\n",
    "        var mx = central_tendency.data[`${menu.value}_sqrt`];\n",
    "    }  else if (transform.value == \"neg_inverse\") {\n",
    "        var left = hist_source.data[`${menu.value}_left`].map(num => -1/num);\n",
    "        var right = hist_source.data[`${menu.value}_right`].map(num => -1/num);\n",
    "        var x = kde_source.data[`${menu.value}_x`].map(num => -1/num);\n",
    "        var mx = central_tendency.data[`${menu.value}_neg_inverse`];\n",
    "    }\n",
    "    const hist_y = hist_source.data[`${menu.value}_hist`];\n",
    "    const kde_y = kde_source.data[`${menu.value}_y`];\n",
    "    hist_source.data.hist_y = hist_y; \n",
    "    hist_source.data.left = left;\n",
    "    hist_source.data.right = right;\n",
    "    kde_source.data.kde_y = kde_y\n",
    "    kde_source.data.x = x;\n",
    "    central_tendency.data.x = mx;\n",
    "    central_tendency.change.emit();\n",
    "    kde_source.change.emit();\n",
    "    hist_source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "menu.js_on_change(\"value\", callback)\n",
    "transform.js_on_change(\"value\", callback)\n",
    "\n",
    "show(column(row(menu, transform), row(hist_figure, kde_figure)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4769c13e",
   "metadata": {},
   "source": [
    "### Fitting your data to a distribution\n",
    "In this next section we will look at how data fits to different distributions. Due to limitations of the format we will stick with IEI data however, I encourage to you modify the code below on either the other data we have or better yet on your own data. You should note that it is quite hard to accurately describe your data with distributions as you will see. Part of this is the limitation of the tools we have to fit a distribution to our data and that our data are messy due to measurement noise, recording conditions, etc.\n",
    "\n",
    "You will also notice that the gamma distribution does not fit very well with the untransformed data. You will also notice how the normal, lognormal and gamma distribution converge as the transform gets stronger. The biggest problem with IEI data is that it is tightly clusters around the mode. The mode can be estimated by looking at peak of the KDE. Many models assume some relationship between the mean and the variance of the data so the functions have limited types of shapes that they can support. For all the models that we used, the lognormal distribution is the best. This is probably because it is parameterized the same way as the normal distribution.\n",
    "\n",
    "Lastly, when Scipy fits data to a distribution it will modify the location of the distribution which usually does not occur when fitting regression models if the model does not explicitly contain a location parameter (of which the normal distribution is one of the few that explicity contain it). So if you want to run a regression model (t-test, ANOVAs included) you may need to shift your data to just above 0 to improve the fit if you are using a gamma, beta or lognormal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e41de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_data = df.loc[df[\"iei\"] > 0, \"iei\"]\n",
    "\n",
    "def fit_distributions(data):\n",
    "    dist_dict = {}\n",
    "\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    padding = (max_val - min_val) * 0.1  # Add 10% padding\n",
    "    grid_min = min_val - padding\n",
    "    grid_max = max_val + padding\n",
    "    grid_min = max(grid_min, 0)\n",
    "    positions = np.linspace(grid_min, grid_max, num=248)\n",
    "    kernel = stats.gaussian_kde(data)\n",
    "    y = kernel(positions)\n",
    "    dist_dict[\"kde_x\"] = positions\n",
    "    dist_dict[\"kde_y\"] = y\n",
    "    \n",
    "\n",
    "    # Gaussian distribution fit\n",
    "    norm_vals = stats.norm.fit(data)\n",
    "    dist_dict[\"norm_x\"] = np.linspace(1e-10, data.max()+1e-10, 248)\n",
    "    dist_dict[\"norm_y\"] = stats.norm.pdf(dist_dict[\"norm_x\"], loc=norm_vals[0], scale=norm_vals[1])\n",
    "\n",
    "    # Lognormal distribution fit\n",
    "    logvals = stats.lognorm.fit(data)\n",
    "    dist_dict[\"lognorm_x\"] = np.linspace(1e-10, data.max()+1e-10, 248)\n",
    "    dist_dict[\"lognorm_y\"] = stats.lognorm.pdf(dist_dict[\"lognorm_x\"], logvals[0], loc=logvals[1], scale=logvals[2])\n",
    "\n",
    "    # Gamma distribution fit\n",
    "    gammavals = stats.gamma.fit(data)\n",
    "    dist_dict[\"gamma_x\"] = np.linspace(1e-10, data.max()+1e-10, 248)\n",
    "    dist_dict[\"gamma_y\"] = stats.gamma.pdf(dist_dict[\"lognorm_x\"], gammavals[0], gammavals[1], gammavals[2])\n",
    "    \n",
    "    return dist_dict\n",
    "\n",
    "def create_fig(source, title):\n",
    "    fig = figure(height=250, width=400, title=title)\n",
    "    fig.line(\"lognorm_x\", \"lognorm_y\", source=source, line_color=\"magenta\")\n",
    "    fig.line(\"norm_x\", \"norm_y\", source=source, line_color=\"orange\")\n",
    "    fig.line(\"gamma_x\", \"gamma_y\", source=source, line_color=\"blue\")\n",
    "    fig.line(\"kde_x\", \"kde_y\", source=source, line_color=\"black\", line_width=3)\n",
    "    return fig\n",
    "\n",
    "source = ColumnDataSource(fit_distributions(dist_data))\n",
    "fig = create_fig(source, title=\"No transform\")\n",
    "\n",
    "sqrt_source = ColumnDataSource(fit_distributions(np.sqrt(dist_data)))\n",
    "sqrt_fig = create_fig(sqrt_source, title=\"Sqrt\")\n",
    "\n",
    "log10_source = ColumnDataSource(fit_distributions(np.log10(dist_data)))\n",
    "log10_fig = create_fig(log10_source, title=\"Log10\")\n",
    "\n",
    "legend = figure(y_range=(5,5), height=250, width=400, title=\"Legend\")\n",
    "legend.line([0, 1], [0, 0], line_color=\"magenta\", line_width=3, legend_label=\"lognormal\")\n",
    "legend.line([0, 1], [1, 1], line_color=\"orange\", line_width=3, legend_label=\"normal\")\n",
    "legend.line([0, 1], [3, 3], line_color=\"blue\", line_width=3, legend_label=\"gamma\")\n",
    "legend.line([0, 1], [4, 4], line_color=\"black\", line_width=3, legend_label=\"kde\")\n",
    "legend.legend.location = \"center\"\n",
    "\n",
    "show(column(row(fig, sqrt_fig), row(log10_fig, legend)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d701341",
   "metadata": {},
   "source": [
    "This concludes the chapter on distributions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
