{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8e6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "\n",
    "base_path = Path().cwd().parent / \"data/stats\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be84882",
   "metadata": {},
   "source": [
    "# One- and two-sample statistics\n",
    "One- and two-sample statistics are statistics where we compare one group to some predifined mean or distribution or compare whether two gropus are different. There are parametric and non-parametric tests.\n",
    "\n",
    "## The t-test\n",
    "The t-test is perhaps one of the simplest statistical tests you can run. Usually in electrophysiology we are comparing two different (independent) groups, comparing two different timepoints or drug treatment within a group (paired, not independent) or comparing whether a single group is different from a predefined mean (one-sample t-test) which can be used for a paired t-test. You can run a t-test using Scipy or Statsmodels in Python. I always recommend getting the confidence intervals of the difference between the two groups. Usually when the confidence intervals of the difference do not include 0 a statistically significant effect. The Further from 0 that your interval is the larger the effect size. Since the p-value is dependent on sample size this helps interpret the p-value. If you are running a t-test between two different groups there are several things to consider.\n",
    "1. Are your data normally distributed? Yes, go to 2.\n",
    "2. Are your two groups independent? Yes, use z-test, t-test or Welch's t-test. No, use paired t-test.\n",
    "3. Is the variance of your two groups the same? Yes, use standard t-test. No, use Welch's t-test.\n",
    "4. Are you sample sizes unequal? Use Welch's t-test\n",
    "5. How large are your sample sizes? Larger than ~30 use the z-test\n",
    "\n",
    "### The output\n",
    "You will get the test statistic, a p-value and the degrees of freedom. You can also get confidence intervals. For simple tests like t-tests I prefer to get bootstrapped confidence intervals.\n",
    "\n",
    "### Scipy vs Statmodels\n",
    "I prefer Scipy for the t-tests but, I will include the code for Statsmodels as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d2bf4",
   "metadata": {},
   "source": [
    "### Two-sample t-test with equal variances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4301da8",
   "metadata": {},
   "source": [
    "#### Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b062f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(base_path / \"ttest.csv\", header=None)\n",
    "one = data.loc[data[1] == \"one\", 0]\n",
    "two = data.loc[data[1] == \"two\", 0]\n",
    "output = stats.ttest_ind(one, two)\n",
    "print(output)\n",
    "print(output.confidence_interval())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff31be",
   "metadata": {},
   "source": [
    "#### Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e472e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ttest_ind(one, two, alternative=\"two-sided\", usevar=\"pooled\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb40142",
   "metadata": {},
   "source": [
    "### Comparing two-sample t-test with unqual variances with and without Welch's correction\n",
    "You will see that without Welch's correction that p-value and the degrees of freedom (df) is inflated. One thing to note is that Welch's correct will converge towards a standard t-test if the variances and sample sizes are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dcdac4",
   "metadata": {},
   "source": [
    "#### Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd99249",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(base_path / \"ttest_welch.csv\", header=None)\n",
    "one = data.loc[data[1] == \"one\", 0]\n",
    "two = data.loc[data[1] == \"two\", 0]\n",
    "output = stats.ttest_ind(one, two, equal_var=True)\n",
    "output_w = stats.ttest_ind(one, two, equal_var=False)\n",
    "print(output, output_w, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed7d19e",
   "metadata": {},
   "source": [
    "#### Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57356d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ttest_ind(one, two, alternative=\"two-sided\", usevar=\"unequal\")\n",
    "output_w = ttest_ind(one, two, alternative=\"two-sided\", usevar=\"equal\")\n",
    "print(output, output_w, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34dcc46",
   "metadata": {},
   "source": [
    "### One tailed t-test\n",
    "One tailed t-tests are used when you have a specific hypothesis about the direction of your effect. This needs to be choosen before you collect your data. An example of when this would be used is if you have a treatment that \"needs\" to be more effective than a previous treatment or if you are replicating data and aleady have a hypothesis about the direction of your effect. The direction of you effect is determine by why group you have as one and two. If you expect one to be larger than two (positive differece between the groups) you want larger/greater. If you expect one to be smaller than two (negative difference between the groups) the you want less/smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d54bdc",
   "metadata": {},
   "source": [
    "### Paired t-test\n",
    "The paired t-test is used when you have a subject that has two measures and you want to compare if the two measures are different. Paired data (aka within-subject data) violates the independence assumption of a traditional t-test. Also paired t-test will increase your statistical power by reducing between subject variance since each subject essentially acts as its own control.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efeb3f7",
   "metadata": {},
   "source": [
    "#### Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e635c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(base_path / \"ttest.csv\", header=None)\n",
    "one = data.loc[data[1] == \"one\", 0]\n",
    "two = data.loc[data[1] == \"two\", 0]\n",
    "output = stats.ttest_rel(one, two)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac08eb82",
   "metadata": {},
   "source": [
    "#### Statsmodels\n",
    "Statsmodels does not have a one sample t-test. Since the one-sample t-test is pretty simple to run we will manually compute it here. If you want to determine whether you difference is different from some other value than zero then you can replace 0 with that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1097e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = one - two\n",
    "m_diff = diffs.mean() - 0\n",
    "se = diffs.std(ddof=1) / np.sqrt(diffs.size)\n",
    "t_value = m_diff / se\n",
    "p = stats.t.sf(np.abs(t_value), df=diffs.size - 1) * 2  # two-sided test\n",
    "print(f\"statistic: {t_value}, pvalue: {p}, df: {diffs.size - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a32f0e",
   "metadata": {},
   "source": [
    "## Rank-based two-sample statistics\n",
    "There are a variety of non-parametric two-sample rank-based statistics such as Mann-Whitney and Wilcoxon signed-rank test and the bootstrap test. Some things you should be aware of before using rank-based tests. If your data is continuous, try transforming your data before running a rank-based test. I often see people fall back to Mann-Whitney for non-normal data. However, in most cases a simple log-transform would have been enough to make the data fit the assumptions of the t-test and its variants. Another is that for rank-based tests in biological sciences we generally want to test if the location or scale is different but not both because that makes interpreting our data hard. To do so we are most interested in the median. If your two samples each have a distribution (same gamma) with different shapes **AND** dispersion you may not have a difference in medians (due to location or scale) which is what we really want to test with rank-based tests. The test is still valid you just cannot say where the difference in your samples are. If you do not know the terms location, shape, scale or dispersion check out the [Distributions](=distributions) chapter. Non-parametric rank-based statistics really have almost as many assumptions as parametric-based statistics. Rank-based statistics can also be sensitive the number of ties or the number of same values between your two group. Rank-based statistics are also not a substitute for parametric tests when your sample size is small (n=3 for each group). The last case where rank-based tests may be useful is when you have outliers\n",
    "\n",
    "### Mann-Whitney U\n",
    "Mann-Whitney is a rank-based statistical test for two indepedent samples. Rather than compare the mean difference, Mann-Whitney essentially scores how many values are larger than in the other group and creates a U score which is used to generate a p-value. For the Mann-Whitney test to be valid your data must come from the same distribution type otherwise interpreting your what difference your significant p-value is showing is next to impossible. When reporting the Mann-Whitney p-values you should also report the difference between the medians (or the median of each group), the Hodges-Lehmann estimator which is like an effects size measures for the Mann-Whitney test and the probability of superiority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab83bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def hodges_lehmann(group1, group2):\n",
    "    \"\"\"Median of all pairwise differences\"\"\"\n",
    "    pairwise_diffs = [y - x for x, y in product(group1, group2)]\n",
    "    return np.median(pairwise_diffs)\n",
    "\n",
    "def probability_of_superiority(group1, group2):\n",
    "    \"\"\"P(Y > X) - common language effect size\"\"\"\n",
    "    pairs = [(x, y) for x, y in product(group1, group2)]\n",
    "    return np.mean([y > x for x, y in pairs])\n",
    "\n",
    "data = pd.read_csv(base_path / \"ttest.csv\", header=None)\n",
    "one = data.loc[data[1] == \"one\", 0]\n",
    "two = data.loc[data[1] == \"two\", 0]\n",
    "output = stats.mannwhitneyu(one, two,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcefeac",
   "metadata": {},
   "source": [
    "\n",
    "### Brunner-Munzel\n",
    "Brunner-Munzel is a rank-based tests with very few assumptions. It is robust to outliers, it can be used when the shape or dispersion of a distributions are different. It tests whether the probability of getting large values in both groups is the same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathclamphandbook (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
