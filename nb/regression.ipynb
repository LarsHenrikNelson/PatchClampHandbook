{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ac4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "base_path = Path().cwd().parent / \"data/stats\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d2cde7",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "Linear regression is one of the most used ways to model data. Linear regression has an equation form of: $$y=X\\beta+\\epsilon$$ where is y is a vector of dependent variable (can also be a maxtrix Y if it is multivariate regression), X is a matrix of independent variables with each column being a feature and each row being a subject, $\\beta$ is a vector coefficients, and $\\epsilon$ is the vector of errors. $\\epsilon$ is add to the equation because we can never fully model the relationship between y and X. Linear regression makes several assumptions about $\\epsilon$. If you look at the equation through the lens of linear algebra $X\\beta$ is actually the inner dot product between the matrix $X$ and vector $\\beta$. $y=mx+b$ is a specific form of the linear equation where there are just two coefficients m and b. Linear regresssion that has more than just one coefficient like $y=mx+b$ is called multiple linear regression. Linear regression can model non-linear and categorical features. Linear regression using least squares which is a minimization method where the $\\epsilon$ is minimized. By minimizing distance between the predicted values and the given values. Minimization uses calculus. In some cases there is a solvable equation and in other cases we just approximate solution.\n",
    "\n",
    "## Ordinary least squares\n",
    "Ordinary least squares (OLS) is probably what you are using when you run linear regression and ANOVA. OLS has the same equation as $y=X\\beta$ but has what is called a closed form solution; the solution is solvable with linear algebra with the equation $\\beta=(X^TX)^{-1}X^Ty$. OLS assumes that a constant change in the predictor leads to a constant change in the dependent variable.\n",
    "\n",
    "## Slopes\n",
    "Slopes are just the regression coefficients without the intercept coefficient. $\\beta$ will be a vector of coefficients that is the number of your independent variables + one. The extra coefficient is the intercept. You can actually measure the mean of you data if your independent variables are all 1 so you slope will be 0 and the intercept will be the mean. Not computationally efficient but possible.\n",
    "\n",
    "## Intercept\n",
    "The intercept is the extra coefficient that can be added to the regression equation. You do not need a intercept to fit a linear regression. The intercept is similar to the y-offset that we saw in the curves chapter.\n",
    "\n",
    "## Building a regression model\n",
    "We will go over how to build a regression. We will start with a simple linear regression, cover multiple linear regression, regression with categorical independent variables, polynomial regression and interactions between variables. You can think of categorical and polynomial variables as a type of multiple linear regression.\n",
    "\n",
    "## Simple linear regression\n",
    "Simple linear regression has the equation $y=mx+b$. $m$ and $b$ are coefficients which make up the $\\beta$ vector of coefficients in the general linear regression equation $y=X\\beta$. You can run simple linear regression three ways in Python; manually or using Scipy and StatModels. We will use Scipy first. After that we will use StatsModels since that will be used for more advanced regression.\n",
    "\n",
    "### Scipy\n",
    "Scipy has a simple interface. I recommend it for any simple linear regression task. However, it does not allow for residual analyis. If you need to 100% ensure your model fits well you should use Statsmodels. You can manually create the residuals but Statsmodels has several helper functions that make it easier. To get the residuals we can just find the predicted point for each x and subtract that from each y. For a simple residual analysis you see the residuals randomly distibutes across 0 when plotted against the independent variable. For simple regression this works but, for multiple regression this will not work because you have many x variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcefee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "n = 20\n",
    "x = rng.random(20)\n",
    "y = 1.6 * x + rng.random(20)\n",
    "res = stats.linregress(x, y)\n",
    "y_fit = res.intercept + res.slope * x\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "ax[0].plot(x, y, \"o\", label=\"original data\")\n",
    "ax[0].plot(x, y_fit, \"r\", label=\"fitted line\")\n",
    "ax[0].set_title(\"Regression\")\n",
    "ax[0].legend()\n",
    "ax[1].set_title(\"Residuals\")\n",
    "ax[1].axhline(0, color=\"red\")\n",
    "_ = ax[1].plot(x, y_fit-y, \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d5992",
   "metadata": {},
   "source": [
    "### Statsmodels\n",
    "For statsmodels we will fit the regression two different ways; using the formula method and the by hand method. The formula method is easier to use to but can hide some preprocessing.\n",
    "\n",
    "#### By hand method\n",
    "By hand is really not by hand but we do have to do some prepocessing. If you want to fit and intercept for your model you need to add a column of 1s to the matrix X so that you matrix X will have a column of 1s and a column for independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9084457",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f911a52",
   "metadata": {},
   "source": [
    "### ANOVA\n",
    "ANOVA just pools the effects of of multiple coefficients in the linear regression model when you have categorical variables. For example, if you have a categorical variable treatment with three groups you end up two regression coefficients, each with their own p value. However, we generally want a single p value for our categorical variable so ANOVA pools the variance estimates and derives a F value with degrees of freedom to derive a p value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathclamphandbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
