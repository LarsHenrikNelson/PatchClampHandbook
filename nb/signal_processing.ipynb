{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a1b3974",
   "metadata": {},
   "source": [
    "# Signal processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da09d5d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.layouts import column, layout, row, gridplot\n",
    "from bokeh.models import ColumnDataSource, CustomJS, Slider\n",
    "from bokeh.plotting import figure\n",
    "from scipy import fft, signal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "temp_path = \"https://cdn.jsdelivr.net/gh/LarsHenrikNelson/PatchClampHandbook/data/current_clamp/\"\n",
    "with urllib.request.urlopen(temp_path + \"11.json\") as url:\n",
    "    temp = json.load(url)\n",
    "    data = np.array(temp[\"array\"])\n",
    "\n",
    "temp_path = (\n",
    "    \"https://cdn.jsdelivr.net/gh/LarsHenrikNelson/PatchClampHandbook/data/mepsc/\"\n",
    ")\n",
    "with urllib.request.urlopen(temp_path + \"1.json\") as url:\n",
    "    temp = json.load(url)\n",
    "    mepsc = np.array(temp[\"array\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30fdbdc",
   "metadata": {},
   "source": [
    "In this chapter we will cover what I consider core signal processing techniques such as convolution, filtering, FFT, peak finding, derivatives and integrals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc134a16",
   "metadata": {},
   "source": [
    "## Derivatives and integrals\n",
    "We will start with derivatives and integrals. Derivatives and integrals are some of the simplest signal processing that we can do. While derivatives and integrals may seem like intimidating calculus they are actually pretty simple for discrete data. \n",
    "\n",
    "### Derivatives\n",
    "#### How to take a derivative\n",
    "If you remember from calculus that the derivative is defined by: $$L=\\lim _{h\\to 0}{\\frac {f(a+h)-f(a)}{h}}$$ This means that we take the slope between each set of values. For continuous data that means there are an infinite number of differences but, for discrete data there are discrete points which means that we can just take the difference between neighboring points for our x and y data. Numpy has a built in function to do this called `np.diff`. If you have evenly spaced data as is for most data that we collect in the digital realm you dy is technically just `y[1]-y[0]`. If you want to get fancy you can use some called forward, central, or backward differences. In numpy there is a func called `np.gradient` that calculates the second order difference, which if you have evenly spaced data is just this equation: $$\\hat f_{i}^{(1)}=\\frac{f\\left(x_{i+1}\\right) - f\\left(x_{i-1}\\right)}{2h}+ \\mathcal{O}\\left(h^{2}\\right)$$ The second order difference just means that you grab the points to each side of the current index and find the slope between those. The gradient is nice because you end up with an output that is the same size as the input. The drawback is that if you have a signal with small peaks you will lose a lot of the variability in your signal. One way around that is to upsample your signal.\n",
    "#### When is a derivative useful?\n",
    "The derivative has many uses. The derivative of voltage signals can be used to transform it into current signals if you are recording voltage from a capacitor (a cell is a capacitor). You can use derivatives to detrend signals. Many statistical algorithms used to analyze signals assume something called stationarity which means that a signal has a relatively constant mean and variance over time. The derivative is a great way to create a stationary signal. We can use the derivative to find specific features of a signal like in the [current clamp](#cc_pt1) chapter. You could correlate the derivatives instead the actual signal this will help find where there are overlapping rates of change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5306f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy = np.diff(data)\n",
    "dx = np.diff(np.arange(data.size) / 10)\n",
    "derivative = (dy / dx)[2500:10500]\n",
    "fig1 = figure(height=250, width=350)\n",
    "fig1.line(np.arange(derivative.size), derivative)\n",
    "fig2 = figure(height=250, width=350, x_range=fig1.x_range, y_range=fig1.y_range)\n",
    "gradient = np.gradient(data, np.arange(data.size) / 10)[2500:10500]\n",
    "fig2.line(np.arange(gradient.size), gradient)\n",
    "show(gridplot([[fig1, fig2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1a4e09",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "Convolution is one the most used signal processing algorithms. It is used in filtering, template matching (which is a type of filtering), spike cross-correlation (different from Pearson's correlation) and is the core of convolution neural networks. We are going to focus on 1D convolution. If you have gone through the mspc chapter you have seen template matching (a version of convolution) used to find PCS. \n",
    "\n",
    "The basic idea of convolution is you are weighting an input array with a template and sliding that template over your input array. If you want to do correlation you just reverse your template. \n",
    "\n",
    "### Time domain convolution\n",
    "Below you can see an implementation of time domain convolution in Python. What happens is that for each value in one array you multiply all the values in the second array and add it to an output array. Time domain convolution outputs an array whose length equals `len(input) + len(template) - 1`. The additive length of your output means your signal is shifted in time by len(template)/2. There are several ways you can output time domain convolution. You can output the arrays only where there is 100% overlap overlap between the two. You can output the full output like I have done below. You can output the same length. For most electrophysiological signal processing we want the same length output. However, there are several ways you can output the same length. One is to get the \"zero phase\" output by subset the array like this `output[template.size//2:input.size+template.size//2]` or you can just grab\n",
    "\n",
    "### Frequency domain convolution\n",
    "For frequency domain convolution you need to run the forward FFT on both of your arrays. You will need to zero-pad your shortest array to the same length as the longer array. Once you have the forward FFT of both signals you just do element-wise multiplication then you take the resulting array and do the backward FFT. One thing to note is that if your input signals are real you can run a faster version of the FFT. The FFT convolution will output an array of length equal to the longest array even if you zero-pad your signal. \n",
    "\n",
    "Below you can see a comparison of the two by convolving an array of noise with a gaussian curve (a common way to filter a signal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf719b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_convolution(array1, array2):\n",
    "    output = np.zeros(array1.size + array2.size - 1)\n",
    "    for i in range(array1.size):\n",
    "        for j in range(array2.size):\n",
    "            output[i + j] += array1[i] * array2[j]\n",
    "    return output\n",
    "\n",
    "\n",
    "def fft_convolution(array1, array2):\n",
    "    size = fft.next_fast_len(max(array1.size, array2.size))\n",
    "    output = fft.irfft(fft.rfft(array1, n=size) * fft.rfft(array2, n=size))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3ad86",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "array1 = rng.random(1000)\n",
    "array1 -= array1.mean()\n",
    "array2 = signal.windows.gaussian(200, 3)\n",
    "tconv = time_convolution(array1, array2)\n",
    "fconv = fft_convolution(array1, array2)\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, constrained_layout=True)\n",
    "ax = ax.flat\n",
    "ax[0].plot(array1, linewidth=1)\n",
    "ax[0].set_title(\"Original\")\n",
    "ax[1].plot(array2, linewidth=1)\n",
    "ax[1].set_title(\"Gaussian kernel\")\n",
    "ax[2].plot(tconv, linewidth=1)\n",
    "ax[2].set_title(\"Time Convolution\")\n",
    "ax[3].plot(fconv, linewidth=1)\n",
    "ax[3].set_title(\"FFT Convolution\")\n",
    "for i in ax:\n",
    "    i.grid(which=\"both\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae94115",
   "metadata": {},
   "source": [
    "You may notice that the original signal looks fuzzier than than the convolved signal. This is because the gaussian kernel acts as a simple high pass filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14065db4",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "Filtering comes in a couple types: IIR filters, FIR filters, convolution filters and moving window filters. When we talk about designing filters, you may come across 4 types of filters: lowpass, highpass, bandpass and notch filters. Lowpass filters let low frequency signals pass and attenuate (reduce the gain/power) of high frequency signals. Highpass filters let high frequencies signals pass and attenuate low frequency signals. Bandpass filters attenuate both low and high frequencies but let frequenceis inbetween a set of frequencies pass such as 300-6000 Hz bandpass that can be used to isolate single unit spikes. Notch filters let most low and high frequency signals pass but typically attentuate a small frequency band such 58-62 Hz to remove line noise (at least in the US). The frequencies that filters let through unattenuated are considered the passband of the filter and the stopband is considered where the frequencies are fully attenuated by the filters (amount of attenuation depends on the filter). Most filters, particularly the ones we use, have a roll off which means that there is some set of frequencies that will be partially attenuated. Ripple is considered unwanted change in gain in the passband and stopband of a filter. Filters can also cause ripple in the time domain signal which means the filter is just introducing time-decaying oscillations into the signal which is usually unwanted. Filters with sharp cutoffs are more likely to have ripple and cause ringing.\n",
    "\n",
    "### Convolution filters\n",
    "Convolution filters are essentially convolving one array with another like we convolved the random noise data with a gaussian filter. Additionally, many FIR filters are just convolution filters. Simple convolution filters typically just filter out high frequency content.\n",
    "\n",
    "### Moving window filters\n",
    "Moving window filters are things like a sliding median or mean. Moving window filters typically act as lowpass filters. They are good for when your data is not sampled in the typical Hertz range but in days, hours, etc. For example you might use a moving mean to smooth stock price data or sales data. Moving window filters can shorten your data since your aggregating your data, however there are ways to make sure your signal is the same length. Moving window filters can also change the phase of your signal. The larger the window the more potential there is for phase change.\n",
    "\n",
    "#### Moving mean filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "array1 = rng.random(500)\n",
    "array1 -= array1.mean()\n",
    "window_size = 10\n",
    "mov_mean = np.zeros(array1.size)\n",
    "for i in range(1, array1.size+1):\n",
    "    j = i -1\n",
    "    if j < window_size:\n",
    "        mov_mean[j] = array1[:i].mean()\n",
    "    else:\n",
    "        mov_mean[j] = array1[i-10:i].mean()\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(array1, linewidth=1)\n",
    "_ = ax.plot(mov_mean, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca949524",
   "metadata": {},
   "source": [
    "### Infinite impulse response (IIR) and minimal-phase filters (primarily the Bessel and Butterworth filters)\n",
    "IIR filters as their name implies have an impulse response that is infinitely long and have internal feedback. In Python they including digital versions of analog filters such as the Bessel and Butterworth filter. The Bessel and Butterworth filter are probably the most commonly used filters in electrophysiology. IIR filters are consider minimal-phase filters which means they disrupt the phase (oscillations) of the signal by introducing a frequency dependent delays in the signal. The Bessel and Butterworth filters have very little ripple or change in gain in frequencies due to the filter. Bessel and Butterworth filters basically are tradeoffs between ripple, the steepness of the filter cutoff and the phase delay. When designing these filters in Python we generally just need to supply the what type of passband we want (lowpass, highpass, bandpass) and the order of the filter. The order of the filter is the steepness of the cutoff between the passband and stopband. In Python there are several different computation ways to create and use these filters. You can use an A/B filter, SOS filter or create an impulse response and convolve the filter. Unless you need GPU acceleration I recommend the SOS filter since it is numerically stable compared the A/B filter and impulse response filter. While Bessel and Butterworth filters change the phase of your signal, in the digital realm we can filter the signal in two directions which removes the phase changes but filters your data twice.\n",
    "\n",
    "#### Frequency response of different Butterworth filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c9d0d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(8, 3), constrained_layout=True)\n",
    "fig.suptitle(\"Effect of order on gain\")\n",
    "ax[0].set_title(\"Lowpass\")\n",
    "ax[1].set_title(\"Highpass\")\n",
    "for i in ax:\n",
    "    i.grid(which=\"both\", axis=\"both\")\n",
    "    i.set_xlabel(\"Frequency [rad/s]\")\n",
    "    i.set_ylabel(\"Amplitude [dB]\")\n",
    "for j in [\n",
    "    (4, 600, \"lowpass\"),\n",
    "    (8, 600, \"lowpass\"),\n",
    "]:\n",
    "    b, a = signal.butter(j[0], [j[1]], btype=j[2], analog=True)\n",
    "    w, h = signal.freqs(b, a)\n",
    "    w, h = signal.freqs(b, a)\n",
    "    ax[0].semilogx(w, 20 * np.log10(abs(h)))\n",
    "for j in [\n",
    "    (4, 600, \"highpass\"),\n",
    "    (8, 600, \"highpass\"),\n",
    "]:\n",
    "    b, a = signal.butter(j[0], [j[1]], btype=j[2], analog=True)\n",
    "    w, h = signal.freqs(b, a)\n",
    "    w, h = signal.freqs(b, a)\n",
    "    ax[1].semilogx(w, 20 * np.log10(abs(h)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d26b209",
   "metadata": {},
   "source": [
    "#### Effect of filtering on phase using a Butterworth filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a10a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = mepsc[2750:3250]\n",
    "array1 -= array1.mean()\n",
    "high = signal.butter(4, [600], btype=\"lowpass\", output=\"sos\", fs=10000)\n",
    "single = signal.sosfilt(high, array1)\n",
    "double = signal.sosfiltfilt(high, array1)\n",
    "x = np.arange(single.size)\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.plot(x, array1, color=\"black\", linewidth=1, alpha=0.2, label=\"Original\")\n",
    "ax.plot(x, single, color=\"orange\", alpha=0.8, label=\"Minimal phase\")\n",
    "ax.plot(x, double, color=\"magenta\", alpha=0.8, label=\"Zero-phase\")\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51741c83",
   "metadata": {},
   "source": [
    "### Finite impulse response (FIR) and linear-phase filters\n",
    "FIR filters are purely digital filters whose impulse response goes to zero in a finite amount of time. FIR filters are considered linear phase which means they do not affect the phase of the signal in passband but can have variable phase disruptions in the stopband. Linear phase response is extremely useful for telecommunications applications. The most common FIR filters are the sinc function window by a window such as the gaussian or Hann window. FIR filters have an order which is basically the length in samples of the filter. FIR filters are also known to cause ringing, however this can be minimized with longer filters and choice of window."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephysbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
