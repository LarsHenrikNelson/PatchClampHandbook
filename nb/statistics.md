# Statistics
Statistics is one of those things where I feel like the classes need to be updated for a modern world. Most of the statistics you will do will be run on a computer, yet in the stats class I took we never touched a computer. It is important to know some of the math underneath the hood of the statistical models but, I feel that we have limited time and cannot become an expert in everything. There are very smart people who have created statistical tools we should use. Programming does allow you to see more of the inner workings of statistical tests. This tutorial well not go deep into the math but will show you best practices on how to use Python (this chapter) and R (the next chapter) to run statistics. The R chapter will mostly be programming examples since the Python chapter will cover more whys. R will cover more mixed model statistics since the ecosystem for that is way better. Lastly, we will cover only frequentist methods and not bayesian.

## Basic terminology
Before we start covering stats we will cover some basic terminology. Part of understanding a topic is sometimes just learning the language a topic uses.<br>
**Mean**: The mean is the central tendency of your data. Many statistical tests that use the normal distribution under the hood are comparing the mean.<br>
**Variance (Var)**: Thespread of your data around the mean. Is is just standard deviation squared and is not in the units your data is but in $units^2$.The variance is very important in generalized linear models.<br>
**Standard deviation (STD or STDEV)**: The spread of your data around the mean. The standard deviation is very important for statistics because most parametric statistical models assume the standard deviation of both groups is the same.<br>
**Standard error of the mean (SEM)**: The precision of your estimated mean. SEM decreases as n increases but, not linearly. Many people plot $mean \pm SEM$ because it looks nice. Since SEM is indirectly related to statistical tests in that p-values are correlated with sample size.  
**Homogeneity of variances (homoscedastic)**: When two or more groups have the same variance. Homogeneity of variances is one the assumptions of many parametric statistics.<br>
**Heterogeneity of variances (heteroscedastic)**: When two or more groups have the different variances. Hetetoscedasticity can lead to incorrect results of hypothesis tests. Sometimes heteroscedasticity can be correct by transforming your data other times you will need specialized statistical tests.<br>
**Model**: A model is essentially anything you can put your data into and get a out something that helps you describe your data. All statistical tests model your data. Models make assumptions about your data thus you need to make sure your data fits those assumptions. Models often a simplified version of what happens in real life thus they have error. We generally want to minimize the error of our statistical tests.<br>
**Within-subject (repeated measures)**: Some sort of measure that is collected from a subject two or more times. This can be cells within mouse, mice within litter, students within a school within a distric within a state. Usually repeated measures can be things like baseline, drug, drug+drug in the same cell or timepoints in the same mouse. Mixed model and repeated measures regression/ANOVA are two ways to control for within-subjects however, these models have different assumptions. Within-subject effects are often ignored in neuroscience and biology. Ignoring within-subjects can inflate p-values since within-subjects tests can trade off within and between mice differences. Within-subject tests often require larger sample sizes for better estimates.<br>
**Between-subject**: Between subjects are things like genotype, sex, age (if you have restricted age groups like adult, child, infant). Between subjects tests are the most common in basic neuroscience and biology.<br>
**Test statistic**: This is a value calculated from the test that can be used for hypothesis testing. You will see $F$, $t$, $z$ and $\chi^2$ test statistics. The test statistics also have an associated degrees of freedom that is used to look up the resulting p-value. $F$ and $t$ values are in some case interchangable such as for simple regression or categorical predictors and t-test.<br>
**Predictor or indepedent variable**: These are the "x" values in your data. Many tests are essentially solving for coefficients to show how much these predictor variables effect the outcome or dependent variable.<br>
**Outcome or depedent variable**: These are the "y" values in your data. These depend on your "x" values or are the outcome of your "x" values.<br>
**Parametric**: We covered this in the distributions chapter but parametric just means you have distributions that have parameters to describe them such as mean and STD. Parametric statistics are most of what we will cover.<br>
**Non-parametric**: These statistical tests are often rank-based tests. They essential rank say two groups and compare how many values in one group are larger than those in the other group. Non-parametric statistics do not assume your data follows a known distribution but do assume that each group follows the same distribution.<br>
**Multiple comparisions**: This when you compare one group with many other groups. Multiple comparisions need to be corrected for the many comparisions. Multiple comparisions are usually run a posthoc tests for ANOVA or on RNA expression data. Multiple comparisions are not a substitute for ANOVA which I have seen used too many times.<br>
**Bootstraping**: Resampling from a set of data over and over again. You can bootstrap with replacement, you can draw a value multiple times, or without replacement, you cannot draw a value multiple times. Bootstrapping can be used to construct non-parametric statistics. Bootstrapping assumes your data is independent and identically distributed.<br>
**Independent and identically distributed**: Indepedent means that each data point you have was not influenced or related to any other data points that you have in your dataset. Knowledge of one value does not give you information about another value. When this assumption is violated you use within-subject tests. Identically distributed means that all datasets are drawn from the same probability distribution. In a way many statistical tests with categorical independent variables (predictors) are testing whether identically distributed is false; the means are different with the assumption that the variance is the same. Other tests can test whether the variance is different such as Levene's test.<br>
**Null hypothesis**: Baseline assumption that there is no difference in a parameter between different groups. Usually the parameter is the mean for normally distributed data but could be the variance for something like Levene's test.<br>
**Hypothesis testing**: Testing to determine whether the data provided is sufficient to reject some other hypothesis. For most statitical tests we run will be used to determine whether we can reject the null hypothesis, usually the hypothesis that there is no difference between groups. Typically your hypothesis does not tell you the direction or magnitude of the difference. <br>
**Significance testing**: Generating a number, usually the p-value, that we use to determine whether we reject the null hypothesis. Random fact, Karl Pearson and Ronald Fisher are thought to have developed significance testing due to their interest in eugenics and testing for differences between different populations of people to justify their views.<br>
**Sample size**: The number of samples or n in your dataset.<br>
**Effect size**: The number tells you how large your difference or effect is in your statistical test. These include $r^2$, Cohen's d, eta/partial-eta squared, omega/partial-omega squared, odds ratio, or you can even use confidence intervals as an effect size though they are not considered an effect size.
**Residuals**: The difference between you fitted model and the actual data at for each set of X values. Linear regression models assume that residuals have a mean of zero and a standard deviation of one. Residuals are one the key ways that we assess the fit of a linear regression model even if the y is not normally distributed.