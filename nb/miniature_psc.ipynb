{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniature/spontaneous postsynaptic currents\n",
    "Recording and analyzing miniature/spontaneous postsynaptic currents (m/sPSCs) is one of the most common experiments in patch clamp electrophysiology. m/sPSCs are ionic currents from AMPA, NMDA, glycine, or GABAA receptors that are evoked due to release of a single or multiple synaptic vesicles. We will cover both mPSCs and sPSCs, and go over how to analyze PSC events. While this chapter focuses on PSCs, most of the theory applies to miniature/spontaneous postsynaptic potentials (PSPs) as well. There are several experiments you can do that utilize PSCs; synapse number, silent synapses, excitatory/inhibitory ratio, synaptic multiplicity, and changes in synaptic release regulated by other non-ionic receptors.\n",
    "\n",
    "Postsynaptic currents have a very specific shape. This shape can modeled by multiply two exponentials of opposite direction together. This shape allows PSCs to act as coincidence detectors. The sharp rise allows PSCs to be temporally precise. The long decay allows PSCs to overlap in time and summate to drive an action potential. The shorter the decay the shorter the time window PSCs have to summate. Different cells and different PSC types have different rise rates and decay rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miniature postsynaptic currents\n",
    "Miniature postsynaptic currents (mPSCs) are ionic currents evoked from the release of a single synaptic vesicle {cite:p}`del_castillo_quantal_1954`. Frequency of mPSCs is used as a proxy for the number of functional synapses (synapses that have presynaptic input) that contain the receptor of interest (but not necessarily the number of synapses). The interpretation of mPSC data depends on what receptor you are recording from. If you are recording mEPSCs from AMPARs then you are likely getting the number of \"active\" or \"non-silent\" synapses. If you are recording NMDARs could be looking at the number of silent synapses (only if you compare to the AMPAR mEPSCs). If you are recording mIPSCs then you are getting the number of inhibitory synapses. With mIPSCs you could be getting GABAAR or GlyR. One important caveat of mPSCs is that you are not getting where the presynaptic input is coming from. If you want projection specific synaptic input you need to run a different type of experiment than we will be covering here, but is covered later in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal and external solutions\n",
    "You will need to block spontaneous activity including tetrodotoxin (TTX) in the bath. Preferably you would also use an internal solution that contains cesium and QX-314. For more information on internals see the [internal solutions](internal-solutions) chapter. Depending on the receptor current(s) you want to record you will need to block certain other receptors by including specific drugs in the external solutions. For more information on externals see the [external solutions](external-solutions) chapter.\n",
    "\n",
    "### How should you record mPSCs\n",
    "mPSCs are currents which means you are recording in voltage-clamp mode. This means that the amplifier will injecct current into the cell to keep it at your choosen holding voltage. Any changes in current means that the cells had a change in voltage that the amplifier is counter acting\n",
    "\n",
    "There are two primary ways you can record mPSCs. One way is you can record continuously for about 3-5 minutes. Technically speaking this is the easiest method since you have a single recording and pretty much any simple recording software will implement this method. The second way is you can record 20-40 sweeps/acquisitions of 5-15 seconds each. Each of these acquisitions act as a kind of technical replicate. This method allows you to discard bad portions of the recording. Usually when you get proficient at patching you will rarely have bad recordings however sometimes you get 30 seconds where there is an unstable seal, digital cell phone noise, your bath gets too low, you get pump/vacuum noise or other issues. When this happens it is fine to discard the 5 or so acquisitions that are bad. You can create acquisitions from continuous recordings by splitting to get the same benefits of the sweeps/acquisitions method.\n",
    "\n",
    "Filtering and sample rate are the other important consideration for capturing mPSCs. You generally want the sample rate to be 3-4x greater than the filter cutoff you are using with the filter cutoff determining the high frequency you are interested in. While signal theory says you have to have the sample rate 2x greater than the high frequency you are interested in, generally to capture that high frequency well you need to sample 3-4x times that rate. In general mPSCs are recorded at a 10000 Hz with a 3000 Hz lowpass cutoff. A 10000 Hz sample rate is high enough to capture the rise of a mPSCs which are fairly quick, especially for mEPSCs on parvalbumin interneurons. 10000 Hz is also a good trade off between accuracy of the signal and digital storage space. In modern times you could realistically record at 20000 Hz with no storage space issues. I suggest a mininum sample rate of 10000 Hz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spontaneous postsynaptic currents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing miniature/spontaneous PSCs.\n",
    "There are several important features of m/aPSCs that you will want to analyze. The primary feature is frequency (the number of events occuring every second). Frequency is used as a proxy for the number of synapses that contain the receptor whose currents you are recording. The more mPSCs, the more synapses. However, if there are changes in release probability you could also get a change in frequency without a change in synapse number. To determine whether there are changes in release probability you will need to run some pair-pulse experiments which are described in a later chapter. Another feature is mPSC amplitude. A larger amplitude could mean two things. Larger mPSCs could mean there are more receptors at the postsynaptic element. Alternatively, larger mPSCs could mean that you get less distal synapses due to decreased dendritic length or more synapses close to the cell body with no change in dendritic length (probably less likely to occur). Amplitude of mPSCs decreases the further from the cell body that the event occurs due to dendritic filtering. To rule out one of the interpretations you can use the mEPSC rise rate (amplitude/peak_time-baseline_start) to determine if the rise rate is changed. More distal mPSCs should have a slower rise rate due to dendritic lowpass filtering. Lastly, you can look at the tau or decay rate of the mPSC. Changes in tau are usually due to changes in receptor subunit composition. Tau is especially useful when you need to identify specific cells types. Interneurons, like parvalbumin interneurons, have a very short mPESC tau compared to pyramidal neurons. Tau can also be affected by dendritic filtering.\n",
    "\n",
    "There are several steps to processing acquisitions to find mEPSCS.\n",
    "1. Filter the acquisition\n",
    "2. Convolution/deconvolution to find events\n",
    "3. Clean events\n",
    "\n",
    "First we are going to import some python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "from scipy import signal, optimize, fft\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import row, column\n",
    "from bokeh.models import ColumnDataSource, CustomJS, Slider, Spinner\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to load the data. All the data is stored on json files. While this file type is not the most practical for storing electrophysiological data, it is the very convenient since it does not require any third party python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"https://raw.githubusercontent.com/LarsHenrikNelson/PathClampHandbook/refs/heads/main/data/mepsc/\"\n",
    "exp_dict = {}\n",
    "for index in range(1, 6):\n",
    "    with urllib.request.urlopen(temp_path + f\"{index}.json\") as url:\n",
    "        temp = json.load(url)\n",
    "        temp[\"array\"] = np.array(temp[\"array\"])[:100000]\n",
    "        exp_dict[index] = temp\n",
    "x_array = np.arange(len(exp_dict[1][\"array\"])) / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is look through your data just to see what it looks like. For reference the data in this tutorial is from a layer 5 cell in the ACC of a P16 mouse. \n",
    "- The recorded data is usually in pA, as is the case for this data.\n",
    "- It can be hard to see the events, however this is a parvalbumin interneuron and has very large mEPSC events.\n",
    "- The acquisition mean hovers around -40 pA. This is the amount of current injected to keep the cell at the holding voltage which in this case is -70 mV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Initial data\n",
    "source = ColumnDataSource(data={\"x\": np.arange(100000)/10, \"y\": exp_dict[1][\"array\"][:100000]})\n",
    "\n",
    "# Create a plot\n",
    "plot = figure(x_axis_label=\"Time (ms)\", y_axis_label=\"Current (pA)\")\n",
    "plot.line(\"x\", \"y\", source=source, line_color=\"black\")\n",
    "spinner = Spinner(title=\"Acquisition\", low=1, high=5, step=1, value=1, width=80)\n",
    "\n",
    "# JavaScript callback to fetch JSON data and update plot\n",
    "callback = CustomJS(\n",
    "    args=dict(source=source, spinner=spinner),\n",
    "    code=\"\"\"\n",
    "    let val = spinner.value\n",
    "    let URL = `https://raw.githubusercontent.com/LarsHenrikNelson/PathClampHandbook/refs/heads/main/data/mepsc/${val}.json`\n",
    "    fetch(URL)\n",
    "    .then(response => response.json())\n",
    "    .then(data => {\n",
    "        console.log(data)\n",
    "        source.data.y = data[\"array\"].slice(0,100000);\n",
    "        source.change.emit();\n",
    "    })\n",
    "    .catch(error => console.error('Error fetching data:', error));\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "# Add a button to trigger the callback\n",
    "spinner.js_on_change(\"value\", callback)\n",
    "\n",
    "# Layout and show\n",
    "layout = column(spinner, plot)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define some important features of the acquisition so that we can reuse the settings throughout the analysis. It is important to note that the all the parameters are going to be in samples. The current files were recorded at 10000 Hz so we multiply the time we want, in milliseconds, time by 10 or divide sample number by 10 to get to milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_start = 0\n",
    "baseline_end = 3000\n",
    "sample_rate = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the acquisition\n",
    "First thing we need to do is filter the acquisition. There are two ways to filter. You can remove the baseline then lowpass filter or you can apply a bandpass filter. Filtering achieves two goals. The first is remove the DC offset. The DC offset is actually the current need to clamp the voltage. The second goal is to remove extraneous high frequency noise which can hinder the analysis. You can also use notch filter to remove 60 Hz, however I recommend finding ways to reduce 60 Hz before you even record. Notch filters can introduce artifacts into and distort your signal.\n",
    "\n",
    "For this tutorial we will use remove the baseline by taking the mean and use a zero-phase Butterworth filter with an order 4 filter and a lowpass cutoff of 600 Hz and compare that to a bandpass cutoff of [0.01, 600] to remove the DC offset and high frequency noise. If you want to learn more about filtering checkout the chapter on filtering. For the PSC tutorial we are going to skip the RC check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter method 1: Remove the baseline and lowpass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in exp_dict.values():\n",
    "    baseline = np.mean(value[\"array\"])\n",
    "    temp = value[\"array\"] - baseline\n",
    "    value[\"holding_current\"] = baseline\n",
    "    sos = signal.butter(\n",
    "            4, Wn=600, btype=\"lowpass\", output=\"sos\", fs=sample_rate\n",
    "        )\n",
    "    filt_array = signal.sosfiltfilt(sos, temp)\n",
    "    value[\"lowpass\"] = filt_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter method 2: Bandpass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in exp_dict.values():\n",
    "    baseline = np.mean(value[\"array\"])\n",
    "    temp = value[\"array\"] - baseline\n",
    "    sos = signal.butter(\n",
    "            4, Wn=[0.01, 600], btype=\"bandpass\", output=\"sos\", fs=sample_rate\n",
    "        )\n",
    "    filt_array = signal.sosfiltfilt(sos, temp)\n",
    "    value[\"bandpass\"] = filt_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the two types of filtering. Some things to notice.\n",
    "- Both methods filter almost identically and substantially reduce the noise.\n",
    "- Both methods reduce the size of the mEPSC.\n",
    "- With a zero-phase filter we can prevent any phase changes so the timing of the baseline and peak of the mEPSC events is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "x = np.arange(len(exp_dict[1][\"array\"]))\n",
    "source = ColumnDataSource({\"x\": x, \"y\": exp_dict[1][\"array\"]-exp_dict[1][\"holding_current\"], \"bandpass\": exp_dict[1][\"bandpass\"], \"lowpass\": exp_dict[1][\"lowpass\"]})\n",
    "p1 = figure(title=\"Lowpass\", height=300, width=600, output_backend=\"webgl\")\n",
    "_ = p1.line(x=\"x\", y=\"y\", source=source, line_color=\"black\", line_width=1)\n",
    "_ = p1.line(x=\"x\", y=\"lowpass\", source=source, line_color=\"red\", line_width=1)\n",
    "p2 = figure(title=\"Bandpass\", height=300, width=600, output_backend=\"webgl\", x_range=p1.x_range)\n",
    "_ = p2.line(x=\"x\", y=\"y\", source=source, line_color=\"black\", line_width=1)\n",
    "_ = p2.line(x=\"x\", y=\"bandpass\", source=source, line_color=\"red\", line_width=1)\n",
    "show(column(p1, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template matching vs deconvolution\n",
    "There are main two ways, template matching (correlation) and deconvolution, to identify m/sPSCs, both need a template PSC. Convolution is the \"traditional\" way however I have seen quite a few new papers using a deconvolution method since it is less dependent on the exact template shape. The deconvolution technique was first proposed by Pernia-Andrade {cite:p}`pernia-andrade_deconvolution-based_2012`. We will cover both methods to see how each works. If you want to learn more about convolution and deconvolution check out the signal processing chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a template\n",
    "First we are going to create a template PSC. We will use the same template for each method. The template is a double exponential with a exponential rise multiplied by an exponential decay. You can see the template equation below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_template(\n",
    "    amplitude: int | float = -20,\n",
    "    rise_tau: int | float = 0.3,\n",
    "    decay_tau: int | float = 5,\n",
    "    risepower: int | float = 0.5,\n",
    "    length: int | float = 30,\n",
    "    spacer: int | float = 1.5,\n",
    "    sample_rate: int = 10000,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Creates a template based on several factors.\n",
    "\n",
    "    Args:\n",
    "        amplitude (float): Amplitude of template\n",
    "        rise_tau (float): Rise tau (ms) of template\n",
    "        decay_tau (float): Decay tau (ms) of template\n",
    "        risepower (float): Risepower of template\n",
    "        length (float): Length of time (ms) for template\n",
    "        spacer (int, optional): Delay (ms) until template starts. Defaults to 1.5.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Numpy array of the template.\n",
    "    \"\"\"\n",
    "    if rise_tau == decay_tau:\n",
    "        rise_tau += 0.001\n",
    "    s_r_c = sample_rate / 1000\n",
    "    rise_tau = int(rise_tau * s_r_c)\n",
    "    decay_tau = int(decay_tau * s_r_c)\n",
    "    length = int(length * s_r_c)\n",
    "    spacer = int(spacer * s_r_c)\n",
    "    template = np.zeros(length + spacer)\n",
    "    t_length = np.arange(0, length)\n",
    "    offset = len(template) - length\n",
    "    Aprime = (decay_tau / rise_tau) ** (rise_tau / (rise_tau - decay_tau))\n",
    "    y = (\n",
    "        amplitude\n",
    "        / Aprime\n",
    "        * ((1 - np.exp(-t_length / rise_tau)) ** risepower * np.exp((-t_length / decay_tau)))\n",
    "    )\n",
    "    template[offset:] = y\n",
    "    return template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the interactive plot below to see how the different parameters effect the template mEPSC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "template = create_template(decay_tau=2.5)\n",
    "source = ColumnDataSource({\"x\": np.arange(template.size)/10, \"y\": template})\n",
    "\n",
    "plot = figure(width=400, height=400)\n",
    "\n",
    "plot.line(\"x\", \"y\", source=source, line_width=3, line_alpha=0.6, line_color=\"black\")\n",
    "\n",
    "rise_tau = Slider(start=0.5, end=10, value=0.5, step=0.5, title=\"Rise tau (ms)\")\n",
    "risepower = Slider(start=0.5, end=10, value=0.5, step=0.25, title=\"Rise power\")\n",
    "decay_tau = Slider(start=0.5, end=50, value=3, step=0.5, title=\"Decay tau (ms)\")\n",
    "amplitude = Slider(start=-60, end=-5, value=-10, step=0.5, title=\"Amplitude (pA)\")\n",
    "length = Slider(start=20, end=70, value=30, step=1, title=\"Length (ms)\")\n",
    "\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        source=source,\n",
    "        rise_tau=rise_tau,\n",
    "        decay_tau=decay_tau,\n",
    "        amplitude=amplitude,\n",
    "        length=length,\n",
    "        risepower=risepower,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    if (rise_tau === decay_tau) {\n",
    "        rise_tau += 0.001;\n",
    "    }\n",
    "    const s_r_c = 10\n",
    "    const rt = Math.round(rise_tau.value * s_r_c)\n",
    "    const dt = Math.round(decay_tau.value * s_r_c)\n",
    "    const len = Math.round(length.value * s_r_c)\n",
    "    const spacer = 15\n",
    "    const y = new Array(len+spacer).fill(0)\n",
    "    const t_length = Array.from({ length: len }, (_, i) => 0 + i)\n",
    "    const Aprime = (dt / rt) ** (rt / (rt - dt))\n",
    "    const temp_y = t_length.map(x => {\n",
    "        return amplitude.value / Aprime * ((1 - Math.exp(-x / rt)) ** risepower.value * Math.exp(-x / dt))\n",
    "    })\n",
    "    y.splice(spacer, temp_y.length, ...temp_y);\n",
    "    const x = Array.from({ length: len+spacer }, (_, i) => 0 + i/10)\n",
    "    source.data = { x, y }\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "rise_tau.js_on_change(\"value\", callback)\n",
    "decay_tau.js_on_change(\"value\", callback)\n",
    "amplitude.js_on_change(\"value\", callback)\n",
    "length.js_on_change(\"value\", callback)\n",
    "\n",
    "show(row(plot, column(rise_tau, decay_tau, amplitude, length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the analysis of mEPSCs on parvalbumin interneurons we just need to modify the decay rate of the template since PV cell mEPSCs tend to have a very fast decay compared to other cell types (think about why this might be related to the function of PV cells in the larger circuit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = create_template(decay_tau=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Template matching\n",
    "Template matching essentially slides the template along the acquisition and correlates the template with the segment of the acquisition it is currently aligned with. I do some extract work to ensure the template matched array is zero phase relative to the original array. This makes it easier to find PSC events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in exp_dict.values():\n",
    "    temp_match = np.correlate(value[\"lowpass\"], template, mode=\"full\")\n",
    "    temp_match = temp_match[template.size-1:]\n",
    "    value[\"temp_match\"] = temp_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Deconvolution\n",
    "Deconvolution essetially divides out the template from the acquisition. Deconvolution is inherently noisy so the deconvolve output has to be filtered to even see the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in exp_dict.values():\n",
    "    kernel = np.hstack((template, np.zeros(len(value[\"lowpass\"]) - len(template))))\n",
    "    template_fft = fft.fft(kernel)\n",
    "    signal_fft = fft.fft(value[\"lowpass\"])\n",
    "    temp = signal_fft / template_fft\n",
    "    temp = np.real(fft.ifft(temp))\n",
    "    sos = signal.butter(\n",
    "            4, Wn=300, btype=\"lowpass\", output=\"sos\", fs=sample_rate\n",
    "        )\n",
    "    deconvolved = signal.sosfiltfilt(sos, temp)\n",
    "    value[\"deconvolved\"] = deconvolved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the two methods.You will notice that peaks end up in approximately the same place and are positive. These peaks are where putative mEPSCs are occuring. There are two major differences. One is that the deconvolved array has a stable baseline which can make event finding easier. The second is that peaks in the deconvolved array are narrower but shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "x = np.arange(len(exp_dict[index][\"array\"]))\n",
    "source = ColumnDataSource({\"x\": x, \"temp_match\": exp_dict[index][\"temp_match\"], \"deconvolved\": exp_dict[index][\"deconvolved\"]})\n",
    "p1 = figure(title=\"Template match\", height=300, width=600, output_backend=\"webgl\")\n",
    "_ = p1.line(x=\"x\", y=\"temp_match\", source=source, line_color=\"black\", line_width=1)\n",
    "p2 = figure(title=\"Deconvolved\", height=300, width=600, output_backend=\"webgl\", x_range=p1.x_range)\n",
    "_ = p2.line(x=\"x\", y=\"deconvolved\", source=source, line_color=\"black\", line_width=1)\n",
    "show(column(p1, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding events\n",
    "\n",
    "The next step involves finding peaks where . For each method we will need to define some threshold so that we don't pick up on the small peaks that are noise. For finding events we will use a way I devised that helps create a per acquisition normalization which allows using a single threshold value for difference acquisitions. First we will get the RMS without the peaks. We will use that to adjust a single threshold value. Finally we will use Scipy find_peaks to find the events above the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentile_rms(deconvolved_array: np.ndarray) -> float | float:\n",
    "    # Get the top and bottom 2.5% cutoff.\n",
    "    bottom, top = np.percentile(deconvolved_array, [2.5, 97.5])\n",
    "\n",
    "    # Return the middle values.\n",
    "    middle = np.hstack(\n",
    "        deconvolved_array[\n",
    "            np.argwhere((deconvolved_array > bottom) & (deconvolved_array < top))\n",
    "        ]\n",
    "    )\n",
    "    # Calculate the mean and rms.\n",
    "    mu = np.mean(middle)\n",
    "    rms = np.sqrt(np.mean(np.square(middle - mu)))\n",
    "\n",
    "    return mu, rms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Template matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = 3.5\n",
    "mini_spacing = 100\n",
    "\n",
    "for value in exp_dict.values():\n",
    "    mu, rms = get_percentile_rms(value[\"temp_match\"])\n",
    "    peaks, _ = signal.find_peaks(\n",
    "            value[\"temp_match\"] - mu,\n",
    "            height=sensitivity * (rms),\n",
    "            distance=mini_spacing,\n",
    "            prominence=rms,\n",
    "        )\n",
    "    value[\"temp_match_events\"] = peaks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = 4\n",
    "mini_spacing = 100\n",
    "\n",
    "for value in exp_dict.values():\n",
    "    mu, rms = get_percentile_rms(value[\"deconvolved\"])\n",
    "    peaks, _ = signal.find_peaks(\n",
    "            value[\"deconvolved\"] - mu,\n",
    "            height=sensitivity * (rms),\n",
    "            distance=mini_spacing,\n",
    "            prominence=rms,\n",
    "        )\n",
    "    value[\"deconvolved_events\"] = peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_match_x = exp_dict[index][\"temp_match_events\"]\n",
    "deconvolved_x = exp_dict[index][\"deconvolved_events\"]\n",
    "_ = p1.scatter(temp_match_x, exp_dict[index][\"temp_match\"][temp_match_x], color=\"orange\")\n",
    "_ = p2.scatter(deconvolved_x, exp_dict[index][\"deconvolved\"][deconvolved_x], color=\"magenta\")\n",
    "show(column(p1, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see where these events are in the original acquisition. Most of the time you will see that purple and orange dots are falling just before the event. You will notice that many of the events found from both methods are in the same place but the smaller event locations seem to be most different between the two methods. You will notice that some locations do not seem to have an event and that is okay because these will be screened out at the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_match_x = exp_dict[index][\"temp_match_events\"]\n",
    "deconvolved_x = exp_dict[index][\"deconvolved_events\"]\n",
    "array = exp_dict[index][\"array\"]\n",
    "f = figure(title=\"Template match\", height=300, width=600, output_backend=\"webgl\")\n",
    "_ = f.line(np.arange(array.size), array, color=\"black\")\n",
    "_ = f.scatter(temp_match_x, array[temp_match_x], color=\"orange\")\n",
    "_ = f.scatter(deconvolved_x, array[deconvolved_x], color=\"magenta\")\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing events\n",
    "\n",
    "The analysis from this point on will get much harder. The are many parameters for many events that we will need to assess and keep track of. There are several ways that you can optimally store and retrieve data in Python. We will primarily use Python dictionaries which are general container, however if you want to create a program your self I would recommend using classes. Since this tutorial is focused on analyzing the data rather than developing an optimal program we will stick with the basics.\n",
    "\n",
    "One important factor to note is that for any method analyzing events noise is always an issue. The quality of the events and the parameters we retrieve will depend on how noisy the acquisitions are. Noise acquisitions make it hard to find the baseline and peak of events. Noise makes it hard to determine what is a real event and what a bad event. For this reason good mini analysis programs tend to let you add and remove events as well as change the baseline and peak of events. There are many do not have interactive features. This tutorial is limited in that it will be very hard to modify event parameters that are incorrect since we do not have a fully interative UI. However, I think that it is extremely useful to see and think about how events are found.\n",
    "\n",
    "For the next step we are going to analyze the events we have found.\n",
    "We will go through the following steps:\n",
    "1. Create the event start and stop\n",
    "2. Find the event peak.\n",
    "3. Find the baseline. You need the baseline to calculate the amplitude and after finding the baseline.\n",
    "4. Find the event amplitude.\n",
    "5. Find the event rise time and rise rate.\n",
    "6. Find the event decay with a simple estimate.\n",
    "7. Find the event decay with curve fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the event start and stop\n",
    "The method of finding events that we have used usually places the event marker just before the start of the event. We will create a window around the event. We will need to define how long we want an event. For mEPSCs 30 ms is usually long enough. We will also need to define how much earlier the event should start compared to the event position. For now 2 ms is good enough. Because we are working in samples we will have to convert both of the times to samples. Since our sample rate is 10000 Hz we need to multiply each time by 10 which we will use many times so we will save it as a variable s_r_c (sample rate correction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_event(event_array: np.ndarray, event_position: int, event_length: int, offset: int):\n",
    "    array_start = int(event_position - offset)\n",
    "    end = int(event_position + event_length)\n",
    "    if end > len(event_array) - 1:\n",
    "        array_end = len(event_array) - 1\n",
    "    else:\n",
    "        array_end = end\n",
    "    return array_start, array_end\n",
    "\n",
    "s_r_c = 10\n",
    "offset = 2 * s_r_c\n",
    "event_length = 30 * s_r_c\n",
    "for value in exp_dict.values():\n",
    "    value[\"events\"] = []\n",
    "    for p in value[\"deconvolved_events\"]:\n",
    "        event = {}\n",
    "        start, stop = create_event(value['array'], p, event_length, offset)\n",
    "        event[\"start\"] = start\n",
    "        event[\"stop\"] = stop\n",
    "        event[\"event_position\"] = p\n",
    "        value[\"events\"].append(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one the acquisitions to see what our events look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = exp_dict[1][\"array\"]\n",
    "events = exp_dict[1][\"events\"]\n",
    "mfig = figure(height=250, width=400)\n",
    "x = np.arange(array.size)\n",
    "mfig.line(x, y, color=\"black\")\n",
    "for i in events:\n",
    "    start = i[\"start\"]\n",
    "    stop = i[\"stop\"]\n",
    "    mfig.line(x[start:stop], y[start:stop], color=\"magenta\")\n",
    "show(mfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the peak\n",
    "\n",
    "There are a couple ways to find the peaks of the event. If your event placement is good enough you can just use min or max depending on the direction of currents/voltages. However, this fails if your event window contains another event which is not that uncommon or if you have noise in your recording. I use an interative method to find the peak. First we use a prominence based peak finding method. If any peaks are found then we will check that we peak we found is not just noise. If that fails then we use a order based peak finding where a peak is just a value that is larger than all the values within 4 ms on both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_corr(event_array, peak: int, s_r_c):\n",
    "    peaks_2 = signal.argrelextrema(\n",
    "        event_array[:peak],\n",
    "        comparator=np.less,\n",
    "        order=int(0.4 * s_r_c),\n",
    "    )[0]\n",
    "    peaks_2 = peaks_2[peaks_2 > peak - 4 * s_r_c]\n",
    "    if len(peaks_2) == 0:\n",
    "        event_peak_x = peak\n",
    "    else:\n",
    "        peaks_3 = peaks_2[\n",
    "            event_array[peaks_2] < 0.85 * event_array[peak]\n",
    "        ]\n",
    "        if len(peaks_3) == 0:\n",
    "            event_peak_x = peak\n",
    "        else:\n",
    "            event_peak_x = peaks_3[0]\n",
    "    event_peak_y = event_array[int(event_peak_x)]\n",
    "    return event_peak_x, event_peak_y\n",
    "\n",
    "def find_peak_alt(event_array, offset):\n",
    "    peaks = signal.argrelextrema(\n",
    "        event_array, comparator=np.less, order=int(3 * s_r_c)\n",
    "    )[0]\n",
    "    peaks = peaks[peaks > offset]\n",
    "    if len(peaks) == 0:\n",
    "        event_peak_x = np.nan\n",
    "        event_peak_y = np.nan\n",
    "    else:\n",
    "        event_peak_x, event_peak_y = peak_corr(peaks[0])\n",
    "    return event_peak_x, event_peak_y\n",
    "\n",
    "def find_peak(event_array, offset, s_r_c):\n",
    "    peaks, _ = signal.find_peaks(\n",
    "        -1 * event_array,\n",
    "        prominence=4,\n",
    "        width=0.4 * 10,\n",
    "        distance=int(3 * 10),\n",
    "    )\n",
    "    peaks = peaks[peaks > offset]\n",
    "    if len(peaks) == 0:\n",
    "        event_peak_x, event_peak_y = find_peak_alt(event_array, offset)\n",
    "    else:\n",
    "        event_peak_x, event_peak_y = peak_corr(event_array, peaks[0], s_r_c)\n",
    "    return event_peak_x, event_peak_y\n",
    "\n",
    "for value in exp_dict.values():\n",
    "    for p in value[\"events\"]:\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the baseline\n",
    "\n",
    "There are two ways to find the baseline. One is to use a slope and find when the slope stops increasing. This method needs several additions to make it work well. The other way is to assume that the baseline of your event is around 0 mV. The problem with this method is that if your acquisition meanders around 0 mV you can find very weird baselines. We will use the slope method with modifications I have found work very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_baseline(array):\n",
    "    baselined_array = event_array - np.max(\n",
    "        event_array[: _event_peak_x]\n",
    "    )\n",
    "    peak = int(_event_peak_x - _array_start)\n",
    "    # search_start = np.argwhere(\n",
    "    #     baselined_array[:peak] > 0.5 * event_peak_y\n",
    "    # ).flatten()\n",
    "    search_start = np.argwhere(\n",
    "        baselined_array[:peak] > 0.35 * event_peak_y\n",
    "    ).flatten()\n",
    "    if search_start.size > 0:\n",
    "        slope = (event_array[search_start[-1]] - event_peak_y) / (\n",
    "            peak - search_start[-1]\n",
    "        )\n",
    "        new_slope = slope + 1\n",
    "        i = search_start[-1]\n",
    "        while new_slope > slope and i > 0:\n",
    "            slope = (event_array[i] - event_peak_y) / (peak - i)\n",
    "            i -= 1\n",
    "            new_slope = (event_array[i] - event_peak_y) / (peak - i)\n",
    "        baseline_start = signal.argrelmax(\n",
    "            baselined_array[int(i - 1 * s_r_c) : i + 2], order=2\n",
    "        )[0]\n",
    "        if baseline_start.size > 0:\n",
    "            temp = int(baseline_start[-1] + (i - 1 * s_r_c))\n",
    "            if temp < 0:\n",
    "                temp = 0\n",
    "        else:\n",
    "            temp = int(baseline_start.size / 2 + (i - 1 * s_r_c))\n",
    "            if temp < 0:\n",
    "                temp = 0\n",
    "        event_start_x = [temp]\n",
    "        event_start_y = event_array[temp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephysbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
