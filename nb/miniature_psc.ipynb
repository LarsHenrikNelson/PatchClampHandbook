{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniature/spontaneous postsynaptic currents\n",
    "Recording and analyzing miniature/spontaneous postsynaptic currents (m/sPSCs) is one of the most common experiments in patch clamp electrophysiology. m/sPSCs are ionic currents from AMPA, NMDA, glycine, or GABAA receptors that are evoked due to release of a single or multiple synaptic vesicles. We will cover both mPSCs and sPSCs, and go over how to analyze these small PSC events. While this chapter focuses on PSCs, most of the theory applies to miniature/spontaneous postsynaptic potentials (PSPs) as well. There are several experiments you can do that utilize PSCs; synapse number, silent synapses, excitatory/inhibitory ratio, synaptic multiplicity, and changes in synaptic release regulated by other non-ionic receptors.\n",
    "\n",
    "Postsynaptic currents have a very specific shape. This shape can modeled by multiply two exponentials of opposite direction together. This shape allows PSCs to act as coincidence detectors. The sharp rise allows PSCs to be temporally precise. The long decay allows PSCs to overlap in time and summate to drive an action potential. The shorter the decay the shorter the time window PSCs have to summate. Different cells and different PSC types have different rise rates and decay rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miniature postsynaptic currents\n",
    "Miniature postsynaptic currents (mPSCs) are ionic currents evoked from the release of a single synaptic vesicle {cite:p}`del_castillo_quantal_1954`. Frequency of mPSCs is used as a proxy for the number of functional synapses (synapses that have presynaptic input) that contain the receptor of interest (but not necessarily the number of synapses). The interpretation of mPSC data depends on what receptor you are recording from. If you are recording mEPSCs from AMPARs then you are likely getting the number of \"active\" or \"non-silent\" synapses (i.e. the number of presynaptic elements that have a postsynaptic elements with the receptor you are interested in). If you are recording NMDARs you will get the number of synapses with NMDAR receptors. If you are recording mIPSCs then you are getting the number of inhibitory synapses. With mIPSCs you could be getting GABAAR or GlyR unlees you block one or the other. One important caveat of mPSCs is that you are not getting where the presynaptic input is coming from. If you want projection specific synaptic input you need to run a different type of experiment than we will be covering in a later chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal and external solutions\n",
    "To record mEPSCs you will need to block spontaneous activity by including tetrodotoxin (TTX) in the bath. Preferably you would also use an internal solution that contains Cs+ (blocks potassium channels) and QX-314 (blocks voltaged-gated Na+ channels) to help with space clamp. For more information on internals see the [internal solutions](internal_solutions) chapter. Depending on the receptor current(s) you want to record you will need to block other receptors by including specific drugs in the external solutions. For more information on externals see the [external solutions](external_solutions) chapter.\n",
    "\n",
    "### How should you record mPSCs\n",
    "mPSCs are currents which means you are recording in voltage-clamp mode. This means that the amplifier will injecct current into the cell to keep it at your choosen holding voltage. Any changes in current means that the cells had a change in voltage that the amplifier is counter acting. When you get an inward current of positive ions the amplifier will inject a negative current to keep the cell at your chosen holding voltage. \n",
    "\n",
    "There are two primary ways you can record mPSCs. One way is you can record continuously for about 3-5 minutes. Technically speaking this is the easiest method since you have a single recording and pretty much any simple recording software will implement this method. The second way is you can record 20-40 sweeps/acquisitions of 5-15 seconds each. Each of these acquisitions act as a kind of technical replicate. This method allows you to discard bad portions of the recording. Usually when you get proficient at patching you will rarely have bad recordings however sometimes you get 30 seconds where there is an unstable seal, digital cell phone noise, your bath gets too low, you get pump/vacuum noise or other issues. When this happens it is fine to discard the 5 or so acquisitions that are bad. You can create acquisitions from continuous recordings by splitting to get the same benefits of the sweeps/acquisitions method.\n",
    "\n",
    "Filtering and sample rate are the other important considerations for capturing mPSCs. You generally want the sample rate to be 3-4x greater than the filter cutoff you are using with the filter cutoff determining the high frequency you are interested in. While signal theory says you have to have the sample rate 2x greater than the high frequency you are interested in, generally to capture that high frequency well you need to sample 3-4x times that rate to get a good resolution signal. In general mPSCs are recorded at a 10000 Hz with a 3000 Hz lowpass cutoff. A 10000 Hz sample rate is high enough to capture the rise of a mPSCs which are fairly quick, especially for mEPSCs on parvalbumin interneurons. 10000 Hz is also a good trade off between accuracy of the signal and digital storage space. In modern times you could realistically record at 20000 Hz with no storage space issues. I suggest a mininum sample rate of 10000 Hz.\n",
    "\n",
    "### Interpreting mPSCs\n",
    "There are several important features of mPSCs that you will want to analyze. PSC frequency is used as a proxy for the number of synapses that contain the receptor whose currents you are recording. The more mPSCs, the more synapses. However, if there are changes in release probability you could also get a change in frequency without a change in synapse number. To determine whether there are changes in release probability you will need to run some pair-pulse experiments which are described in a later chapter. Another feature is PSC amplitude. A larger amplitude could mean two things. Larger mPSCs could mean there are more receptors at the postsynaptic element (thus a larger current). Alternatively, larger mPSC amplitude could mean that you get less distal synapses due to decreased dendritic length or more synapses close to the cell body with no change in dendritic length. More distal mPSCs should have a slower rise rate due to dendritic lowpass filtering. Lastly, you can look at the tau or decay rate of the mPSC. Changes in tau are usually due to changes in receptor subunit composition. Tau is especially useful when you need to identify specific cells types. Interneurons, like parvalbumin interneurons, have a very short mPESC tau compared to pyramidal neurons. Tau can also be affected by dendritic filtering.\n",
    "\n",
    "Lastly, most of the data you get from mPSCs will be lognormal, exponential or gamma distributed. Rarely will you get normally distributed data. This has big implication for downstream analysis. For example the mean of a lognormal distribution is just the geometric mean: 10**np.mean(np.log10(data)). If your data is gamma distributed then log transforming with just skew the data in opposite direction which makes it much harder to get good measure of central tendency, however a square root transform also works well. If you want to learn more about distributions check out the [distributions chapter](distributions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spontaneous postsynaptic currents\n",
    "Unlike mEPSCs, you do not want to include TTX in the bath since you are trying to get neurally evoked PSCs. You can use a Cs+ based internal with QX-314 though to improve space clamp. One reason to record sPSCs is to see how different receptors may change synaptic input to a cell by changing the activity/release probability of the presynaptic neurons/axons. sPSC experiments usually involve recording baseline sPSCS for 10 min, then flowing in your drug/peptide for 10 min and finally washing out the drug/peptide for 10 min. Changes in sPSC frequency suggest that the presynaptic neuron is affected by whatever drug you applied to the bath. If you just see a change in the holding current (the amount of current it takes to hold your cell at the mV you are holding it at) then there is likely a postsynaptic effect of the peptide. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several steps to processing acquisitions to find PSCS (in this case mEPSCs).\n",
    "1. Filter the acquisition\n",
    "2. Convolution/deconvolution to find events\n",
    "3. Clean events\n",
    "\n",
    "First we are going to import some python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import Checkbox, ColumnDataSource, CustomJS, Select, Slider, Spinner\n",
    "from bokeh.plotting import figure\n",
    "from scipy import fft, optimize, signal, stats\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to load the data. All the data is stored on json files. While this file type is not the most practical for storing electrophysiological data, it is the very convenient since it does not require any third party python packages and can be loaded from a remote source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = (\n",
    "    \"https://cdn.jsdelivr.net/gh/LarsHenrikNelson/PatchClampHandbook/data/mepsc/\"\n",
    ")\n",
    "exp_dict = {}\n",
    "for index in range(1, 6):\n",
    "    with urllib.request.urlopen(temp_path + f\"{index}.json\") as url:\n",
    "        temp = json.load(url)\n",
    "        temp[\"array\"] = np.array(temp[\"array\"])[:100000]\n",
    "        exp_dict[index] = temp\n",
    "x_array = np.arange(len(exp_dict[1][\"array\"])) / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is look through your data just to see what it looks like. For reference the data in this tutorial is from a layer 5 cell in the ACC of a P16 mouse. \n",
    "- The recorded data is usually in pA, as is the case for this data.\n",
    "- It can be hard to see the events, however this is a parvalbumin interneuron and has very large mEPSC events.\n",
    "- The acquisition mean hovers around -40 pA. This is the amount of current injected to keep the cell at the holding voltage which in this case is -70 mV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Initial data\n",
    "source = ColumnDataSource(\n",
    "    data={\"x\": np.arange(100000) / 10, \"y\": exp_dict[1][\"array\"][:100000]}\n",
    ")\n",
    "\n",
    "# Create a plot\n",
    "plot = figure(x_axis_label=\"Time (ms)\", y_axis_label=\"Current (pA)\")\n",
    "plot.line(\"x\", \"y\", source=source, line_color=\"black\")\n",
    "spinner = Spinner(title=\"Acquisition\", low=1, high=5, step=1, value=1, width=80)\n",
    "\n",
    "# JavaScript callback to fetch JSON data and update plot\n",
    "callback = CustomJS(\n",
    "    args=dict(source=source, spinner=spinner),\n",
    "    code=\"\"\"\n",
    "    let val = spinner.value\n",
    "    let URL = `https://cdn.jsdelivr.net/gh/LarsHenrikNelson/PatchClampHandbook/data/mepsc/${val}.json`\n",
    "    fetch(URL)\n",
    "    .then(response => response.json())\n",
    "    .then(data => {\n",
    "        console.log(data)\n",
    "        source.data.y = data[\"array\"].slice(0,100000);\n",
    "        source.change.emit();\n",
    "    })\n",
    "    .catch(error => console.error('Error fetching data:', error));\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "# Add a button to trigger the callback\n",
    "spinner.js_on_change(\"value\", callback)\n",
    "\n",
    "# Layout and show\n",
    "layout = column(spinner, plot)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define some important features of the acquisition so that we can reuse the settings throughout the analysis. It is important to note that the all the parameters are going to be in samples. The current files were recorded at 10000 Hz so we multiply the time we want, in milliseconds, time by 10 or divide sample number by 10 to get to milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_start = 0\n",
    "baseline_end = 3000\n",
    "sample_rate = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the acquisition\n",
    "First thing we need to do is filter the acquisition. There are two ways to filter. You can remove the baseline then lowpass filter or you can apply a bandpass filter. Filtering achieves two goals. The first is remove the DC offset. The DC offset is actually the current need to clamp the voltage. The second goal is to remove extraneous high frequency noise which can hinder the analysis. You can also use notch filter to remove 60 Hz, however I recommend finding ways to reduce 60 Hz before you even record. Notch filters can introduce artifacts into and distort your signal.\n",
    "\n",
    "For this tutorial we will use remove the baseline by taking the mean and use a zero-phase Butterworth filter with an order 4 filter and a lowpass cutoff of 600 Hz and compare that to a bandpass cutoff of [0.01, 600] to remove the DC offset and high frequency noise. If you want to learn more about filtering checkout the chapter on filtering. For the PSC tutorial we are going to skip the RC check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter method 1: Remove the baseline and lowpass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in exp_dict.values():\n",
    "    baseline = np.mean(value[\"array\"])\n",
    "    temp = value[\"array\"] - baseline\n",
    "    value[\"holding_current\"] = baseline\n",
    "    sos = signal.butter(4, Wn=600, btype=\"lowpass\", output=\"sos\", fs=sample_rate)\n",
    "    filt_array = signal.sosfiltfilt(sos, temp)\n",
    "    value[\"lowpass\"] = filt_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter method 2: Bandpass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in exp_dict.values():\n",
    "    baseline = np.mean(value[\"array\"])\n",
    "    temp = value[\"array\"] - baseline\n",
    "    sos = signal.butter(\n",
    "        4, Wn=[0.01, 600], btype=\"bandpass\", output=\"sos\", fs=sample_rate\n",
    "    )\n",
    "    filt_array = signal.sosfiltfilt(sos, temp)\n",
    "    value[\"bandpass\"] = filt_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the two types of filtering. Some things to notice.\n",
    "- Both methods filter almost identically and substantially reduce the noise.\n",
    "- Both methods reduce the size of the mEPSC.\n",
    "- With a zero-phase filter we can prevent any phase changes so the timing of the baseline and peak of the mEPSC events is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "x = np.arange(len(exp_dict[1][\"array\"]))\n",
    "source = ColumnDataSource(\n",
    "    {\n",
    "        \"x\": x,\n",
    "        \"y\": exp_dict[1][\"array\"] - exp_dict[1][\"holding_current\"],\n",
    "        \"bandpass\": exp_dict[1][\"bandpass\"],\n",
    "        \"lowpass\": exp_dict[1][\"lowpass\"],\n",
    "    }\n",
    ")\n",
    "p1 = figure(title=\"Lowpass\", height=300, width=600, output_backend=\"webgl\")\n",
    "_ = p1.line(x=\"x\", y=\"y\", source=source, line_color=\"black\", line_width=1)\n",
    "_ = p1.line(x=\"x\", y=\"lowpass\", source=source, line_color=\"red\", line_width=1)\n",
    "p2 = figure(\n",
    "    title=\"Bandpass\", height=300, width=600, output_backend=\"webgl\", x_range=p1.x_range\n",
    ")\n",
    "_ = p2.line(x=\"x\", y=\"y\", source=source, line_color=\"black\", line_width=1)\n",
    "_ = p2.line(x=\"x\", y=\"bandpass\", source=source, line_color=\"red\", line_width=1)\n",
    "show(column(p1, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template matching vs deconvolution\n",
    "There are main two ways, template matching (correlation) and deconvolution, to identify m/sPSCs, both need a template PSC. Convolution is the \"traditional\" way however I have seen quite a few new papers using a deconvolution method since it is less dependent on the exact template shape. The deconvolution technique was first proposed by Pernia-Andrade {cite:p}`pernia-andrade_deconvolution-based_2012`. We will cover both methods to see how each works. If you want to learn more about convolution and deconvolution check out the signal processing chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a template\n",
    "First we are going to create a template PSC. We will use the same template for each method. The template is a double exponential with a exponential rise multiplied by an exponential decay. You can see the template equation below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_template(\n",
    "    amplitude: int | float = -20,\n",
    "    rise_tau: int | float = 0.3,\n",
    "    decay_tau: int | float = 5,\n",
    "    risepower: int | float = 0.5,\n",
    "    length: int | float = 30,\n",
    "    spacer: int | float = 1.5,\n",
    "    sample_rate: int = 10000,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Creates a template based on several factors.\n",
    "\n",
    "    Args:\n",
    "        amplitude (float): Amplitude of template\n",
    "        rise_tau (float): Rise tau (ms) of template\n",
    "        decay_tau (float): Decay tau (ms) of template\n",
    "        risepower (float): Risepower of template\n",
    "        length (float): Length of time (ms) for template\n",
    "        spacer (int, optional): Delay (ms) until template starts. Defaults to 1.5.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Numpy array of the template.\n",
    "    \"\"\"\n",
    "    if rise_tau == decay_tau:\n",
    "        rise_tau += 0.001\n",
    "    s_r_c = sample_rate / 1000\n",
    "    rise_tau = int(rise_tau * s_r_c)\n",
    "    decay_tau = int(decay_tau * s_r_c)\n",
    "    length = int(length * s_r_c)\n",
    "    spacer = int(spacer * s_r_c)\n",
    "    template = np.zeros(length + spacer)\n",
    "    t_length = np.arange(0, length)\n",
    "    offset = len(template) - length\n",
    "    Aprime = (decay_tau / rise_tau) ** (rise_tau / (rise_tau - decay_tau))\n",
    "    y = (\n",
    "        amplitude\n",
    "        / Aprime\n",
    "        * (\n",
    "            (1 - np.exp(-t_length / rise_tau)) ** risepower\n",
    "            * np.exp((-t_length / decay_tau))\n",
    "        )\n",
    "    )\n",
    "    template[offset:] = y\n",
    "    return template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the interactive plot below to see how the different parameters effect the template mEPSC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "template = create_template(decay_tau=2.5)\n",
    "source = ColumnDataSource({\"x\": np.arange(template.size) / 10, \"y\": template})\n",
    "\n",
    "plot = figure(width=400, height=400)\n",
    "\n",
    "plot.line(np.arange(template.size)/10, template, color=\"black\")\n",
    "plot.line(\"x\", \"y\", source=source, line_width=3, line_alpha=0.6, line_color=\"magenta\")\n",
    "\n",
    "rise_tau = Slider(start=0.5, end=10, value=0.5, step=0.5, title=\"Rise tau (ms)\")\n",
    "risepower = Slider(start=0.5, end=10, value=0.5, step=0.25, title=\"Rise power\")\n",
    "decay_tau = Slider(start=0.75, end=50, value=3, step=0.25, title=\"Decay tau (ms)\")\n",
    "amplitude = Slider(start=-60, end=-5, value=-20, step=0.5, title=\"Amplitude (pA)\")\n",
    "length = Slider(start=20, end=70, value=30, step=1, title=\"Length (ms)\")\n",
    "\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        source=source,\n",
    "        rise_tau=rise_tau,\n",
    "        decay_tau=decay_tau,\n",
    "        amplitude=amplitude,\n",
    "        length=length,\n",
    "        risepower=risepower,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    if (rise_tau === decay_tau) {\n",
    "        rise_tau += 0.001;\n",
    "    }\n",
    "    const s_r_c = 10\n",
    "    const rt = Math.round(rise_tau.value * s_r_c)\n",
    "    const dt = Math.round(decay_tau.value * s_r_c)\n",
    "    const len = Math.round(length.value * s_r_c)\n",
    "    const spacer = 15\n",
    "    const y = new Array(len+spacer).fill(0)\n",
    "    const t_length = Array.from({ length: len }, (_, i) => 0 + i)\n",
    "    const Aprime = (dt / rt) ** (rt / (rt - dt))\n",
    "    const temp_y = t_length.map(x => {\n",
    "        return amplitude.value / Aprime * ((1 - Math.exp(-x / rt)) ** risepower.value * Math.exp(-x / dt))\n",
    "    })\n",
    "    y.splice(spacer, temp_y.length, ...temp_y);\n",
    "    const x = Array.from({ length: len+spacer }, (_, i) => 0 + i/10)\n",
    "    source.data = { x, y }\n",
    "    source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "rise_tau.js_on_change(\"value\", callback)\n",
    "decay_tau.js_on_change(\"value\", callback)\n",
    "amplitude.js_on_change(\"value\", callback)\n",
    "length.js_on_change(\"value\", callback)\n",
    "\n",
    "show(row(plot, column(rise_tau, decay_tau, amplitude, length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a template\n",
    "For the analysis of mEPSCs on parvalbumin interneurons we just need to modify the decay rate of the template since PV cell mEPSCs tend to have a very fast decay compared to other cell types (think about why this might be related to the function of PV cells in the larger circuit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = create_template(decay_tau=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Template matching\n",
    "Template matching essentially slides the template along the acquisition and correlates the template with the segment of the acquisition it is currently aligned with. I do some extract work to ensure the template matched array is zero phase relative to the original array. This makes it easier to find PSC events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in exp_dict.values():\n",
    "    temp_match = np.correlate(value[\"lowpass\"], template, mode=\"full\")\n",
    "    temp_match = temp_match[template.size - 1 :]\n",
    "    value[\"temp_match\"] = temp_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Deconvolution\n",
    "Deconvolution essetially divides out the template from the acquisition. Deconvolution is inherently noisy so the deconvolve output has to be filtered to even see the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in exp_dict.values():\n",
    "    kernel = np.hstack((template, np.zeros(len(value[\"lowpass\"]) - len(template))))\n",
    "    template_fft = fft.fft(kernel)\n",
    "    signal_fft = fft.fft(value[\"lowpass\"])\n",
    "    temp = signal_fft / template_fft\n",
    "    temp = np.real(fft.ifft(temp))\n",
    "    sos = signal.butter(4, Wn=300, btype=\"lowpass\", output=\"sos\", fs=sample_rate)\n",
    "    deconvolved = signal.sosfiltfilt(sos, temp)\n",
    "    value[\"deconvolved\"] = deconvolved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the two methods.You will notice that peaks end up in approximately the same place and are positive. These peaks are where putative mEPSCs are occuring. There are two major differences. One is that the deconvolved array has a stable baseline which can make event finding easier. The second is that peaks in the deconvolved array are narrower but shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "index = 1\n",
    "x = np.arange(len(exp_dict[index][\"array\"]))\n",
    "source = ColumnDataSource(\n",
    "    {\n",
    "        \"x\": x,\n",
    "        \"temp_match\": exp_dict[index][\"temp_match\"],\n",
    "        \"deconvolved\": exp_dict[index][\"deconvolved\"],\n",
    "    }\n",
    ")\n",
    "p1 = figure(title=\"Template match\", height=300, width=600, output_backend=\"webgl\")\n",
    "_ = p1.line(x=\"x\", y=\"temp_match\", source=source, line_color=\"black\", line_width=1)\n",
    "p2 = figure(\n",
    "    title=\"Deconvolved\",\n",
    "    height=300,\n",
    "    width=600,\n",
    "    output_backend=\"webgl\",\n",
    "    x_range=p1.x_range,\n",
    ")\n",
    "_ = p2.line(x=\"x\", y=\"deconvolved\", source=source, line_color=\"black\", line_width=1)\n",
    "show(column(p1, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding events\n",
    "\n",
    "The next step involves finding peaks where . For each method we will need to define some threshold so that we don't pick up on the small peaks that are noise. For finding events we will use a way I devised that helps create a per acquisition normalization which allows using a single threshold value for difference acquisitions. First we will get the RMS without the peaks. We will use that to adjust a single threshold value. Finally we will use Scipy find_peaks to find the events above the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentile_rms(deconvolved_array: np.ndarray) -> float | float:\n",
    "    # Get the top and bottom 2.5% cutoff.\n",
    "    bottom, top = np.percentile(deconvolved_array, [2.5, 97.5])\n",
    "\n",
    "    # Return the middle values.\n",
    "    middle = np.hstack(\n",
    "        deconvolved_array[\n",
    "            np.argwhere((deconvolved_array > bottom) & (deconvolved_array < top))\n",
    "        ]\n",
    "    )\n",
    "    # Calculate the mean and rms.\n",
    "    mu = np.mean(middle)\n",
    "    rms = np.sqrt(np.mean(np.square(middle - mu)))\n",
    "\n",
    "    return mu, rms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Template matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = 3.5\n",
    "mini_spacing = 100\n",
    "\n",
    "for value in exp_dict.values():\n",
    "    mu, rms = get_percentile_rms(value[\"temp_match\"])\n",
    "    peaks, _ = signal.find_peaks(\n",
    "        value[\"temp_match\"] - mu,\n",
    "        height=sensitivity * (rms),\n",
    "        distance=mini_spacing,\n",
    "        prominence=rms,\n",
    "    )\n",
    "    value[\"temp_match_events\"] = peaks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = 4\n",
    "mini_spacing = 100\n",
    "\n",
    "for value in exp_dict.values():\n",
    "    mu, rms = get_percentile_rms(value[\"deconvolved\"])\n",
    "    peaks, _ = signal.find_peaks(\n",
    "        value[\"deconvolved\"] - mu,\n",
    "        height=sensitivity * (rms),\n",
    "        distance=mini_spacing,\n",
    "        prominence=rms,\n",
    "    )\n",
    "    value[\"deconvolved_events\"] = peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_match_x = exp_dict[index][\"temp_match_events\"]\n",
    "deconvolved_x = exp_dict[index][\"deconvolved_events\"]\n",
    "_ = p1.scatter(\n",
    "    temp_match_x, exp_dict[index][\"temp_match\"][temp_match_x], color=\"orange\"\n",
    ")\n",
    "_ = p2.scatter(\n",
    "    deconvolved_x, exp_dict[index][\"deconvolved\"][deconvolved_x], color=\"magenta\"\n",
    ")\n",
    "show(column(p1, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see where these events are in the original acquisition. Most of the time you will see that purple and orange dots are falling just before the event. You will notice that many of the events found from both methods are in the same place but the smaller event locations seem to be most different between the two methods. You will notice that some locations do not seem to have an event and that is okay because these will be screened out at the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "temp_match_x = exp_dict[index][\"temp_match_events\"]\n",
    "deconvolved_x = exp_dict[index][\"deconvolved_events\"]\n",
    "array = exp_dict[index][\"lowpass\"]\n",
    "f = figure(title=\"Template match\", height=300, width=600, output_backend=\"webgl\")\n",
    "_ = f.line(np.arange(array.size), array, color=\"black\")\n",
    "_ = f.scatter(temp_match_x, array[temp_match_x], color=\"orange\")\n",
    "_ = f.scatter(deconvolved_x, array[deconvolved_x], color=\"magenta\")\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing events\n",
    "\n",
    "The analysis from this point on will get much harder. The are many parameters for many events that we will need to assess and keep track of. There are several ways that you can optimally store and retrieve data in Python. We will primarily use Python dictionaries which are general container, however if you want to create a program your self I would recommend using classes. Since this tutorial is focused on analyzing the data rather than developing an optimal program we will stick with the basics.\n",
    "\n",
    "One important factor to note is that for any method analyzing events noise is always an issue. The quality of the events and the parameters we retrieve will depend on how noisy the acquisitions are. Noise acquisitions make it hard to find the baseline and peak of events. Noise makes it hard to determine what is a real event and what a bad event. For this reason good mini analysis programs tend to let you add and remove events as well as change the baseline and peak of events. There are many do not have interactive features. This tutorial is limited in that it will be very hard to modify event parameters that are incorrect since we do not have a fully interative UI. However, I think that it is extremely useful to see and think about how events are found.\n",
    "\n",
    "For the next step we are going to analyze the events we have found.\n",
    "We will go through the following steps:\n",
    "1. Create the event start and stop\n",
    "2. Find the event peak.\n",
    "3. Find the baseline. You need the baseline to calculate the amplitude and after finding the baseline.\n",
    "4. Find the event decay with a simple estimate.\n",
    "5. Find the event decay with curve fitting.\n",
    "7. Clean the events.\n",
    "\n",
    "After finding these parameters we calculate other parameters such as rise time, rise rate and amplitude of the event. Since it is computationally fast to compute these other values, it is beneficial to save on memory and just compute them as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the event start and stop\n",
    "The method of finding events that we have used usually places the event marker just before the start of the event. We will create a window around the event. We will need to define how long we want an event. For mEPSCs 30 ms is usually long enough. We will also need to define how much earlier the event should start compared to the event position. For now 2 ms is good enough. Because we are working in samples we will have to convert both of the times to samples. Since our sample rate is 10000 Hz we need to multiply each time by 10 which we will use many times so we will save it as a variable s_r_c (sample rate correction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_event(\n",
    "    event_array: np.ndarray, event_position: int, event_length: int, offset: int\n",
    "):\n",
    "    array_start = int(event_position - offset)\n",
    "    end = int(event_position + event_length)\n",
    "    if end > len(event_array) - 1:\n",
    "        array_end = len(event_array) - 1\n",
    "    else:\n",
    "        array_end = end\n",
    "    array_start = max(0, array_start)\n",
    "    array_end = min(array_end, array.size)\n",
    "    return array_start, array_end\n",
    "\n",
    "\n",
    "s_r_c = 10\n",
    "offset = 2 * s_r_c\n",
    "event_length = 30 * s_r_c\n",
    "for value in exp_dict.values():\n",
    "    value[\"events\"] = []\n",
    "    array = value[\"array\"]\n",
    "    for p in value[\"deconvolved_events\"]:\n",
    "        event = {}\n",
    "        start, stop = create_event(array, p, event_length, offset)\n",
    "        event[\"start\"] = start\n",
    "        event[\"stop\"] = stop\n",
    "        event[\"event_position\"] = p\n",
    "        value[\"events\"].append(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the peak\n",
    "\n",
    "There are a couple ways to find the peaks of the event. If your event placement is good enough you can just use min or max depending on the direction of currents/voltages. However, this fails if your event window contains another event which is not that uncommon or if you have noise in your recording. I use an interative method to find the peak. First we use a prominence based peak finding method. If any peaks are found then we will check that we peak we found is not just noise. If that fails then we use a order based peak finding where a peak is just a value that is larger than all the values within 4 ms on both sides. You could simply find the minimum/maximum value, however I found that in practice this does not work very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_corr(event_array, peak: int, s_r_c):\n",
    "    peaks_2 = signal.argrelextrema(\n",
    "        event_array[:peak],\n",
    "        comparator=np.less,\n",
    "        order=int(0.4 * s_r_c),\n",
    "    )[0]\n",
    "    peaks_2 = peaks_2[peaks_2 > peak - 4 * s_r_c]\n",
    "    if len(peaks_2) == 0:\n",
    "        event_peak_x = peak\n",
    "    else:\n",
    "        peaks_3 = peaks_2[event_array[peaks_2] < 0.85 * event_array[peak]]\n",
    "        if len(peaks_3) == 0:\n",
    "            event_peak_x = peak\n",
    "        else:\n",
    "            event_peak_x = peaks_3[0]\n",
    "    event_peak_y = event_array[int(event_peak_x)]\n",
    "    return event_peak_x, event_peak_y\n",
    "\n",
    "\n",
    "def find_peak_alt(event_array, offset):\n",
    "    peaks = signal.argrelextrema(event_array, comparator=np.less, order=int(3 * s_r_c))[\n",
    "        0\n",
    "    ]\n",
    "    peaks = peaks[peaks > offset]\n",
    "    if len(peaks) == 0:\n",
    "        event_peak_x = np.nan\n",
    "        event_peak_y = np.nan\n",
    "    else:\n",
    "        event_peak_x, event_peak_y = peak_corr(event_array, peaks[0], s_r_c)\n",
    "    return event_peak_x, event_peak_y\n",
    "\n",
    "\n",
    "def find_peak(event_array, offset, s_r_c):\n",
    "    peaks, _ = signal.find_peaks(\n",
    "        -1 * event_array,\n",
    "        prominence=4,\n",
    "        width=0.4 * s_r_c,\n",
    "        distance=int(3 * s_r_c),\n",
    "    )\n",
    "    peaks = peaks[peaks > offset]\n",
    "    if len(peaks) == 0:\n",
    "        event_peak_x, event_peak_y = find_peak_alt(event_array, offset)\n",
    "    else:\n",
    "        event_peak_x, event_peak_y = peak_corr(event_array, peaks[0], s_r_c)\n",
    "    return event_peak_x, event_peak_y\n",
    "\n",
    "\n",
    "for value in exp_dict.values():\n",
    "    for p in value[\"events\"]:\n",
    "        event_array = value[\"lowpass\"][p[\"start\"] : p[\"stop\"]]\n",
    "        offset = p[\"start\"] - p[\"event_position\"]\n",
    "        peak_x, peak_y = find_peak(event_array, offset, 10)\n",
    "        peak_x += p[\"start\"]\n",
    "        p[\"peak_x\"] = peak_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the baseline\n",
    "\n",
    "There are two ways to find the baseline. One is to use a slope and find when the slope stops increasing. This method needs several additions to make it work well. The other way is to assume that the baseline of your event is around 0 mV. The problem with this method is that if your acquisition meanders around 0 mV you can find very weird baselines. We will use the slope method with modifications I have found work very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_baseline(event_array, event_peak_x, event_peak_y, s_r_c):\n",
    "    baselined_array = event_array - np.max(event_array[:event_peak_x])\n",
    "    peak = int(event_peak_x)\n",
    "    search_start = np.argwhere(baselined_array[:peak] > 0.35 * event_peak_y).flatten()\n",
    "    if search_start.size > 0:\n",
    "        slope = (event_array[search_start[-1]] - event_peak_y) / (\n",
    "            peak - search_start[-1]\n",
    "        )\n",
    "        new_slope = slope + 1\n",
    "        i = search_start[-1]\n",
    "        while new_slope > slope and i > 0:\n",
    "            slope = (event_array[i] - event_peak_y) / (peak - i)\n",
    "            i -= 1\n",
    "            new_slope = (event_array[i] - event_peak_y) / (peak - i)\n",
    "        baseline_start = signal.argrelmax(\n",
    "            baselined_array[int(i - 1 * s_r_c) : i + 2], order=2\n",
    "        )[0]\n",
    "        if baseline_start.size > 0:\n",
    "            baseline_x = int(baseline_start[-1] + (i - 1 * s_r_c))\n",
    "            if baseline_x < 0:\n",
    "                baseline_x = 0\n",
    "        else:\n",
    "            baseline_x = int(baseline_start.size / 2 + (i - 1 * s_r_c))\n",
    "            if baseline_x < 0:\n",
    "                baseline_x = 0\n",
    "    return baseline_x\n",
    "\n",
    "\n",
    "for value in exp_dict.values():\n",
    "    array = value[\"lowpass\"]\n",
    "    for p in value[\"events\"]:\n",
    "        event_array = array[p[\"start\"] : p[\"stop\"]]\n",
    "        baseline_x = find_baseline(\n",
    "            event_array, p[\"peak_x\"] - p[\"start\"], array[p[\"peak_x\"]], 10\n",
    "        )\n",
    "        p[\"baseline_x\"] = baseline_x + p[\"start\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the decay\n",
    "Next we will estimate the decay of the events. You can estimate the decay by finding the time when the signal is 1/3 the amplitude. One thing to note is that we have just been keeping track of the x sample where each point of interest occurs. However, with the estimate decay we will interpolate y so we will collect the y value as well since interpolating requires a bit more work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_decay(event_array, baseline, peak_x):\n",
    "    peak_y = event_array[peak_x]\n",
    "    baselined_event = event_array - baseline\n",
    "    x_array = np.arange(event_array.size)\n",
    "    return_to_baseline = int(\n",
    "        (np.argmax(baselined_event[peak_x:] >= (peak_y - baseline) * 0.25)) + (peak_x)\n",
    "    )\n",
    "    decay_y = event_array[peak_x:return_to_baseline]\n",
    "    if decay_y.size > 0:\n",
    "        est_tau_y = ((peak_y - baseline) * (1 / np.exp(1))) + baseline\n",
    "        decay_x = x_array[peak_x:return_to_baseline]\n",
    "        event_tau_x = np.interp(est_tau_y, decay_y, decay_x)\n",
    "    else:\n",
    "        event_tau_x = np.nan\n",
    "        est_tau_y = np.nan\n",
    "    return event_tau_x, est_tau_y\n",
    "\n",
    "\n",
    "for value in exp_dict.values():\n",
    "    array = value[\"lowpass\"]\n",
    "    for p in value[\"events\"]:\n",
    "        event_array = array[p[\"start\"] : p[\"stop\"]]\n",
    "        baseline = array[p[\"baseline_x\"]]\n",
    "        tau_x, tau_y = est_decay(event_array, baseline, p[\"peak_x\"] - p[\"start\"])\n",
    "        p[\"tau_x\"] = tau_x + p[\"start\"]\n",
    "        p[\"tau_y\"] = tau_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curve fitting the decay\n",
    "We will curve fit the decay for a more accurate measure of the decay. We will use a double exponential decay. If you want to try fitting a single exponential decay I challenge you to look at the code below and figure it out. One thing we will do is run the decay fit using a try except statement since the fit can fail and it is hard to predict when it will fail. You will also notice that we are using what is called a double exponential decay. This is simply two exponentials added together. You could fit a single or even triple exponential decay. Doulbe exponential decay tends to give a pretty good fit. Since the exponential decays are additive you just add the fast and slow decay tau together to get your decay fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_exp_decay(x, amplitude_fast, tau_fast, amplitude_slow, tau_slow):\n",
    "    y = (amplitude_fast * np.exp((-x) / tau_fast)) + (\n",
    "        amplitude_slow * np.exp((-x) / tau_slow)\n",
    "    )\n",
    "    return y\n",
    "\n",
    "\n",
    "def fit_decay(event_array, est_tau, s_r_c):\n",
    "    try:\n",
    "        x_array = np.arange(event_array.size) / s_r_c\n",
    "        upper_bounds = [0.0, np.inf, 0.0, np.inf]\n",
    "        lower_bounds = [-np.inf, 0.0, -np.inf, 0.0]\n",
    "        init_param = np.array([event_array[0], est_tau, 0.0, 0.0])\n",
    "        popt, _ = optimize.curve_fit(\n",
    "            db_exp_decay,\n",
    "            x_array,\n",
    "            event_array,\n",
    "            p0=init_param,\n",
    "            bounds=[lower_bounds, upper_bounds],\n",
    "        )\n",
    "        amplitude_fast, tau_fast, amplitude_slow, tau_slow = popt\n",
    "        output = {\n",
    "            \"amplitude_fast\": amplitude_fast,\n",
    "            \"tau_fast\": tau_fast,\n",
    "            \"amplitude_slow\": amplitude_slow,\n",
    "            \"tau_slow\": tau_slow,\n",
    "            # \"y_offset\": y_offset\n",
    "        }\n",
    "    except (RuntimeError, ValueError):\n",
    "        output = {\n",
    "            \"amplitude_fast\": np.nan,\n",
    "            \"tau_fast\": np.nan,\n",
    "            \"amplitude_slow\": np.nan,\n",
    "            \"tau_slow\": np.nan,\n",
    "            # \"y_offset\": np.nan\n",
    "        }\n",
    "    return output\n",
    "\n",
    "\n",
    "for value in exp_dict.values():\n",
    "    array = value[\"lowpass\"]\n",
    "    for p in value[\"events\"]:\n",
    "        est_tau = p[\"tau_x\"] - p[\"peak_x\"]\n",
    "        temp = int(est_tau * 1.5) + p[\"peak_x\"]\n",
    "        stop = min(temp, p[\"stop\"])\n",
    "        event_array = array[p[\"peak_x\"] : p[\"stop\"]]\n",
    "        decay_fit = fit_decay(event_array, est_tau / 10, 10)\n",
    "        p[\"decay_fit\"] = decay_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at all the analysis to see how well it worked. There is a least one instance where two events occured close together and only one event was analyzed. The are a couple events where the place of a baseline or peak may not be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "index = 1\n",
    "y = exp_dict[index][\"lowpass\"]\n",
    "events = exp_dict[index][\"events\"]\n",
    "mfig = figure(height=300, width=600, output_backend=\"webgl\")\n",
    "x = np.arange(array.size)\n",
    "mfig.line(x, y, color=\"black\")\n",
    "for i in events:\n",
    "    start = i[\"start\"]\n",
    "    stop = i[\"stop\"]\n",
    "    mfig.line(x[start:stop], y[start:stop], color=\"magenta\")\n",
    "\n",
    "    # Plot curve fit\n",
    "    if not np.isnan(i[\"decay_fit\"][\"amplitude_fast\"]):\n",
    "        start = i[\"peak_x\"]\n",
    "        fit_x = np.arange(stop - start) / 10\n",
    "        fit_y = db_exp_decay(fit_x, **i[\"decay_fit\"])\n",
    "        mfig.line(x[start:stop], fit_y, color=\"#06DBE2\")\n",
    "\n",
    "# Plot baseline\n",
    "peak_x = [i[\"baseline_x\"] for i in exp_dict[index][\"events\"]]\n",
    "peak_y = [y[i] for i in peak_x]\n",
    "mfig.scatter(peak_x, peak_y, color=\"grey\", marker=\"plus\", size=7)\n",
    "\n",
    "# Plot peaks\n",
    "peak_x = [i[\"peak_x\"] for i in exp_dict[index][\"events\"]]\n",
    "peak_y = [y[i] for i in peak_x]\n",
    "mfig.scatter(peak_x, peak_y, color=\"orange\", size=7)\n",
    "\n",
    "# Plot tau\n",
    "tau_x = [i[\"tau_x\"] for i in exp_dict[index][\"events\"] if not np.isnan(i[\"tau_x\"])]\n",
    "tau_y = [i[\"tau_y\"] for i in exp_dict[index][\"events\"] if not np.isnan(i[\"tau_y\"])]\n",
    "mfig.scatter(tau_x, tau_y, color=\"#06DBE2\", marker=\"triangle\", size=7)\n",
    "\n",
    "show(mfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to calculate all the other parameters, amplitude, rise time and rise rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in exp_dict.values():\n",
    "    array = value[\"lowpass\"]\n",
    "    for p in value[\"events\"]:\n",
    "        p[\"rise_time\"] = (p[\"peak_x\"] - p[\"baseline_x\"]) / 10\n",
    "        p[\"amplitude\"] = np.abs(array[p[\"peak_x\"]] - array[p[\"baseline_x\"]])\n",
    "        p[\"rise_rate\"] = p[\"amplitude\"] / p[\"rise_time\"]\n",
    "        p[\"est_tau\"] = (p[\"tau_x\"] - p[\"peak_x\"]) / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will exclude events based on amplitude, rise time, decay time. Some of the settings may be a little bit redundant however, I have found this to be pretty robust in excluding events. It is common to exclude events that:\n",
    "- Have a rise time that is too fast (usually 0.5ms is a good starting point)\n",
    "- Have a rise time that is too slow. This will depend on the type of m/sEPSC you are analyzing. Inhibitory events have a slow rise time compared to excitatory events.\n",
    "- Amplitude that is too small. The minimum amplitude is depends on your signal to noise. I have found that PV interneurons have a particularly high signal noise ratio (good) where as MSNs have a much lower signal to noise ratio so the amplitude setting is 4 pA vs 6 pA respectively.\n",
    "- Decay time that is too short. This really helps remove events that are just noise since noise tends to be symmetrical.\n",
    "- Decay faster than rise. This is a boolean. Usually if the decay tau is faster than the rise the event is just noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_event(event, event_criteria, prior_peaks, prior_peak=0, fs=10000) -> bool:\n",
    "    \"\"\"The function is used to screen out events based\n",
    "    on several values set by the experimenter.\n",
    "\n",
    "    Args:\n",
    "        event (MiniEvent): An analyzed MiniEvent\n",
    "        events (list): List of previous events\n",
    "\n",
    "    Returns:\n",
    "        Bool: Boolean value can be used to determine if\n",
    "        the event qualifies for inclusion in final events.\n",
    "    \"\"\"\n",
    "    # Retrieve the peak to compare to values set\n",
    "    # by the experimenter.\n",
    "    s_r_c = fs / 1000\n",
    "    event_peak = event[\"peak_x\"]\n",
    "\n",
    "    # The function checks, in order of importance, the\n",
    "    # qualities of the event.\n",
    "    if np.isnan(event_peak) or event_peak in prior_peaks:\n",
    "        return False\n",
    "    elif (\n",
    "        (event_peak - prior_peak) / s_r_c < event_criteria[\"mini_spacing\"]\n",
    "        or event[\"amplitude\"] <= event_criteria[\"amp_threshold\"]\n",
    "        or event[\"rise_time\"] <= event_criteria[\"min_rise_time\"]\n",
    "        or event[\"rise_time\"] >= event_criteria[\"max_rise_time\"]\n",
    "        or event[\"est_tau\"] <= event_criteria[\"min_decay_time\"]\n",
    "        or event[\"start\"] > event_peak\n",
    "    ):\n",
    "        return False\n",
    "    elif event_criteria[\"decay_rise\"] and (event[\"est_tau\"] <= event[\"rise_time\"]):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "event_criteria = {\n",
    "    \"decay_rise\": False,\n",
    "    \"max_rise_time\": 10,\n",
    "    \"min_rise_time\": 0.1,\n",
    "    \"amp_threshold\": 4,\n",
    "    \"mini_spacing\": 2,\n",
    "    \"min_decay_time\": 0.5,\n",
    "}\n",
    "for value in exp_dict.values():\n",
    "    prior_peak = 0\n",
    "    accepted_events = []\n",
    "    event_peaks = set()\n",
    "    for event in value[\"events\"]:\n",
    "        check = check_event(event, event_criteria, event_peaks, prior_peak)\n",
    "        if check:\n",
    "            accepted_events.append(event)\n",
    "            prior_peak = event[\"peak_x\"]\n",
    "    value[\"accepted_events\"] = accepted_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final data inspection\n",
    "\n",
    "Finally we have cleaned our data. It is time to pull all the data and see how it looks. There are many ways to inspect the data we collected. The two primary ways I like to look at the data is through distributions and stem plots. Distributions give you an idea of whether the data is gaussian, or lognormally distributed. In the plots below you can log transform the data to see how it changes. Stem plots allow you to see how the data changes over time. There is plot where you can plot the different measures against each other on the x and y axis to see how they are related. Finally we will plot all the PSCs together as a scaled and unscaled version. Scaling events (between 0 and 1) is useful for determining whether the events are coming from the same population since different populations of events tend to have different rise and decay times, and different amplitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem plots and distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "gathered_data = []\n",
    "time_add = 0\n",
    "for key, value in exp_dict.items():\n",
    "    output = defaultdict(list)\n",
    "    array = value[\"lowpass\"]\n",
    "    for event in value[\"accepted_events\"]:\n",
    "        output[\"peak_x\"].append(event[\"peak_x\"] / 10)\n",
    "        output[\"timestamp\"].append(event[\"peak_x\"] / 10 + time_add)\n",
    "        output[\"est_tau\"].append(event[\"est_tau\"])\n",
    "        output[\"rise_time\"].append(event[\"rise_time\"])\n",
    "        output[\"rise_rate\"].append(event[\"rise_rate\"])\n",
    "        output[\"amplitude\"].append(event[\"amplitude\"])\n",
    "        output[\"amplitude_fast\"].append(event[\"decay_fit\"][\"amplitude_fast\"])\n",
    "        output[\"amplitude_slow\"].append(event[\"decay_fit\"][\"amplitude_slow\"])\n",
    "        output[\"tau_slow\"].append(event[\"decay_fit\"][\"tau_slow\"])\n",
    "        output[\"tau_fast\"].append(event[\"decay_fit\"][\"tau_fast\"])\n",
    "        start = event[\"start\"]\n",
    "        stop = event[\"stop\"]\n",
    "        output[\"event\"].append(array[start:stop])\n",
    "        output[\"start\"].append(start)\n",
    "    output[\"iei\"] = [\n",
    "        output[\"peak_x\"][i] - output[\"peak_x\"][i - 1]\n",
    "        for i in range(1, len(output[\"peak_x\"]))\n",
    "    ]\n",
    "    output[\"acquisition\"] = [key]\n",
    "    output[\"holding_current\"] = [output[\"holding_current\"]]\n",
    "    time_add += 10000\n",
    "\n",
    "    gathered_data.append(output)\n",
    "\n",
    "final_data = defaultdict(list)\n",
    "for item in gathered_data:\n",
    "    for key, value in item.items():\n",
    "        final_data[key].extend(value)\n",
    "final_data[\"fit_tau\"] = np.array([final_data[\"tau_fast\"], final_data[\"tau_slow\"]]).sum(\n",
    "    axis=0\n",
    ")\n",
    "final_data[\"fit_amplitude\"] = np.abs(\n",
    "    np.array([final_data[\"amplitude_fast\"], final_data[\"amplitude_slow\"]]).sum(axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "figure1 = figure(height=250, width=400)\n",
    "figure2 = figure(height=250, width=400)\n",
    "variable_dict = {\n",
    "    key: final_data[key]\n",
    "    for key in (\n",
    "        \"est_tau\",\n",
    "        \"rise_time\",\n",
    "        \"amplitude\",\n",
    "        \"rise_rate\",\n",
    "        \"fit_amplitude\",\n",
    "        \"fit_tau\",\n",
    "    )\n",
    "}\n",
    "variable_dict[\"iei\"] = final_data[\"iei\"] + [0] * int(\n",
    "    len(final_data[\"est_tau\"]) - len(final_data[\"iei\"])\n",
    ")\n",
    "for key, value in variable_dict.items():\n",
    "    variable_dict[key] = [[0, i] for i in value]\n",
    "\n",
    "source1 = ColumnDataSource(variable_dict)\n",
    "\n",
    "hist_dict = {}\n",
    "for i in [\n",
    "    \"est_tau\",\n",
    "    \"rise_time\",\n",
    "    \"amplitude\",\n",
    "    \"rise_rate\",\n",
    "    \"iei\",\n",
    "    \"fit_amplitude\",\n",
    "    \"fit_tau\",\n",
    "]:\n",
    "    data = final_data[i]\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    padding = (max_val - min_val) * 0.1  # Add 10% padding\n",
    "    grid_min = min_val - padding\n",
    "    grid_max = max_val + padding\n",
    "    grid_min = max(grid_min, 0)\n",
    "    positions = np.linspace(grid_min, grid_max, num=124)\n",
    "    kernel = stats.gaussian_kde(data)\n",
    "    y = kernel(positions)\n",
    "    hist_dict[f\"{i}_x\"] = positions\n",
    "    hist_dict[f\"{i}_y\"] = y\n",
    "\n",
    "hist_dict[\"y\"] = hist_dict[\"amplitude_y\"]\n",
    "hist_dict[\"x\"] = hist_dict[\"amplitude_x\"]\n",
    "source2 = ColumnDataSource(hist_dict)\n",
    "\n",
    "\n",
    "mline = figure1.multi_line(\n",
    "    [[i, i] for i in final_data[\"timestamp\"]],\n",
    "    source1.data[\"amplitude\"],\n",
    "    line_width=1,\n",
    "    line_alpha=0.6,\n",
    "    line_color=\"black\",\n",
    ")\n",
    "line = figure2.line(x=\"x\", y=\"y\", source=source2, line_color=\"black\")\n",
    "\n",
    "menu = Select(\n",
    "    title=\"Variables\",\n",
    "    value=\"amplitude\",\n",
    "    options=[\n",
    "        \"est_tau\",\n",
    "        \"rise_time\",\n",
    "        \"amplitude\",\n",
    "        \"rise_rate\",\n",
    "        \"iei\",\n",
    "        \"fit_amplitude\",\n",
    "        \"fit_tau\",\n",
    "    ],\n",
    ")\n",
    "xcheck = Checkbox(label=\"Log(x)\")\n",
    "\n",
    "callback = CustomJS(\n",
    "    args=dict(\n",
    "        source1=source1,\n",
    "        source2=source2,\n",
    "        mline=mline,\n",
    "        line=line,\n",
    "        menu=menu,\n",
    "        xcheck=xcheck,\n",
    "    ),\n",
    "    code=\"\"\"\n",
    "    const x_name = `${menu.value}_x`;\n",
    "    if (xcheck.active) {\n",
    "        var x = source2.data[x_name].map(num => Math.log10(num));\n",
    "    } else {\n",
    "        var x = source2.data[x_name];\n",
    "    }\n",
    "    mline.data_source.data.ys = source1.data[menu.value];\n",
    "    mline.data_source.change.emit();\n",
    "    const y = `${menu.value}_y`;\n",
    "    line.data_source.data.y = source2.data[y];\n",
    "    line.data_source.data.x = x;\n",
    "    line.data_source.change.emit();\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "menu.js_on_change(\"value\", callback)\n",
    "xcheck.js_on_change(\"active\", callback)\n",
    "\n",
    "show(column(row(menu, xcheck), row(figure1, figure2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average and scaled events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "max_event_length = max([len(i) for i in final_data[\"event\"]])\n",
    "x = [np.arange(max_event_length) / 10 for _ in final_data[\"event\"]]\n",
    "events = np.zeros((len(final_data[\"event\"]), max_event_length))\n",
    "index = 0\n",
    "for e in final_data[\"event\"]:\n",
    "    events[index, : len(e)] = e\n",
    "    index += 1\n",
    "scaled_events = events - events.max(axis=1, keepdims=True)\n",
    "scaled_events /= abs(scaled_events.min(axis=1, keepdims=True))\n",
    "source1 = ColumnDataSource(\n",
    "    {\"x\": list(x), \"events\": list(events), \"scaled_events\": list(scaled_events)}\n",
    ")\n",
    "source2 = ColumnDataSource(\n",
    "    {\n",
    "        \"x\": np.arange(max_event_length) / 10,\n",
    "        \"avg_event\": events.mean(axis=0),\n",
    "        \"avg_scaled\": scaled_events.mean(axis=0),\n",
    "    }\n",
    ")\n",
    "fig1 = figure(title=\"Events\", height=250, width=400, output_backend=\"webgl\")\n",
    "fig1.multi_line(\"x\", \"events\", source=source1, alpha=0.2, color=\"black\")\n",
    "fig1.line(\"x\", \"avg_event\", source=source2, color=\"orange\")\n",
    "fig2 = figure(title=\"Scaled events\", height=250, width=400, output_backend=\"webgl\")\n",
    "fig2.multi_line(\"x\", \"scaled_events\", source=source1, alpha=0.2, color=\"black\")\n",
    "fig2.line(\"x\", \"avg_scaled\", source=source2, color=\"orange\")\n",
    "show(row(fig1, fig2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is it for the basic PSC analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": -1
  },
  "kernelspec": {
   "display_name": "ephysbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
