{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7c89af",
   "metadata": {},
   "source": [
    "# Synthesizing data (IN PROGRESS)\n",
    "Synthesizing data is rarely covered in slice electrophysiology but I feel like it can help one understand how data is being generated and also provides some insight to the statistical assumptions we make about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c3500",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.layouts import gridplot, row, layout\n",
    "from bokeh.models import ColumnDataSource, CustomJS, Select, Slider, Whisker, Div\n",
    "from bokeh.plotting import figure\n",
    "from scipy import stats, signal\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "cwd = Path.cwd().parent / \"data/pv\"\n",
    "df = pd.read_csv(cwd / \"mini_data.csv\")\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f7a952",
   "metadata": {},
   "source": [
    "## Point processes\n",
    "[Point processes](https://en.wikipedia.org/wiki/Point_process) are a mathematical way to describe how points are randomly located in space. Time is just a space along a real line. We can describe point processes in time by their interevent interval, the space between each event.\n",
    "\n",
    "### Poisson point process\n",
    "In the [distributions](distributions) section we cover the Poisson distribution. The Poisson distribution is one of the simplest ways to generate a point process. The Poisson process assumes that events are indepedent of each other and randomly distributes events in time. With the Poisson distribution you could model how many minis or spikes are likely to occur in a time frame based on a rate. To model a Poisson process you would draw IEIs from an exponential distribution. We can use the Fano Factor to model how \"Poissonian\" timeseries data is. There is one big caveat to using the fano factor in that it is rate dependent.\n",
    "\n",
    "Below I will create two different point processes and show how at face value they look similar but, when you dig deeper you find important differences. While this analysis may not be super useful for mPSCs, I believe understanding the basics of point processes can help you think about your how your data is generated. Think about what processes might lead you have events non-randomly distributed in time. What if the fano factor differs between your experiments groups? Can you use this information to inform your interpretation of the data and future experiments? Remember that when you put TTX in your bath you are blocking any evoked activity therefore your events should be randomly distributed over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c718d28",
   "metadata": {},
   "source": [
    "### Pseudo Poisson process: Creating a point process by randomly distributing points in 10 second segments using a fixed 3 Hz rate for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d9273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "ieis_pseudo = []\n",
    "train_pseudo = []\n",
    "for i in range(30):\n",
    "    times = rng.uniform(0, 10, size=30)\n",
    "    times = np.sort(times)\n",
    "    ieis_pseudo.extend(np.diff(np.sort(times)))\n",
    "    train_pseudo.extend(times + i * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85b7f93",
   "metadata": {},
   "source": [
    "### Poisson process: Creating a point process by drawing from an exponential distribution for each 10 second segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa58053",
   "metadata": {},
   "outputs": [],
   "source": [
    "ieis_poisson = []\n",
    "train_poisson = []\n",
    "num_minis = 30\n",
    "length = 10\n",
    "for i in range(30):\n",
    "    start = stats.expon.rvs(size=1, scale=1 / 3)\n",
    "    times = np.array(start)\n",
    "\n",
    "    three_stds = int(np.ceil(num_minis + 3 * np.sqrt(num_minis)))\n",
    "\n",
    "    while times[-1] < length:\n",
    "        isi = stats.expon.rvs(size=three_stds, scale=1 / 3)\n",
    "\n",
    "        t_last_mepscs = times[-1]\n",
    "        times = np.r_[times, t_last_mepscs + np.cumsum(isi)]\n",
    "    stop = times.searchsorted(length)\n",
    "    times = times[:stop]\n",
    "    ieis_poisson.extend(np.diff(times))\n",
    "    train_poisson.extend(times + i * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d82b3",
   "metadata": {},
   "source": [
    "Below you can see the IEI distribution for timeseries as well as a \"continuous\" event rate. The continuous event rate t is just the number of events in a bin divided by the bin width (in this case 300 seconds divided by 50 bins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08efe4",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "pseudo_div = Div(\n",
    "    text=\"\"\"\n",
    "<h2>Pseudo Poisson</h2>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "loc, scale = stats.expon.fit(ieis_pseudo)\n",
    "fig_pseudo = figure(\n",
    "    height=250,\n",
    "    width=350,\n",
    "    title=f\"Expected scale: {1 / 3:.3f} vs Fit scale: {scale:.3f}\",\n",
    ")\n",
    "hist, bins = np.histogram(ieis_pseudo, bins=50, density=True)\n",
    "hist_data = fig_pseudo.quad(\n",
    "    top=hist,\n",
    "    bottom=0,\n",
    "    left=bins[:-1],\n",
    "    right=bins[1:],\n",
    "    alpha=0.5,\n",
    ")\n",
    "x = np.linspace(\n",
    "    stats.expon.ppf(0.0001, scale=scale), stats.expon.ppf(0.9999, scale=scale), 100\n",
    ")\n",
    "\n",
    "line = fig_pseudo.line(x, stats.expon.pdf(x, scale=scale), line_color=\"black\")\n",
    "ts_pseudo = figure(height=250, title=\"Continuous event rate\")\n",
    "hist, bins = np.histogram(train_pseudo, bins=50)\n",
    "hist = hist / (300 / 50)\n",
    "hist_data = ts_pseudo.quad(\n",
    "    top=hist,\n",
    "    bottom=0,\n",
    "    left=bins[:-1],\n",
    "    right=bins[1:],\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "poisson_div = Div(\n",
    "    text=\"\"\"\n",
    "<h2>Poisson</h2>\n",
    "\"\"\"\n",
    ")\n",
    "loc, scale = stats.expon.fit(ieis_poisson)\n",
    "fig_poisson = figure(\n",
    "    height=250,\n",
    "    width=350,\n",
    "    title=f\"Expected scale: {1 / 3:.3f} vs Fit scale: {scale:.3f}\",\n",
    ")\n",
    "hist, bins = np.histogram(ieis_poisson, bins=50, density=True)\n",
    "hist_data = fig_poisson.quad(\n",
    "    top=hist,\n",
    "    bottom=0,\n",
    "    left=bins[:-1],\n",
    "    right=bins[1:],\n",
    "    alpha=0.5,\n",
    ")\n",
    "x = np.linspace(\n",
    "    stats.expon.ppf(0.0001, scale=scale), stats.expon.ppf(0.9999, scale=scale), 100\n",
    ")\n",
    "\n",
    "line = fig_poisson.line(x, stats.expon.pdf(x, scale=scale), line_color=\"black\")\n",
    "ts_poisson = figure(height=250, title=\"Continuous event rate\")\n",
    "hist, bins = np.histogram(train_poisson, bins=50)\n",
    "hist = hist / (300 / 50)\n",
    "hist_data = ts_poisson.quad(\n",
    "    top=hist,\n",
    "    bottom=0,\n",
    "    left=bins[:-1],\n",
    "    right=bins[1:],\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "show(\n",
    "    layout(\n",
    "        [pseudo_div], [fig_pseudo, ts_pseudo], [poisson_div], [fig_poisson, ts_poisson]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5f484",
   "metadata": {},
   "source": [
    "By eye these times series look almost identical. If you analyzed these two timeseries use rate alone you would not find a statistical difference since the mean IEI is not different. To see the differences we need to look deeper in the structure of the timeseries. To do this we can calculate something called dispersion or the Fano Factor which we talked about in the [distributions](distributions) chapter. To calculate the Fano Factor we just need to bin the data and get mean and variance of the counts in each bin. However, we are going to bin our data several different bin sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d53ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fano_factor_bins(timestamps, total_time, b0=0.5, b1=5, bn=100):\n",
    "    bsizes = np.linspace(b0, b1, num=bn)\n",
    "    ff = np.zeros(bsizes.size)\n",
    "    for index, bin_size in enumerate(bsizes):\n",
    "        n_bins = int(total_time / bin_size)\n",
    "        bins = np.linspace(0, total_time, n_bins + 1)\n",
    "\n",
    "        # Count events in each bin\n",
    "        counts, _ = np.histogram(timestamps, bins)\n",
    "        ff[index] = np.var(counts) / np.mean(counts)\n",
    "    return bsizes, ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a8784",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 10 * 30\n",
    "pseudo_ff = figure(height=250, width=350, title=\"Pseudo Poisson\")\n",
    "pseudo_ff.scatter(*fano_factor_bins(train_pseudo, total_time, b1=2.5))\n",
    "poisson_ff = figure(height=250, width=350, title=\"Poisson\")\n",
    "poisson_ff.scatter(*fano_factor_bins(train_poisson, total_time, b1=2.5))\n",
    "show(row(pseudo_ff, poisson_ff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f747ea",
   "metadata": {},
   "source": [
    "You can see the difference between these two timeseries when we compare the fano factor across bins. The Pseudo Poisson show a decreasing Fano Factor with increasing binwidth which means the signal becomes more regular or predictable. The Poisson process shows a Fano Factor that hovers around 1 for most of the bin sizes.\n",
    "\n",
    "There are some caveats to the way I created the Poisson process. I created short 10 second chunks and concatenated them together. This technically violates an assumption of the Poisson process however, I ran it this way to model how mPSC data is collected. Usually you record short 5-20 second chunks. These chunks are usually not 100% continguous due to how digitizers work. The Fano factor will also depend on your refactory time. Refactory time is minimum time between events. Neurons have a reset period between spikes, or for PSCs have cannot be resolved when they are too close together.\n",
    "\n",
    "Below I simulate an exponential with a refractory period. You will notice that the fano factor tends to be lower the longer the refractory period even though the scale is technically the same. The refractory period does change the rate. So our effective rate is: $$R_e = \\frac{(1/R)}{\\frac{1/R}*t+1}$$ where $R_e$ is the effective rate, $R$ is the rate you put in and $t$ is the refractory period. If you want to correct for the refractory period we can change the effective rate with this equation: $$R = \\frac{R_t}{1-R_t*t}$$ where $R_t$ is your target and $R$ is the actual rate. If we don't correct for the refractory rate you can see that the fano factor decreases which means the regularity of the signal increases. The effect gets more noticable as you increase the rate. So we if we wanted to analyze signal with refractory period or say lower recovery events in a noisy environment we could use create synthetic data to test our hypothesis and compare to our actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a47293",
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets = [(0, 3), (0.005, 3), (0.01, 3), (0, 5), (0.005, 5), (0.01, 5)]\n",
    "time = 300\n",
    "trains = []\n",
    "for rf, rate in offsets:\n",
    "    ff = []\n",
    "    for j in range(100):\n",
    "        start = stats.expon.rvs(size=1, scale=1 / rate)\n",
    "        event_times = np.array(start)\n",
    "\n",
    "        three_stds = int(np.ceil(time * 3 + 3 * np.sqrt(time * 3)))\n",
    "\n",
    "        while event_times[-1] < time:\n",
    "            isi = stats.expon.rvs(size=three_stds, scale=1 / rate) + rf\n",
    "\n",
    "            t_last_mepscs = event_times[-1]\n",
    "            event_times = np.r_[event_times, t_last_mepscs + np.cumsum(isi)]\n",
    "        stop = event_times.searchsorted(time)\n",
    "        event_times = event_times[:stop]\n",
    "        x, y = fano_factor_bins(event_times, time, b0=0.1, b1=2.5, bn=50)\n",
    "        ff.append(y)\n",
    "    ff = np.array(ff)\n",
    "    ff = ff.mean(axis=0)\n",
    "    temp = figure(height=250, width=300, title=f\"Rate {rate}, Refractory {rf}\")\n",
    "    temp.scatter(x, ff)\n",
    "    trains.append(temp)\n",
    "show(gridplot(trains, ncols=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephysbook (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
